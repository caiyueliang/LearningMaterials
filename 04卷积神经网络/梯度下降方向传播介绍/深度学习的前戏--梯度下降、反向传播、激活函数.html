<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/32714733 -->
<html lang="zh" data-theme="light" class=" no-touch"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>深度学习的前戏--梯度下降、反向传播、激活函数</title><meta name="renderer" content="webkit"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><link rel="shortcut icon" href="https://static.zhihu.com/static/favicon.ico" type="image/x-icon"><link href="./深度学习的前戏--梯度下降、反向传播、激活函数_files/app.aba113267232df22b22239921d0d14b2.css" rel="stylesheet"><script type="text/javascript" charset="utf-8" async="" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/editor.3f071b8af3f7c490563e.js.下载"></script><style type="text/css">.Notification{position:fixed;top:0;left:50%;z-index:499;padding:14px 24px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-transform:translate(-50%);transform:translate(-50%);font-size:14px;color:#262626;pointer-events:all;border-radius:4px;-webkit-box-shadow:0 1px 3px 0 #afbdcf;box-shadow:0 1px 3px 0 #afbdcf;-webkit-box-sizing:border-box;box-sizing:border-box}@media (max-width:768px){.Notification{width:calc(100vw - 32px)}}@media (min-width:769px){.Notification{width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;max-width:600px;min-width:520px}}.Notification-textSection{width:100%;display:inline-block}@media (max-width:768px){.Notification-textSection{white-space:nowrap;overflow:hidden;text-overflow:ellipsis}}.Notification-textSection--withButton{text-align:left}.Notification-textSection--withoutButton{text-align:center}.Notification-actionsSection{white-space:nowrap;height:100%;margin:auto 0 auto 32px}.Notification-wrapper{position:fixed;top:0;right:0;bottom:0;left:0;z-index:503;overflow:hidden;pointer-events:none}.Notification-white{color:#262626;background:#fff}.Notification-red{color:#fff;background:#f75659}.Notification-red-ghost{color:#f75659;background:#fff}@-webkit-keyframes spring-in{0%{-webkit-transform:translate(-50%,-20px);transform:translate(-50%,-20px);opacity:.01}to{-webkit-transform:translate(-50%);transform:translate(-50%);opacity:1}}@keyframes spring-in{0%{-webkit-transform:translate(-50%,-20px);transform:translate(-50%,-20px);opacity:.01}to{-webkit-transform:translate(-50%);transform:translate(-50%);opacity:1}}@-webkit-keyframes spring-out{0%{-webkit-transform:translate(-50%);transform:translate(-50%);opacity:1}to{-webkit-transform:translate(-50%,-20px);transform:translate(-50%,-20px);opacity:0}}@keyframes spring-out{0%{-webkit-transform:translate(-50%);transform:translate(-50%);opacity:1}to{-webkit-transform:translate(-50%,-20px);transform:translate(-50%,-20px);opacity:0}}.Notification-enter{-webkit-animation:spring-in .3s;animation:spring-in .3s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.Notification-leave{-webkit-animation:spring-out .3s;animation:spring-out .3s;-webkit-animation-fill-mode:both;animation-fill-mode:both}</style><style type="text/css">.Formula{display:inline-block;vertical-align:middle;background:no-repeat 50%;background-size:contain;font-size:0}.Formula.isEditable{cursor:pointer}.Formula-image{max-width:100%;opacity:0;visibility:hidden}.Formula-placeholder{opacity:0}</style><style type="text/css">.MathToolbar{display:block;padding:4px 12px;border-radius:inherit inherit 0 0;background:#f7f8fa;border-bottom:1px solid #e7eaf1;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.MathToolbar-button{height:28px;padding:2px 4px;-webkit-box-sizing:border-box;box-sizing:border-box;border:1px solid transparent;vertical-align:middle}.MathToolbar-button+.MathToolbar-button{margin-left:16px}.MathToolbar-button:hover{background:#fcfcfc;border-color:#ebeef5}.MathToolbar-palettes{-ms-flex-wrap:wrap;flex-wrap:wrap;margin:-4px;padding:16px;max-width:384px}.MathToolbar-palettes,.MathToolbar-palettesButton{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row}.MathToolbar-palettesButton{margin:4px;padding:0;width:24px;height:24px;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px;color:#8590a6;border-radius:4px}.MathToolbar-palettesButton:hover{background-color:#f7f8fa}.MathToolbar-paletteIcon{max-width:calc(100% - 2px)}.MathToolbar-palettes--math{max-width:380px}.MathToolbar-palettes--math .MathToolbar-palettesButton{padding:0 3px;width:30px;height:60px}.MathToolbar-palettes--arrow .MathToolbar-palettesButton{height:35px}</style><style type="text/css">.FormulaModal{width:550px}.FormulaModal-input{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-bottom:30px}.FormulaModal-input .Input{padding:6px 12px;min-height:100px;-webkit-box-sizing:border-box;box-sizing:border-box}.FormulaModal-formula{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding:0 3px;background:#fff;border:2px dashed #e7eaf1;border-radius:3px;min-height:106px;overflow-x:auto}.FormulaModal-formula img{max-width:100%}.FormulaModal-buttonGroup{margin-top:32px}.FormulaModal-previewText{color:#9fadc7;font-size:14px}</style><style type="text/css">.FocusPlugin--unfocused:hover{cursor:default;-webkit-box-shadow:0 0 0 2px rgba(15,136,235,.3);box-shadow:0 0 0 2px rgba(15,136,235,.3)}.FocusPlugin--focused{cursor:default;-webkit-box-shadow:0 0 0 2px #0f88eb;box-shadow:0 0 0 2px #0f88eb}</style><style type="text/css">.Image{max-width:100%;margin:0 auto}.Image[data-size=small]{max-width:40%}.Image--isBlock{display:block}</style><style type="text/css">.Editable-imageUploader{text-align:center}.Editable-imageUploader-layout{position:relative;display:inline-block;max-width:100%;vertical-align:top}.Editable-imageUploader-layout.is-fullWidth{width:100%}.Editable-imageUploader-image{display:block;max-width:100%;opacity:.3}.Editable-imageUploader-placeholder{height:192px;background-color:#e7eaf1}.Editable-imageUploader-status{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%}.Editable-imageUploader-statusText{font-size:28px;line-height:40px;color:#8597a6}.Editable-imageUploader-status.is-error .Editable-imageUploader-statusText{color:#e26d6d}.Editable-imageUploader-retry{margin-top:4px;font-size:16px;line-height:32px}.Editable-imageUploader-retry .Button{font-size:inherit}.Editable-imageUploader-progress{position:absolute;left:0;bottom:0;width:100%;height:4px;background-color:#0f88eb}.Editable-imageUploader-progress.is-error{background-color:#e26d6d}.Editable-imageUploader-progress .LoadingBar{position:relative;height:100%;background-color:hsla(0,0%,100%,.3)}</style><style type="text/css">.Image-caption.is-placeholder{color:#bfbfbf}.Image-caption.is-editing{opacity:0}.Image-captionInput{position:absolute;z-index:203}.Image-captionInput textarea{display:block;overflow:hidden;width:100%;height:100%;padding:0;border:none;font:inherit;font-size:14px;line-height:1.5;text-align:center;color:#8590a6;background:none;resize:none}.Image-captionInput textarea::-webkit-input-placeholder{color:#bfbfbf}.Image-captionInput textarea:-ms-input-placeholder{color:#bfbfbf}.Image-captionInput textarea::placeholder{color:#bfbfbf}.Image-captionInput textarea:focus{outline:none}</style><style type="text/css">.Image-resizer{padding:8px}.Image-resizerButton{padding:0 8px;vertical-align:middle}.Image-resizerButton .Zi{display:block}.Image-resizerButton.is-active{color:#0f88eb}</style><style type="text/css">.Editable-video{margin:16px 0;border-radius:4px}.Video-uploadPosterButton{background:rgba(0,0,0,.5);border:1px solid hsla(0,0%,100%,.3);border-radius:3px;height:32px;width:109px;position:absolute;bottom:16px;right:16px;text-align:center;line-height:32px;cursor:pointer}.Video-uploadPosterButton p{color:#fff;font-size:14px;margin:0!important}</style><style type="text/css">.Editable-videoUploader{margin:16px 0;border-radius:4px}.Editable-videoUploader-thumbnail{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:100%}.Editable-videoUploader-text{text-align:center;position:relative;color:#8597a6}.Editable-videoUploader-status{font-size:28px;line-height:40px}.Editable-videoUploader-size{margin-top:10px;font-size:14px;line-height:20px}.Editable-videoUploader-progress{position:absolute;left:0;right:0;bottom:0;overflow:hidden;height:4px;background-color:rgba(0,0,0,.05)}.Editable-videoUploader-progress-bar{position:absolute;left:0;top:0;height:100%;background-color:#0f88eb}.Editable-videoUploader-progress-bar.is-error{background-color:#e26d6d}.Editable-videoUploader-progress .LoadingBar{position:absolute;left:0;top:0;height:100%;background-color:hsla(0,0%,100%,.3)}.Editable-videoUploader-image{position:absolute}.Editable-videoUploader-uploadPosterButton{background:rgba(0,0,0,.5);border:1px solid hsla(0,0%,100%,.3);border-radius:3px;height:32px;width:109px;position:absolute;bottom:-130px;right:16px;text-align:center;line-height:32px;cursor:pointer}.Editable-videoUploader-uploadPosterButton p{color:#fff;font-size:14px;margin:0!important}.Editable-videoUploader-imageMask{width:100%;height:100%;position:absolute;background:rgba(231,234,241,.9)}</style><style type="text/css">.Editable-videoError{margin:16px 0;border-radius:4px}</style><style type="text/css">.Editable-divider{overflow:hidden}</style><style type="text/css">.Link+.Link{margin-left:2px}.Link[data-editable]{cursor:text!important}</style><style type="text/css">.LinkModal-input{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.LinkModal-input+.LinkModal-input{margin-top:10px}.LinkModal-input .Input{margin-left:8px}</style><style type="text/css">.LinkBubble.Popover-content--top.Popover-content--arrowed{margin-top:-10px}.LinkBubble.Popover-content--bottom.Popover-content--arrowed{margin-top:10px}.LinkBubble-content{display:block;padding:7px 10px 7px 16px}.LinkBubble-button,.LinkBubble-previewLink{vertical-align:middle}.LinkBubble-previewLink{display:inline-block;margin-right:8px;max-width:233px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;text-decoration:none;font-size:14px;line-height:1.3;border-bottom:1px solid transparent}.LinkBubble-previewLink:hover{color:#175199;border-bottom-color:rgba(62,122,194,.72)}.LinkBubble-button{padding:0 6px}</style><style type="text/css">.MentionSuggestions{position:absolute;z-index:203;line-height:1}.MentionSuggestions-input{width:200px;padding:4px 6px;font-size:inherit}.MentionSuggestions-input .Input{height:auto;background:transparent}.MentionSuggestions-menu{width:200px}.MentionSuggestions-menu .Menu-item{padding:0 10px}</style><style type="text/css">.Dropzone{position:relative}.Dropzone-cursor{position:absolute;left:0;right:0;margin:-1px 0;border-bottom:2px solid #0f88eb;pointer-events:none}</style><style type="text/css">.Editable-toolbar{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;background:#fff;border-bottom:1px solid #e7eaf1;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.isToolbarSticky .Editable-toolbar,.Sticky.is-fixed .Editable-toolbar{border-top:none!important;border-bottom:none!important}.isToolbarSticky .Editable-toolbar:after,.Sticky.is-fixed .Editable-toolbar:after{content:" ";position:absolute;left:0;top:100%;width:100%;pointer-events:none;height:3px;background:radial-gradient(ellipse at 50% 1%,rgba(0,0,0,.1),hsla(0,0%,100%,0) 80%)}.Editable-control,.Editable-toolbar-separator{margin-right:10px}.Editable-control:last-child,.Editable-toolbar-separator:last-child{margin-right:0}.Editable-toolbar-separator{display:inline-block;width:1px;height:28px;vertical-align:middle;background-color:#e7eaf1}.Editable-control{padding:0 1px;height:28px;cursor:pointer;-webkit-box-sizing:border-box;box-sizing:border-box;white-space:nowrap;border:1px solid transparent}.Editable-control .Zi{fill:#8590a6}.Editable-control:not(:disabled):hover{background:#fcfcfc;border-color:#ebeef5}.Editable-control.is-active .Zi{fill:#0f88eb}.Editable-control .Zi,.Editable-control span{vertical-align:middle}.Editable-control .Zi+span{margin-left:6px}</style><style type="text/css">.Editable-videoModal{display:block}.Editable-videoModal-typeButton{font-size:20px;font-weight:400;color:inherit}.Editable-videoModal-typeButton.is-active{font-weight:500;color:#0f88eb}.Editable-videoModal-uploader{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-sizing:border-box;box-sizing:border-box;height:220px;border:2px dashed #e7eaf1;border-radius:10px;cursor:pointer}.Editable-videoModal-uploader:hover{border-color:#9fadc7}.Editable-videoModal-uploader:hover .Zi{fill:#9fadc7}.Editable-videoModal-uploader input{display:none}.Editable-videoModal-uploader-icon{text-align:center;display:block}.Editable-videoModal-uploader-icon .Zi{vertical-align:middle;fill:#e7eaf1}.Editable-videoModal-uploader-text{margin-top:10px;font-size:18px;line-height:30px;text-align:center}.Editable-videoModal-uploader-tip{font-size:14px;line-height:30px;text-align:center;color:#8590a6}</style><style type="text/css">.Editable-languageSuggestions{position:fixed;z-index:203}.Editable-languageSuggestionsInput{cursor:pointer}.Editable-languageSuggestionsInput input{cursor:inherit}.Editable-languageSuggestionsInput input:focus{cursor:text}.Editable-languageSuggestionsMenu{max-height:300px;margin-top:-8px;margin-bottom:-8px;overflow-y:auto}</style><style type="text/css">.Editable-notification-layout{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.Editable-notification-actions,.Editable-notification-layout{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row}.Editable-notification-actions{margin:0 -12px}.Editable-notification-action{margin:0 12px}.Editable .RichText{cursor:text}.public-DraftEditorPlaceholder-root{position:absolute;pointer-events:none}.Editable--hidePlaceholder .public-DraftEditorPlaceholder-root{display:none}.Editable-styled,.Editable-unstyled{margin:0 0 .72em}.Editable-styled:last-child,.Editable-unstyled:last-child{margin-bottom:0}.DraftEditor-root blockquote+blockquote{margin-top:-1em}.DraftEditor-root pre.public-DraftStyleDefault-pre{border-radius:4px}.DraftEditor-root pre.public-DraftStyleDefault-pre pre{padding:0;margin:0;border-radius:0;overflow:visible;overflow:initial}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth1,.DraftEditor-root .public-DraftStyleDefault-unorderedListItem.public-DraftStyleDefault-depth1{margin-left:2em}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth2,.DraftEditor-root .public-DraftStyleDefault-unorderedListItem.public-DraftStyleDefault-depth2{margin-left:4em}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth3,.DraftEditor-root .public-DraftStyleDefault-unorderedListItem.public-DraftStyleDefault-depth3{margin-left:6em}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth4,.DraftEditor-root .public-DraftStyleDefault-unorderedListItem.public-DraftStyleDefault-depth4{margin-left:8em}.DraftEditor-root .public-DraftStyleDefault-orderedListItem{position:relative;list-style-type:none}.DraftEditor-root .public-DraftStyleDefault-orderedListItem:before{position:absolute;left:-36px;width:30px;text-align:right}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth0.public-DraftStyleDefault-reset{counter-reset:ol0}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth0:before{content:counter(ol0) ". ";counter-increment:ol0}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth1.public-DraftStyleDefault-reset{counter-reset:ol1}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth1:before{content:counter(ol1) ". ";counter-increment:ol1}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth2.public-DraftStyleDefault-reset{counter-reset:ol2}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth2:before{content:counter(ol2) ". ";counter-increment:ol2}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth3.public-DraftStyleDefault-reset{counter-reset:ol3}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth3:before{content:counter(ol3) ". ";counter-increment:ol3}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth4.public-DraftStyleDefault-reset{counter-reset:ol4}.DraftEditor-root .public-DraftStyleDefault-orderedListItem.public-DraftStyleDefault-depth4:before{content:counter(ol4) ". ";counter-increment:ol4}_:-ms-lang(x),pre.public-DraftStyleDefault-pre,pre.public-DraftStyleDefault-pre pre{overflow:visible;overflow:initial;word-wrap:break-word}</style><style type="text/css">.DraftEditor-root code[class*=language-],.DraftEditor-root pre[class*=language-]{color:#000;background:none;text-shadow:0 1px #fff;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none}.DraftEditor-root code[class*=language-]::-moz-selection,.DraftEditor-root code[class*=language-] ::-moz-selection,.DraftEditor-root pre[class*=language-]::-moz-selection,.DraftEditor-root pre[class*=language-] ::-moz-selection{text-shadow:none;background:#b3d4fc}.DraftEditor-root code[class*=language-]::selection,.DraftEditor-root code[class*=language-] ::selection,.DraftEditor-root pre[class*=language-]::selection,.DraftEditor-root pre[class*=language-] ::selection{text-shadow:none;background:#b3d4fc}@media print{.DraftEditor-root code[class*=language-],.DraftEditor-root pre[class*=language-]{text-shadow:none}}.DraftEditor-root pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto}.DraftEditor-root :not(pre)>code[class*=language-],.DraftEditor-root pre[class*=language-]{background:#f5f2f0}.DraftEditor-root :not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.DraftEditor-root .token.cdata,.DraftEditor-root .token.comment,.DraftEditor-root .token.doctype,.DraftEditor-root .token.prolog{color:#708090}.DraftEditor-root .token.punctuation{color:#999}.DraftEditor-root .namespace{opacity:.7}.DraftEditor-root .token.boolean,.DraftEditor-root .token.constant,.DraftEditor-root .token.deleted,.DraftEditor-root .token.number,.DraftEditor-root .token.property,.DraftEditor-root .token.symbol,.DraftEditor-root .token.tag{color:#905}.DraftEditor-root .token.attr-name,.DraftEditor-root .token.builtin,.DraftEditor-root .token.char,.DraftEditor-root .token.inserted,.DraftEditor-root .token.selector,.DraftEditor-root .token.string{color:#690}.DraftEditor-root .language-css .token.string,.DraftEditor-root .style .token.string,.DraftEditor-root .token.entity,.DraftEditor-root .token.operator,.DraftEditor-root .token.url{color:#a67f59;background:hsla(0,0%,100%,.5)}.DraftEditor-root .token.atrule,.DraftEditor-root .token.attr-value,.DraftEditor-root .token.keyword{color:#07a}.DraftEditor-root .token.function{color:#dd4a68}.DraftEditor-root .token.important,.DraftEditor-root .token.regex,.DraftEditor-root .token.variable{color:#e90}.DraftEditor-root .token.bold,.DraftEditor-root .token.important{font-weight:600;font-synthesis:style}html[data-ios] .DraftEditor-root .token.bold,html[data-ios] .DraftEditor-root .token.important{font-weight:500}html[data-android] .DraftEditor-root .token.bold,html[data-android] .DraftEditor-root .token.important{font-weight:700}.DraftEditor-root .token.italic{font-style:italic}.DraftEditor-root .token.entity{cursor:help}</style></head><body><div id="react-root"><div data-reactroot=""><!-- react-empty: 2 --><div class="Layout av-cardBackground" data-zop-usertoken="{&quot;userToken&quot;:&quot;yue-liang-he-xiao&quot;}"><!-- react-empty: 4 --><div class="Layout-navbarHolder"><header class="Navbar ScrollBackFixed"><div class="Navbar-logo-wrapper"><a class="Navbar-logo icon-ic_zhihu_logo" href="https://www.zhihu.com/" target="_blank" rel="noopener noreferrer" aria-label="知乎首页"></a></div><div class="Navbar-postTitle Navbar-title Navbar-titleMr60"><a href="https://zhuanlan.zhihu.com/leemoo"><img class="Navbar-columnIcon" alt="每天都要机器学习哦" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-8456af8026d9b1d75719eab1971a3662_m.jpg"></a><div class="Navbar-postTitleName"><span class="Navbar-postTitleMeta">首发于</span><a class="Navbar-postTitleMain" href="https://zhuanlan.zhihu.com/leemoo">每天都要机器学习哦</a></div><!-- react-empty: 15 --></div><div class="Navbar-functionality"><a class="Navbar-write"><i class="icon icon-ic_nav_new"></i><!-- react-text: 19 -->写文章<!-- /react-text --></a><div class="Menu Navbar-menu"><button class="Button Button Button--plain MenuButton MenuButton-listen-hover icon icon-ic_nav_more" aria-label="更多选项" type="button"></button><div class="Menu-dropdown" style="visibility: hidden;"></div></div></div></header></div><!-- react-empty: 23 --><div></div><div class="Layout-main av-card av-paddingBottom av-bodyFont Layout-titleImage--normal"><div class="PostIndex-header av-paddingTop av-card" data-zop="{&quot;authorName&quot;:&quot;王乐&quot;,&quot;itemId&quot;:&quot;32714733&quot;,&quot;title&quot;:&quot;深度学习的前戏--梯度下降、反向传播、激活函数&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;32714733&quot;}}}"><div class="TitleImage"><img alt="深度学习的前戏--梯度下降、反向传播、激活函数" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-5022c810d5368fc75d86d7c2b2dc0860_r.jpg" class="TitleImage-imagePure TitleImage-imagePure--fixed" height="326px"></div><h1 class="PostIndex-title av-paddingSide av-titleFont">深度学习的前戏--梯度下降、反向传播、激活函数</h1><div class="PostIndex-author"><a href="https://www.zhihu.com/people/wangle-nlp" target="_blank" rel="noopener noreferrer"><img class="Avatar-hemingway PostIndex-authorAvatar Avatar--xs" alt="王乐" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-5a76beb4edb4fee8e8b6026394b47368_xs.jpg" srcset="https://pic1.zhimg.com/v2-5a76beb4edb4fee8e8b6026394b47368_l.jpg 2x"></a><a href="https://www.zhihu.com/people/wangle-nlp" target="_blank" class="PostIndex-authorName">王乐</a><!-- react-empty: 34 --><span class="Bull"></span><div class="HoverTitle" data-hover-title="2018 年 1月 8 日星期一下午 5 点 54 分"><time datetime="Mon Jan 08 2018 17:54:08 GMT+0800 (中国标准时间)">5 天前</time></div></div></div><div class="RichText PostIndex-content av-paddingSide av-card"><p>在入门深度学习时，梯度下降、反向传播、激活函数这三个概念是绕不过的知识点，如果不能好好理解这些点那么深度学习可能就入不了门；如果不能好好的将这些点联系起来，我觉得对深度神经网络的理解也会很迷惑。网上介绍这些概念的文章有很多，但是往往都是单独介绍的，因为每一个概念要介绍起来到需要很多笔墨；然而为了更好的理解这些概念，为了知其然并且知其所以然，我觉得有必要将它们串起来讲一讲。</p><p>为什么说它们是深度学习的前戏呢，只因为它们是深度学习的基础之中的基础概念。在我看来，深度学习包含两方面内容：</p><ol><li>更好的训练深度神经网络。神经网络隐藏层超过两层就算深度神经网络，三层的NN的训练还好说，但是如果NN很多层数呢？那将会面临梯度弥散和梯度爆炸等问题。所以为了让训练的DNN取得好的效果，就有了一些训练DNN的技巧，比如反向传播算法、激活函数、批量归一化、dropout等技术的发明；而梯度下降是为了更好的优化代价函数，不管是机器学习还是深度学习，总会需要优化代价函数（损失函数）。</li><li>设计网络结构以更好的提取特征。增加神经网络隐藏层就能提取更高层次特征，卷积神经网络能提取空间上的特征，循环神经网络能够提取时间序列特征，等等；于是各种网络结构被发明出来，比如AlexNet，LeNet，GooleNet，Inception系列，ResNet等等，另外还有LSTM等等。</li></ol><p>网络结构再美，如果不能训练到收敛，就是不work。所以我们今天介绍的这些技术就是为了更好的训练DNN，它们是保证能够训练好的DNN的基础，所以它们叫深度学习的前戏！！</p><hr><h2>1、梯度下降</h2><p>假设有输入 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation" alt="\{x_1,x_2,...,x_n\}" eeimg="1"> ，对应的输出为 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(1)" alt="\{y_1,y_2,..,y_n\}" eeimg="1"> ，我们希望神经网络的输出f(x)可以拟合所有训练输入xi，为此，我们需要定义一个代价函数：</p><p><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(2)" alt="C(v_1,v_2)\equiv \frac{1}{2n}\sum_{i}^{n}{(f(x_i)-y_i)^2}" eeimg="1"> </p><p>要找到一组合适的参数（v1,v2）最小化上述代价函数，只要用微积分的知识解出上述代价函数右边部分的极值点就行了，也就是求导就够了；然而求导的方法在参数较少的时候行的通，但是参数数量一旦多了就不好办了。正好，深度学习中神经网络参数动辄几百万几千万个参数，所以直接计算倒数求极值行不通。</p><p>为了解决这个问题，我们以上述例子来介绍一下梯度下降算法是如何求得极值的。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/v2-2db5731c0563357c3205c0d1bc05bb08_b.jpg" data-caption="" data-size="normal" data-rawwidth="482" data-rawheight="342" class="origin_image zh-lightbox-thumb" width="482" data-original="https://pic3.zhimg.com/v2-2db5731c0563357c3205c0d1bc05bb08_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-2db5731c0563357c3205c0d1bc05bb08_hd.jpg" data-caption="" data-size="normal" data-rawwidth="482" data-rawheight="342" class="origin_image zh-lightbox-thumb lazy" width="482" data-original="https://pic3.zhimg.com/v2-2db5731c0563357c3205c0d1bc05bb08_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-2db5731c0563357c3205c0d1bc05bb08_b.jpg"></figure><p>如上图所示，首先我们初始化v1,v2，假如现在（v1,v2）的取值如上图小球所在的位置，我们要做的就是寻找最佳v1,v2的取值使得代价函数最小，也就是使上图上的小球从山坡上移动到谷底。这里有两个方向v1,v2，也就是两个变量，想象一下小球分别往两个方向移动很小的量，即 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(3)" alt="\Delta v_1,\Delta v_2" eeimg="1"> ,那么小球移动的大小将为：</p><p><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(4)" alt="\Delta C\approx \frac{\partial C}{\partial v_1}\Delta v_1 + \frac{\partial C}{\partial v_2}\Delta v_2" eeimg="1"> </p><p><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(5)" alt="\frac{\partial C}{\partial v_1}" eeimg="1"> 表示函数C对变量v1的偏导数，也就是代价函数在v1上的变化速率，乘以变量的变化量就是代价函数自身的变化量了。</p><p>定义倒三角形C为梯度向量，即：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-f21461ba8f1b6b914210086467d7c749_b.jpg" data-caption="" data-size="normal" data-rawwidth="202" data-rawheight="67" class="content_image" width="202"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-f21461ba8f1b6b914210086467d7c749_hd.jpg" data-caption="" data-size="normal" data-rawwidth="202" data-rawheight="67" class="content_image lazy" width="202" data-actualsrc="https://pic2.zhimg.com/v2-f21461ba8f1b6b914210086467d7c749_b.jpg"></figure><p>那么小球移动的量可以表示为：</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/v2-2445bfa0ff00e95eb688425b08542b22_b.jpg" data-caption="" data-size="normal" data-rawwidth="151" data-rawheight="32" class="content_image" width="151"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-2445bfa0ff00e95eb688425b08542b22_hd.jpg" data-caption="" data-size="normal" data-rawwidth="151" data-rawheight="32" class="content_image lazy" width="151" data-actualsrc="https://pic1.zhimg.com/v2-2445bfa0ff00e95eb688425b08542b22_b.jpg"></figure><p>为了使得 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(6)" alt="\Delta C" eeimg="1"> 为负数，也就是C能够逐渐变小，我们可以取</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-9a1c82722d2cc5da6a54904f93e29ddc_b.jpg" data-caption="" data-size="normal" data-rawwidth="128" data-rawheight="28" class="content_image" width="128"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-9a1c82722d2cc5da6a54904f93e29ddc_hd.jpg" data-caption="" data-size="normal" data-rawwidth="128" data-rawheight="28" class="content_image lazy" width="128" data-actualsrc="https://pic2.zhimg.com/v2-9a1c82722d2cc5da6a54904f93e29ddc_b.jpg"></figure><p>这里 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(7)" alt="\eta" eeimg="1"> 称为学习率，一般是一个很小的正数，于是有了</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-ec183811ed393478b409beeba8103a8a_b.jpg" data-caption="" data-size="normal" data-rawwidth="257" data-rawheight="30" class="content_image" width="257"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-ec183811ed393478b409beeba8103a8a_hd.jpg" data-caption="" data-size="normal" data-rawwidth="257" data-rawheight="30" class="content_image lazy" width="257" data-actualsrc="https://pic2.zhimg.com/v2-ec183811ed393478b409beeba8103a8a_b.jpg"></figure><p>这样保证了 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(6)" alt="\Delta C" eeimg="1"> 为负数。因此我们得到了变量v的变动方式了：</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-90789d1af40ae9346593e0de29f195c1_b.jpg" data-caption="" data-size="normal" data-rawwidth="171" data-rawheight="30" class="content_image" width="171"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-90789d1af40ae9346593e0de29f195c1_hd.jpg" data-caption="" data-size="normal" data-rawwidth="171" data-rawheight="30" class="content_image lazy" width="171" data-actualsrc="https://pic4.zhimg.com/v2-90789d1af40ae9346593e0de29f195c1_b.jpg"></figure><p>总结起来，梯度下降算法的工作方式就是重复计算梯度，然后沿着相反的方向移动，使得小球沿着山谷“滚动”。</p><p>由于考虑到梯度下降算法的性能各方面因素，后来又有了随机梯度下降算法等；但是我这里不是介绍优化算法有什么，而是告诉大家，<b>在优化代价函数的时候需要计算“梯度”，也就是代价函数必须可导，且代价函数在变量（参数）上的导数不能为0，不然就不能通过改变变量来优化代价函数了。</b></p><hr><h2>2、反向传播</h2><p>回忆一下线性回归， <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(8)" alt="y = f(\theta;x)" eeimg="1"> ，x是输入，y是输出， <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(9)" alt="\theta" eeimg="1"> 是参数，梯度下降算法就是用来求得最优参数 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(9)" alt="\theta" eeimg="1"> 的。在这里最初始的输入x和最终的输出y是直接关联的，如果将线性回归看做一个神经网络的话，那这个网络就只有输入层和输出层，而没有隐藏层。在深度神经网络中，隐藏层可能有多层，那么它的初始输入和最终输出是如何关联的呢？怎么应用梯度下降到神经网络中呢？</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-d24339c0b303f6e1a0100492a82f2cd0_b.jpg" data-caption="" data-size="normal" data-rawwidth="499" data-rawheight="281" class="origin_image zh-lightbox-thumb" width="499" data-original="https://pic4.zhimg.com/v2-d24339c0b303f6e1a0100492a82f2cd0_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-d24339c0b303f6e1a0100492a82f2cd0_hd.jpg" data-caption="" data-size="normal" data-rawwidth="499" data-rawheight="281" class="origin_image zh-lightbox-thumb lazy" width="499" data-original="https://pic4.zhimg.com/v2-d24339c0b303f6e1a0100492a82f2cd0_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-d24339c0b303f6e1a0100492a82f2cd0_b.jpg"></figure><p><br></p><p>如上图所示，该网络有两个隐藏层，每个隐藏层 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(10)" alt="layer_i" eeimg="1"> 的输入其实就是上一层 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(11)" alt="layer_{i-1}" eeimg="1"> 的输出，而它的输出又是下一层 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(12)" alt="layer_{i+1}" eeimg="1"> 的输入；反向传播的思想其实就是，对于每一个训练实例，将它传入神经网络，计算它的输出；然后测量网络的输出误差（即期望输出和实际输出之间的差异），并<b>计算出上一个隐藏层中各神经元为该输出结果贡献了多少的误差</b>；反复一直从后一层计算到前一层，直到算法到达初始的输入层为止。此反向传递过程有效地测量网络中所有连接权重的误差梯度，最后<b>通过在每一个隐藏层中应用梯度下降算法来优化该层的参数</b>（反向传播算法的名称也因此而来）。</p><p>上面中文看不懂没关系，可以参考一下英文“ for each training instance the backpropagation algorithm first makes a prediction (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally slightly tweaks the connection weights to reduce the error (Gradient Descent step).”</p><p>上面的描述仅仅是反向传播算法的思想，工作原理；那么具体反向传播算法是怎样计算出每个神经元的误差，并且将这个误差关联到“梯度”上的呢？有兴趣的可以看看下面的数学描述，没兴趣的可以略过……</p><p>引入一个中间量 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(13)" alt="\delta _j^{l}" eeimg="1"> ，将其称为神经网络中在 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(14)" alt="l^{th}" eeimg="1"> 层第 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(15)" alt="j^{th}" eeimg="1"> 个神经元上的误差。如下图所示：</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-e87901a4f6c43398b747034ef0bd288b_b.jpg" data-caption="" data-size="normal" data-rawwidth="609" data-rawheight="306" class="origin_image zh-lightbox-thumb" width="609" data-original="https://pic4.zhimg.com/v2-e87901a4f6c43398b747034ef0bd288b_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-e87901a4f6c43398b747034ef0bd288b_hd.jpg" data-caption="" data-size="normal" data-rawwidth="609" data-rawheight="306" class="origin_image zh-lightbox-thumb lazy" width="609" data-original="https://pic4.zhimg.com/v2-e87901a4f6c43398b747034ef0bd288b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e87901a4f6c43398b747034ef0bd288b_b.jpg"></figure><p>反向传播将给出计算误差 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(13)" alt="\delta _j^{l}" eeimg="1"> 的流程，然后将其关联到“梯度”（ <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(16)" alt="\partial C/\partial w_{jk}^l,\partial C/\partial b_{j}^l" eeimg="1"> ）的计算上。</p><p>想象一下，当每一层的输入进入到该层的每一个神经元，由于前后两层的神经元之间是由权重w连接的，而这个权重在最开始是由我们人为随机设置的，肯定不是最优的权重，所以必然会给上一层的输出带来一个误差，我们将这个误差记为 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(17)" alt="\Delta z_j^l" eeimg="1"> ，它是的神经元的输出从 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(18)" alt="\sigma (z_j^l)" eeimg="1"> 变成 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(19)" alt="\sigma(\Delta z_j^l +z_j^l)" eeimg="1"> ，这个变化会向网络后面的层进行传播，最终导致整个代价产生 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(20)" alt="\frac{\partial C}{\partial z_j^l}\Delta z_j^l" eeimg="1"> 的改变。我们的梯度下降算法正是用来优化代价的，为了使得 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(17)" alt="\Delta z_j^l" eeimg="1"> 更小，假设 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(21)" alt="\frac{\partial C}{\partial z_j^l}" eeimg="1"> 有一个很大的值（或正或负），那么梯度下降将会选择与 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(21)" alt="\frac{\partial C}{\partial z_j^l}" eeimg="1"> 符号相反的 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(17)" alt="\Delta z_j^l" eeimg="1"> 来降低代价。而如果 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(21)" alt="\frac{\partial C}{\partial z_j^l}" eeimg="1"> 接近0，那么无论如何也不能优化代价函数了。因此，我们直觉的认为 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(21)" alt="\frac{\partial C}{\partial z_j^l}" eeimg="1"> 是神经元误差的度量。因此，我们有：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-4d70a9281504497f472e0e24cd13a3ba_b.jpg" data-caption="" data-size="normal" data-rawwidth="112" data-rawheight="68" class="content_image" width="112"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-4d70a9281504497f472e0e24cd13a3ba_hd.jpg" data-caption="" data-size="normal" data-rawwidth="112" data-rawheight="68" class="content_image lazy" width="112" data-actualsrc="https://pic2.zhimg.com/v2-4d70a9281504497f472e0e24cd13a3ba_b.jpg"></figure><p> 我们使用 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(22)" alt="\delta ^l" eeimg="1"> 表示关联于 l 层的误差向量。</p><p>由于我们每一层神经网络的输出和输入之间使用了激活函数 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(23)" alt="\sigma" eeimg="1">（这里提到激活函数，先不管为什么有激活函数，后面再讲） ，我们使用 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(24)" alt="a_j ^l" eeimg="1"> 表示 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(25)" alt="z_j^l" eeimg="1"> 的激活值，那么我们可以使用 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(26)" alt="\frac{\partial C}{\partial a_j^l}" eeimg="1"> 作为度量误差的方法。</p><p>有了以上认识之后，我们来描述反向传播算法：</p><p><b>计算输出层误差的方程</b>，δ^L: 每个元素定义如下:</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-6cbf465e93a1371bbb6019961231a738_b.jpg" data-caption="" data-size="normal" data-rawwidth="496" data-rawheight="67" class="origin_image zh-lightbox-thumb" width="496" data-original="https://pic4.zhimg.com/v2-6cbf465e93a1371bbb6019961231a738_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-6cbf465e93a1371bbb6019961231a738_hd.jpg" data-caption="" data-size="normal" data-rawwidth="496" data-rawheight="67" class="origin_image zh-lightbox-thumb lazy" width="496" data-original="https://pic4.zhimg.com/v2-6cbf465e93a1371bbb6019961231a738_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-6cbf465e93a1371bbb6019961231a738_b.jpg"></figure><p>右式第一个项 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(27)" alt="\partial _{a_j^L}" eeimg="1"> 表示代价随着 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(15)" alt="j^{th}" eeimg="1"> 输出激活值的变化而变化的速度。假如 C 不太依赖一个特定的输出神经元 j，那么 δ_j^L 就会很小，这也是我们想要的效果。右式第二项 σ′(zjL) 刻画了在 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(28)" alt="z_j^L" eeimg="1"> 处激活函数 σ 变化的速度。</p><p><b>使用下一层的误差 δ^{l+1} 来表示当前层的误差 δ^l:</b> 特别地，</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-8dcc9d0f61140415cbdfec4cfb286bb4_b.jpg" data-caption="" data-size="normal" data-rawwidth="519" data-rawheight="56" class="origin_image zh-lightbox-thumb" width="519" data-original="https://pic4.zhimg.com/v2-8dcc9d0f61140415cbdfec4cfb286bb4_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-8dcc9d0f61140415cbdfec4cfb286bb4_hd.jpg" data-caption="" data-size="normal" data-rawwidth="519" data-rawheight="56" class="origin_image zh-lightbox-thumb lazy" width="519" data-original="https://pic4.zhimg.com/v2-8dcc9d0f61140415cbdfec4cfb286bb4_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-8dcc9d0f61140415cbdfec4cfb286bb4_b.jpg"></figure><p>其中 (w^{l+1})^T 是 (l + 1)^{th} 层权重矩阵 w^{l+1} 的转置。这个公式看上去有些复杂，但每一个元素有很好的解释。假设我们知道 l + 1^{th} 层的误差 δ^{l+1}。当我们应用转置的权重矩阵 (w^{l+1})^T ，我们可以凭直觉地把它看作是在沿着网络反向移动误差，给了我们度量在 lth 层输出的误差方法。然后，我们进行 Hadamard 乘积运算 ⊙σ′(z^l)。这会让误差通过 l 层的激活函数反向传递回来并给出在第 l 层的带权输入的误差 δ。<br>通过组合 (BP1) 和 (BP2)，我们可以计算任何层的误差 δ^l。首先使用 (BP1) 计算 δ^L，然后应用方程 (BP2) 来计算 δ^{L−1}，然后再次用方程 (BP2) 来计算 δ^{L−2}，如此一步一步地反向传播完整个网络。</p><p><b>代价函数关于网络中任意偏置的改变率:</b></p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-b7f8fd04de05c5c9936de0d4f1635ab4_b.jpg" data-caption="" data-size="normal" data-rawwidth="462" data-rawheight="79" class="origin_image zh-lightbox-thumb" width="462" data-original="https://pic4.zhimg.com/v2-b7f8fd04de05c5c9936de0d4f1635ab4_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-b7f8fd04de05c5c9936de0d4f1635ab4_hd.jpg" data-caption="" data-size="normal" data-rawwidth="462" data-rawheight="79" class="origin_image zh-lightbox-thumb lazy" width="462" data-original="https://pic4.zhimg.com/v2-b7f8fd04de05c5c9936de0d4f1635ab4_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b7f8fd04de05c5c9936de0d4f1635ab4_b.jpg"></figure><p><b>代价函数关于任何一个权重的改变率:</b></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/v2-8efddaee9d75bcf4a9f33a3d891becb5_b.jpg" data-caption="" data-size="normal" data-rawwidth="473" data-rawheight="83" class="origin_image zh-lightbox-thumb" width="473" data-original="https://pic3.zhimg.com/v2-8efddaee9d75bcf4a9f33a3d891becb5_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-8efddaee9d75bcf4a9f33a3d891becb5_hd.jpg" data-caption="" data-size="normal" data-rawwidth="473" data-rawheight="83" class="origin_image zh-lightbox-thumb lazy" width="473" data-original="https://pic3.zhimg.com/v2-8efddaee9d75bcf4a9f33a3d891becb5_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-8efddaee9d75bcf4a9f33a3d891becb5_b.jpg"></figure><p>将上式简化可以写成如下形式：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-4d10beb82b17a72dc7c731548eec880c_b.jpg" data-caption="" data-size="normal" data-rawwidth="138" data-rawheight="60" class="content_image" width="138"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-4d10beb82b17a72dc7c731548eec880c_hd.jpg" data-caption="" data-size="normal" data-rawwidth="138" data-rawheight="60" class="content_image lazy" width="138" data-actualsrc="https://pic2.zhimg.com/v2-4d10beb82b17a72dc7c731548eec880c_b.jpg"></figure><p>其中 a_{in} 是输入给权重 w 的神经元的激活值，δ_{out} 是输出自权重 w 的神经元的误差。从上式很直观的可以看到，当激活值很小时，梯度也会很小，趋近于0，这样，我们就说权重缓慢学习，表示梯度下降的时候，这个权重改变不多；这样的情形我们称之为神经元已经饱和了，最终层的权重学习也会终止（或缓慢）。这种现象也称为梯度弥散（消失），这很不利于深度神经网络的学习。</p><hr><h2>3、激活函数</h2><p>激活函数的作用网上有很多帖子介绍，也不需我多言。总而言之，它能使得神经网络的每层输出结果变得非线性化，为什么神经网络的输出结果需要非线性化？因为只有如此，神经网络才能拟合任意函数（线性函数、非线性函数），<a href="https://www.zhihu.com/question/22334626" class="internal">点这里详细了解</a>；其实非线性化的作用还有这一点：保持每一层的输出具有“梯度”，只有有了“梯度”我们才能应用反向传播算法、梯度下降算法来优化代价函数（正如我们在前文看到的那样），从而训练出更深的神经网络。</p><p>在讲反向传播的时候我们已经说过，误差需要从输出层一层一层的传递到输入层的，然而在传递过程中你会发现梯度越来越小，甚至都没有梯度了（这种现象称为梯度弥散问题 <i>vanishing gradients </i>problem）；而又存在再一些相反的案例，比如循环神经网络中，梯度会越来越大，这样权重始终在更新，因而训练一直得不到收敛，这种现象称为梯度爆炸问题（explod<i>ing gradients </i>problem）。</p><p>最开始作为激活函数的函数是sigmod函数，如下图所示：</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/v2-d3d6983a63be8a2ba0b260e23e3bee68_b.jpg" data-caption="" data-size="normal" data-rawwidth="535" data-rawheight="326" class="origin_image zh-lightbox-thumb" width="535" data-original="https://pic3.zhimg.com/v2-d3d6983a63be8a2ba0b260e23e3bee68_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-d3d6983a63be8a2ba0b260e23e3bee68_hd.jpg" data-caption="" data-size="normal" data-rawwidth="535" data-rawheight="326" class="origin_image zh-lightbox-thumb lazy" width="535" data-original="https://pic3.zhimg.com/v2-d3d6983a63be8a2ba0b260e23e3bee68_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-d3d6983a63be8a2ba0b260e23e3bee68_b.jpg"></figure><p>从图中可以看到，当输入的值比较大时（负值或者正值），sigmod函数的导数趋近于0，也就是梯度饱和了。当反向传播kicks in时，它几乎没有梯度通过神经网络传播到输入层，即使有小梯度存在，也会不断的被稀释在顶层；所以到了低层时，已经没有什么梯度剩下了，也就是权重将不能得到更新。为了避免梯度小时或者爆炸，有人认为神经网络的每一层的输出的方差必须等于该层的输入值的方差（ we need the variance of the outputs of each layer to be equal to the variance of its inputs），于是他们针对sigmod激活函数发明了一种初始化权重的技术，具体公式如下：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-b867e77353360e0a93c85d9d7683b913_b.jpg" data-caption="" data-size="normal" data-rawwidth="659" data-rawheight="132" class="origin_image zh-lightbox-thumb" width="659" data-original="https://pic2.zhimg.com/v2-b867e77353360e0a93c85d9d7683b913_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-b867e77353360e0a93c85d9d7683b913_hd.jpg" data-caption="" data-size="normal" data-rawwidth="659" data-rawheight="132" class="origin_image zh-lightbox-thumb lazy" width="659" data-original="https://pic2.zhimg.com/v2-b867e77353360e0a93c85d9d7683b913_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-b867e77353360e0a93c85d9d7683b913_b.jpg"></figure><p>这个初始化技术称为  <i>Xavier initialization或者 Glorot initialization，</i><b>这能加速神经网络的训练速度，这也是导致当前训练深度神经网络成功的诀窍之一。</b></p><p><b>针对不同激活函数，不同论文也给出了它们相应的权重初始化策略：</b></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/v2-e54a3fc34175798d4dd6b8c7ceab6b70_b.jpg" data-caption="" data-size="normal" data-rawwidth="560" data-rawheight="243" class="origin_image zh-lightbox-thumb" width="560" data-original="https://pic3.zhimg.com/v2-e54a3fc34175798d4dd6b8c7ceab6b70_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-e54a3fc34175798d4dd6b8c7ceab6b70_hd.jpg" data-caption="" data-size="normal" data-rawwidth="560" data-rawheight="243" class="origin_image zh-lightbox-thumb lazy" width="560" data-original="https://pic3.zhimg.com/v2-e54a3fc34175798d4dd6b8c7ceab6b70_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-e54a3fc34175798d4dd6b8c7ceab6b70_b.jpg"></figure><p>在tensorflow中的全连接函数 fully_connected()中默认使用的是<i>Xavier initialization</i>，你也可以手动替换为 He initialization，通过使用 variance_scaling_initializer()函数：</p><div class="highlight"><pre><code class="language-python3"><span></span><span class="n">he_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">weights_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">"h1"</span><span class="p">)</span>
</code></pre></div><p>但是因为存在梯度饱和的问题，所以sigmod作为激活函数始终不怎么好，于是有了ReLU激活函数，它不存在饱和现象（当值为正数的时候），如下图红色虚线所示：</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/v2-9c55ebd7d1fd06db5b176f9184c3cd7e_b.jpg" data-caption="" data-size="normal" data-rawwidth="341" data-rawheight="240" class="content_image" width="341"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-9c55ebd7d1fd06db5b176f9184c3cd7e_hd.jpg" data-caption="" data-size="normal" data-rawwidth="341" data-rawheight="240" class="content_image lazy" width="341" data-actualsrc="https://pic3.zhimg.com/v2-9c55ebd7d1fd06db5b176f9184c3cd7e_b.jpg"></figure><p>然而，当输入为负值的时候神经网络的经过ReLU激活之后输出就变成0了，这一定程度上加速了神经网络的计算速度，但是却也使得一大半的神经元“死了”，因为这些神经元不输出任何值（只有0），而当学习率比较大时，你会发现有更多的神经元“死了”，这不是我们想要的结果。为了解决这个问题，于是有了LeakyReLU激活函数：</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-b11400dc9a6cb8051d3cbbb7de13e362_b.jpg" data-caption="" data-size="normal" data-rawwidth="257" data-rawheight="31" class="content_image" width="257"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-b11400dc9a6cb8051d3cbbb7de13e362_hd.jpg" data-caption="" data-size="normal" data-rawwidth="257" data-rawheight="31" class="content_image lazy" width="257" data-actualsrc="https://pic4.zhimg.com/v2-b11400dc9a6cb8051d3cbbb7de13e362_b.jpg"></figure><figure><noscript>&lt;img src="https://pic3.zhimg.com/v2-c8270560c16ee0c9c1fc9069f6b13ee5_b.jpg" data-caption="" data-size="normal" data-rawwidth="440" data-rawheight="258" class="origin_image zh-lightbox-thumb" width="440" data-original="https://pic3.zhimg.com/v2-c8270560c16ee0c9c1fc9069f6b13ee5_r.jpg"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-c8270560c16ee0c9c1fc9069f6b13ee5_hd.jpg" data-caption="" data-size="normal" data-rawwidth="440" data-rawheight="258" class="origin_image zh-lightbox-thumb lazy" width="440" data-original="https://pic3.zhimg.com/v2-c8270560c16ee0c9c1fc9069f6b13ee5_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-c8270560c16ee0c9c1fc9069f6b13ee5_b.jpg"></figure><p>当输入z小于0时不再直接输出0，而是输出 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(29)" alt="\alpha z" eeimg="1"> ，这里 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(30)" alt="\alpha" eeimg="1"> 通常设置为0.01，这样神经元就不会“死”，而有机会“复苏”起来。有人发现使用这个变体激活函数往往比原激活函数效果更好，而设置 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(31)" alt="\alpha = 0.2" eeimg="1"> （大的leaky）效果比设置为0.01更好。</p><p>有时，也可以不将 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(30)" alt="\alpha" eeimg="1"> 设置为一个固定的值，而是在训练神经网络时将其作为一个参数进行学习，这在大数据集上表现良好，然而对于小数据集却容易过拟合。</p><p><b>指数线性单元激活函数  <i>exponential linear unit </i>(ELU)</b></p><p>2015年， Djork-Arné Clevert等人提出这个激活函数，表示它表任何ReLU及其变体激活函数效果都要好，ELU具体定义如下：</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/v2-70c2143940263d230bd578b22ac6f8f6_b.jpg" data-caption="" data-size="normal" data-rawwidth="332" data-rawheight="63" class="content_image" width="332"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-70c2143940263d230bd578b22ac6f8f6_hd.jpg" data-caption="" data-size="normal" data-rawwidth="332" data-rawheight="63" class="content_image lazy" width="332" data-actualsrc="https://pic4.zhimg.com/v2-70c2143940263d230bd578b22ac6f8f6_b.jpg"></figure><figure><noscript>&lt;img src="https://pic2.zhimg.com/v2-746dfb0cf0d49ee84d6104ad955a2b83_b.jpg" data-caption="" data-size="normal" data-rawwidth="393" data-rawheight="249" class="content_image" width="393"&gt;</noscript><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-746dfb0cf0d49ee84d6104ad955a2b83_hd.jpg" data-caption="" data-size="normal" data-rawwidth="393" data-rawheight="249" class="content_image lazy" width="393" data-actualsrc="https://pic2.zhimg.com/v2-746dfb0cf0d49ee84d6104ad955a2b83_b.jpg"></figure><p>可以看到它的主要改变就是在输出值z&lt;0时，将其替换为一个系数为 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(30)" alt="\alpha" eeimg="1"> 的指数函数。当负值很大时，超参数 <img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/equation(30)" alt="\alpha" eeimg="1"> 一般设置为1，当然也可以在训练时通过调参去设置它。</p><p>ELU的缺点是计算度慢，但在训练过程中，更快的收敛速度补偿了这一点。</p><p>在训练神经网络需要选择激活函数时，你一般可以按照这个顺序取选择：ELU &gt; leaky ReLU (and its variants) &gt; ReLU &gt; tanh &gt; logistic。  </p><blockquote>If you care a lot about runtime performance, then you may prefer leaky ReLUs over ELUs. If you don’t want to tweak yet another hyperparameter, you may just use the default <i>α </i>values suggested earlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set.</blockquote><p>tensorflow里面有ELU激活函数。可以这样调用：</p><div class="highlight"><pre><code class="language-python3"><span></span><span class="n">hidden1</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">)</span>
</code></pre></div><p>你也可以自定义  leaky ReLUs激活函数：</p><div class="highlight"><pre><code class="language-python3"><span></span><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<span class="n">hidden1</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">)</span>
</code></pre></div><p>好了，本文到此结束，如有纰漏敬请斧正；另外希望在过年前我能写完Dropout，Softmax， Batch Normalization等深度学习基础概念的介绍。</p><h2>4、参考文献：</h2><blockquote>1、Aurélien Géron，《 Hands-On Machine Learning with Scikit-Learn and TensoFlow》<br>2、Michael Nielsen，《Neural Networks and Deep Learning》<br>3、Ian Goodfellow et. al，《Deep Learning》</blockquote><p>禁止转载。</p></div><div class="Tipjar"><div class="Tipjar-tagLine"><!-- react-text: 41 -->「<!-- /react-text --><!-- react-text: 42 -->真诚赞赏，手留余香<!-- /react-text --><!-- react-text: 43 -->」<!-- /react-text --></div><button class="Button Button Tipjar-button" type="button"><!-- react-text: 45 -->赞赏<!-- /react-text --></button><div class="Tipjar-stateDescription">还没有人赞赏，快来当第一个赞赏的人吧！</div><!-- react-empty: 47 --><!-- react-empty: 48 --></div><div class="PostIndex-footer"><div class="PostIndex-topics TopicItem-wrapper"><a class="TopicItem u-ellipsis PostIndex-topicItem" href="https://www.zhihu.com/topic/19813032"><!-- react-text: 52 -->深度学习（Deep Learning）<!-- /react-text --></a><a class="TopicItem u-ellipsis PostIndex-topicItem" href="https://www.zhihu.com/topic/19559450"><!-- react-text: 54 -->机器学习<!-- /react-text --></a><a class="TopicItem u-ellipsis PostIndex-topicItem" href="https://www.zhihu.com/topic/20032249"><!-- react-text: 56 -->TensorFlow<!-- /react-text --></a></div><div class="PostIndex-reviewers"></div><div class="PostIndex-vote"><button class="Button PostIndex-voteButton Button--green" aria-label="赞" type="button"><i class="icon icon-ic_column_like"></i><!-- react-text: 61 -->81<!-- /react-text --></button><div class="PostIndex-voters"><div class="HoverTitle HoverTitle--slim" data-hover-title="BearLin"><a href="https://www.zhihu.com/people/lin-guo-sheng-97" class="PostIndex-voter" target="_blank"><img class="Avatar-hemingway Avatar--is" alt="BearLin" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-f29e42630818810e2852028bc75852a3_is.jpg" srcset="https://pic3.zhimg.com/v2-f29e42630818810e2852028bc75852a3_im.jpg 2x"></a></div><div class="HoverTitle HoverTitle--slim" data-hover-title="方恨水"><a href="https://www.zhihu.com/people/anakin_tatooine" class="PostIndex-voter" target="_blank"><img class="Avatar-hemingway Avatar--is" alt="方恨水" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/c58a09446c06989c3f98668e034696ec_is.jpg" srcset="https://pic2.zhimg.com/c58a09446c06989c3f98668e034696ec_im.jpg 2x"></a></div><div class="HoverTitle HoverTitle--slim" data-hover-title="星澜海"><a href="https://www.zhihu.com/people/wang-wei-77-36" class="PostIndex-voter" target="_blank"><img class="Avatar-hemingway Avatar--is" alt="星澜海" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/fa59702d0fac718e41f820b22d27effe_is.jpg" srcset="https://pic4.zhimg.com/fa59702d0fac718e41f820b22d27effe_im.jpg 2x"></a></div><div class="HoverTitle HoverTitle--slim" data-hover-title="王思"><a href="https://www.zhihu.com/people/wang-si-76-53" class="PostIndex-voter" target="_blank"><img class="Avatar-hemingway Avatar--is" alt="王思" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/a05dab5d5e204d8176df3b1ba204081b_is.jpg" srcset="https://pic1.zhimg.com/a05dab5d5e204d8176df3b1ba204081b_im.jpg 2x"></a></div><div class="HoverTitle HoverTitle--slim" data-hover-title="刘旭松"><a href="https://www.zhihu.com/people/liu-xu-song-40" class="PostIndex-voter" target="_blank"><img class="Avatar-hemingway Avatar--is" alt="刘旭松" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/e5294395d5fa49c26cd5846b5053fff2_is.jpg" srcset="https://pic3.zhimg.com/e5294395d5fa49c26cd5846b5053fff2_im.jpg 2x"></a></div><a href="https://zhuanlan.zhihu.com/p/32714733/voters" target="_blank" rel="noopener noreferrer" title="查看全部" class="PostIndex-allVoters"><i class="icon-ic_like_more"></i></a></div></div><div class="PostIndex-control"><div class="Fav"><button class="Button Button Button--plain FavButton" type="button"><i class="icon icon-ic_collect"></i><!-- react-text: 84 -->收藏<!-- /react-text --></button><!-- react-empty: 85 --></div><div class="PostShare"><div class="Menu"><button class="Button Button Button--plain MenuButton MenuButton-listen-hover Button Button--plain" type="button"><i class="icon icon-ic_column_share"></i><!-- react-text: 90 -->分享<!-- /react-text --></button><div class="Menu-dropdown" style="visibility: hidden;"></div></div></div><div class="Report"><button class="Button Button Button--plain ReportButton" type="button"><i class="icon icon-ic_column_report"></i><!-- react-text: 95 -->举报<!-- /react-text --></button><!-- react-empty: 96 --></div></div></div><div class="Contributes PostIndex-contributes av-card"><div class="BlockTitle av-marginLeft av-borderColor"><span class="BlockTitle-title">文章被以下专栏收录</span><span class="BlockTitle-line"></span></div><ul class="Contributes-list"><li class="Contributes-listItem av-borderColor av-marginLeft" style="opacity: 1; max-height: 300px;"><div class="ContributesItem av-paddingRight" role="link"><a class="ContributesItem-avatar" href="https://zhuanlan.zhihu.com/leemoo"><img class="Avatar-hemingway" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-8456af8026d9b1d75719eab1971a3662_m.jpg" srcset="https://pic2.zhimg.com/v2-8456af8026d9b1d75719eab1971a3662_xl.jpg 2x"></a><div class="ContributesItem-info"><div class="ContributesItem-nameLine"><a class="ContributesItem-name" href="https://zhuanlan.zhihu.com/leemoo">每天都要机器学习哦</a></div><p class="ContributesItem-intro u-ellipsis">记录ML和NLP的学习之路</p></div><a class="ContributesItem-entrance" href="https://zhuanlan.zhihu.com/leemoo">进入专栏</a></div></li></ul></div><div class="PostComment"><div class="BlockTitle av-marginLeft av-borderColor PostComment-blockTitle"><span class="BlockTitle-title"><!-- react-text: 114 -->4 条评论<!-- /react-text --></span><span class="BlockTitle-line"></span></div><div class="CommentEditor PostComment-mainEditor"><img class="Avatar-hemingway CommentEditor-avatar Avatar--xs" alt="sike" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/e519a5bfbe9e58ce6098cf1d858dc7d0_xs.jpg" srcset="https://pic4.zhimg.com/e519a5bfbe9e58ce6098cf1d858dc7d0_l.jpg 2x"><div class="CommentEditor-input"><div class="Input-wrapper Input-wrapper--spread Input-wrapper--large Input-wrapper--noPadding"><div class="Input Editable"><div class="Dropzone RichText" style="min-height: 38px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-39uul">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-39uul" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; white-space: pre-wrap; word-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="39uul" data-offset-key="34sjd-0-0"><div data-offset-key="34sjd-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="34sjd-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input type="file" multiple="" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><!-- react-empty: 346 --><!-- react-empty: 347 --><div></div><!-- react-empty: 349 --><!-- react-empty: 350 --><!-- react-empty: 351 --></div></div><div class="CommentEditor-actions"><button class="Button Button--plain" type="button"><!-- react-text: 122 -->取消<!-- /react-text --></button><button class="Button Button--blue" disabled="" type="button"><!-- react-text: 124 -->评论<!-- /react-text --></button></div></div></div><div class="PostCommentList"><div class="CommentItem"><a class="UserAvatar CommentItem-author" href="https://www.zhihu.com/people/chen-yan-jun-88-34" target="_blank"><img class="Avatar-hemingway Avatar--xs" alt="卿卿" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-0401711e046672ab6361c2a34e7e1104_xs.jpg" srcset="https://pic2.zhimg.com/v2-0401711e046672ab6361c2a34e7e1104_l.jpg 2x"></a><div class="CommentItem-headWrapper"><div class="CommentItem-head"><span class="CommentItem-context"><a href="https://www.zhihu.com/people/chen-yan-jun-88-34" class="" target="_blank">卿卿</a></span></div></div><div class="CommentItem-content">深度学习？</div><div class="CommentItem-foot"><span class="CommentItem-like CommentItem-like--empty" title="0 人觉得这个很赞"><!-- react-text: 224 -->0<!-- /react-text --><!-- react-text: 225 --> 赞<!-- /react-text --></span><div class="HoverTitle CommentItem-createdTime" data-hover-title="2018 年 1月 9 日星期二下午 2 点 22 分"><time datetime="Tue Jan 09 2018 14:22:42 GMT+0800 (中国标准时间)">4 天前</time></div><button class="Button CommentItem-action CommentItem-actionReply Button--plain" type="button"><i class="icon icon-ic_column_reply"></i><!-- react-text: 230 -->回复<!-- /react-text --></button><button class="Button CommentItem-action CommentItem-actionLike Button--plain" type="button"><i class="icon icon-ic_comment_like"></i><!-- react-text: 233 -->赞<!-- /react-text --></button><div class="Report CommentItem-action CommentItem-actionReport"><button class="Button Button Button--plain ReportButton" type="button"><i class="icon icon-ic_column_report"></i><!-- react-text: 237 -->举报<!-- /react-text --></button><!-- react-empty: 238 --></div></div><!-- react-empty: 239 --></div><div class="CommentItem"><a class="UserAvatar CommentItem-author" href="https://www.zhihu.com/people/wangle-nlp" target="_blank"><img class="Avatar-hemingway Avatar--xs" alt="王乐" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-5a76beb4edb4fee8e8b6026394b47368_xs.jpg" srcset="https://pic1.zhimg.com/v2-5a76beb4edb4fee8e8b6026394b47368_l.jpg 2x"></a><div class="CommentItem-headWrapper"><button class="Button CommentItem-conversationButton Button--plain" type="button"><i class="icon icon-ic_conversations"></i><!-- react-text: 246 --> 查看对话<!-- /react-text --></button><div class="CommentItem-head CommentItem-head--rightPad"><span class="CommentItem-context"><a href="https://www.zhihu.com/people/wangle-nlp" class="" target="_blank">王乐</a><span class="CommentItem-authorTitle">（作者）</span><span class="CommentItem-replyTo"><span class="CommentItem-replySplit">回复</span><a href="https://www.zhihu.com/people/chen-yan-jun-88-34" class="" target="_blank">卿卿</a></span></span></div></div><div class="CommentItem-content">嗯</div><div class="CommentItem-foot"><span class="CommentItem-like CommentItem-like--empty" title="0 人觉得这个很赞"><!-- react-text: 257 -->0<!-- /react-text --><!-- react-text: 258 --> 赞<!-- /react-text --></span><div class="HoverTitle CommentItem-createdTime" data-hover-title="2018 年 1月 9 日星期二下午 3 点 02 分"><time datetime="Tue Jan 09 2018 15:02:01 GMT+0800 (中国标准时间)">4 天前</time></div><button class="Button CommentItem-action CommentItem-actionReply Button--plain" type="button"><i class="icon icon-ic_column_reply"></i><!-- react-text: 263 -->回复<!-- /react-text --></button><button class="Button CommentItem-action CommentItem-actionLike Button--plain" type="button"><i class="icon icon-ic_comment_like"></i><!-- react-text: 266 -->赞<!-- /react-text --></button><div class="Report CommentItem-action CommentItem-actionReport"><button class="Button Button Button--plain ReportButton" type="button"><i class="icon icon-ic_column_report"></i><!-- react-text: 270 -->举报<!-- /react-text --></button><!-- react-empty: 271 --></div></div><!-- react-empty: 272 --></div><div class="CommentItem"><a class="UserAvatar CommentItem-author" href="https://www.zhihu.com/people/xu-yang-19-50" target="_blank"><img class="Avatar-hemingway Avatar--xs" alt="骄阳" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/d85bfe1a567f2e5a0e944063c8244c5a_xs.jpg" srcset="https://pic2.zhimg.com/d85bfe1a567f2e5a0e944063c8244c5a_l.jpg 2x"></a><div class="CommentItem-headWrapper"><div class="CommentItem-head"><span class="CommentItem-context"><a href="https://www.zhihu.com/people/xu-yang-19-50" class="" target="_blank">骄阳</a></span></div></div><div class="CommentItem-content">总结至《神经网络与深度学习》?~</div><div class="CommentItem-foot"><span class="CommentItem-like CommentItem-like--empty" title="0 人觉得这个很赞"><!-- react-text: 283 -->0<!-- /react-text --><!-- react-text: 284 --> 赞<!-- /react-text --></span><div class="HoverTitle CommentItem-createdTime" data-hover-title="2018 年 1月 10 日星期三晚上 11 点 08 分"><time datetime="Wed Jan 10 2018 23:08:02 GMT+0800 (中国标准时间)">3 天前</time></div><button class="Button CommentItem-action CommentItem-actionReply Button--plain" type="button"><i class="icon icon-ic_column_reply"></i><!-- react-text: 289 -->回复<!-- /react-text --></button><button class="Button CommentItem-action CommentItem-actionLike Button--plain" type="button"><i class="icon icon-ic_comment_like"></i><!-- react-text: 292 -->赞<!-- /react-text --></button><div class="Report CommentItem-action CommentItem-actionReport"><button class="Button Button Button--plain ReportButton" type="button"><i class="icon icon-ic_column_report"></i><!-- react-text: 296 -->举报<!-- /react-text --></button><!-- react-empty: 297 --></div></div><!-- react-empty: 298 --></div><div class="CommentItem"><a class="UserAvatar CommentItem-author" href="https://www.zhihu.com/people/wangle-nlp" target="_blank"><img class="Avatar-hemingway Avatar--xs" alt="王乐" src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-5a76beb4edb4fee8e8b6026394b47368_xs.jpg" srcset="https://pic1.zhimg.com/v2-5a76beb4edb4fee8e8b6026394b47368_l.jpg 2x"></a><div class="CommentItem-headWrapper"><button class="Button CommentItem-conversationButton Button--plain" type="button"><i class="icon icon-ic_conversations"></i><!-- react-text: 305 --> 查看对话<!-- /react-text --></button><div class="CommentItem-head CommentItem-head--rightPad"><span class="CommentItem-context"><a href="https://www.zhihu.com/people/wangle-nlp" class="" target="_blank">王乐</a><span class="CommentItem-authorTitle">（作者）</span><span class="CommentItem-replyTo"><span class="CommentItem-replySplit">回复</span><a href="https://www.zhihu.com/people/xu-yang-19-50" class="" target="_blank">骄阳</a></span></span></div></div><div class="CommentItem-content">后面的三本参考书</div><div class="CommentItem-foot"><span class="CommentItem-like CommentItem-like--empty" title="0 人觉得这个很赞"><!-- react-text: 316 -->0<!-- /react-text --><!-- react-text: 317 --> 赞<!-- /react-text --></span><div class="HoverTitle CommentItem-createdTime" data-hover-title="2018 年 1月 10 日星期三晚上 11 点 53 分"><time datetime="Wed Jan 10 2018 23:53:27 GMT+0800 (中国标准时间)">3 天前</time></div><button class="Button CommentItem-action CommentItem-actionReply Button--plain" type="button"><i class="icon icon-ic_column_reply"></i><!-- react-text: 322 -->回复<!-- /react-text --></button><button class="Button CommentItem-action CommentItem-actionLike Button--plain" type="button"><i class="icon icon-ic_comment_like"></i><!-- react-text: 325 -->赞<!-- /react-text --></button><div class="Report CommentItem-action CommentItem-actionReport"><button class="Button Button Button--plain ReportButton" type="button"><i class="icon icon-ic_column_report"></i><!-- react-text: 329 -->举报<!-- /react-text --></button><!-- react-empty: 330 --></div></div><!-- react-empty: 331 --></div></div><!-- react-empty: 126 --><!-- react-empty: 127 --></div><div class="PostIndex-recommendZone av-card"><div class="BlockTitle av-marginLeft av-borderColor"><span class="BlockTitle-title">推荐阅读</span><span class="BlockTitle-line"></span></div><ul class="PostIndex-recommends"><li class="PostIndex-recommendItem av-marginLeft av-borderColor"><div class="PostListItem PostListItem--narrow av-paddingRight"><!-- react-text: 142 --><!-- /react-text --><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/32822550"><span class="PostListItem-title">分析了558个知乎专栏，竟然让我发现这样的事</span><p class="PostListItem-summary"><!-- react-text: 147 -->灵机一动，于是爬了机器学习、自然语言处理、深度学习相关的558个知乎专栏，经过一番操作，竟然让我发现这里这些事！！1、专栏发文数量大部分专栏的文章数量在10篇以下，达418个，占比75%；…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 149 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" target="_blank" href="https://www.zhihu.com/people/wangle-nlp">王乐</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2018 年 1月 10 日星期三晚上 11 点 46 分"><time datetime="2018-01-10T23:46:04+08:00">3 天前</time></div></span></div></div></div></li><li class="PostIndex-recommendItem av-marginLeft av-borderColor"><div class="PostListItem PostListItem--narrow av-paddingRight"><!-- react-text: 159 --><!-- /react-text --><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/32354021"><span class="PostListItem-title">关于机器学习（数据科学）竞赛的一些小思考</span><p class="PostListItem-summary"><!-- react-text: 164 -->十一月底到十二月中旬的这一段时间参加了京东举办的京东金融信贷需求预测竞赛，致使那段时间我都没有好好准备考六级：），当然有可能好好准备了还是过不了的：）；虽然六级没有好好准备，但…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 166 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" target="_blank" href="https://www.zhihu.com/people/wangle-nlp">王乐</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 12月 26 日星期二下午 4 点 43 分"><time datetime="2017-12-26T16:43:21+08:00">18 天前</time></div></span></div></div></div></li><li class="PostIndex-recommendItem av-marginLeft av-borderColor"><div class="PostListItem PostListItem--narrow av-paddingRight"><a class="PostListItem-titleImageWrapper" href="https://zhuanlan.zhihu.com/p/22142013"><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/ffa57442611ce65a1665de8e844de768_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22142013"><span class="PostListItem-title">深度学习中的激活函数导引</span><p class="PostListItem-summary"><!-- react-text: 182 -->深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 184 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" target="_blank" href="https://www.zhihu.com/people/cheng-cheng-65-1-30">程程</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 23 日星期二下午 3 点 57 分"><time datetime="2016-08-23T15:57:13+08:00">1 年前</time></div><span class="PostListItem-source"><span class="Bull"></span><span class="PostListItem-sourcePrefix">发表于 </span><a title="深度学习大讲堂" href="https://zhuanlan.zhihu.com/dlclass">深度学习大讲堂</a></span></span></div></div></div></li><li class="PostIndex-recommendItem av-marginLeft av-borderColor"><div class="PostListItem PostListItem--narrow av-paddingRight"><a class="PostListItem-titleImageWrapper" href="https://zhuanlan.zhihu.com/p/30567264"><img src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/v2-a7f872a80b7223ca5bb7690497ab2239_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/30567264"><span class="PostListItem-title">你说说激活函数？</span><p class="PostListItem-summary"><!-- react-text: 204 -->说说你想项目中的激活函数，为啥选这个?有没有考虑过其他的激活函数？效果怎样？除此之外你…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 206 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" target="_blank" href="https://www.zhihu.com/people/li-li-31-41-67">李理</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 10月 30 日星期一中午 11 点 41 分"><time datetime="2017-10-30T11:41:36+08:00">2 个月前</time></div></span></div></div></div></li></ul></div><!-- react-empty: 134 --><!-- react-empty: 128 --><!-- react-empty: 129 --></div></div><!-- react-empty: 130 --><!-- react-empty: 131 --><!-- react-empty: 132 --></div></div><textarea id="clientConfig" hidden="">{"debug":false,"apiRoot":"","paySDK":"https:\u002F\u002Fpay.zhihu.com\u002Fapi\u002Fjs","wechatConfigAPI":"\u002Fapi\u002Fwechat\u002Fjssdkconfig","name":"production","instance":"column","tokens":{"X-XSRF-TOKEN":null,"X-UDID":"\"AFCi4xvE-wyPThZXndLF3_xulXtLnKv5Cq4=|1515834163\"","Authorization":["\"2","1:0","10:1515834369","4:z_c0","92:Mi4xNHhMdkFRQUFBQUFBVUtMakc4VDdEQ2NBQUFDRUFsVk5BVm1CV2dBWVVuTlEwRGVZd0NmQTFWRnR1aHphOG85c0tn","281b8b271fcd7c903ff070797c4206ff9dc170f4e0efe590c7ae32ab662bddc5\""]}}</textarea><textarea id="preloadedState" hidden="">{"database":{"Post":{"32714733":{"isPending":false,"contributes":[{"sourceColumn":{"lastUpdated":1515038760,"description":"","permission":"COLUMN_PUBLIC","memberId":8416570,"contributePermission":"COLUMN_PUBLIC","translatedCommentPermission":"all","canManage":true,"intro":"记录ML和NLP的学习之路","urlToken":"leemoo","id":66046,"imagePath":"v2-8456af8026d9b1d75719eab1971a3662.jpg","slug":"leemoo","applyReason":"0","name":"每天都要机器学习哦","title":"每天都要机器学习哦","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fleemoo","commentPermission":"COLUMN_ALL_CAN_COMMENT","canPost":true,"created":1507449667,"state":"COLUMN_NORMAL","followers":1658,"avatar":{"id":"v2-8456af8026d9b1d75719eab1971a3662","template":"https:\u002F\u002Fpic2.zhimg.com\u002F{id}_{size}.jpg"},"activateAuthorRequested":false,"following":false,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8456af8026d9b1d75719eab1971a3662_l.jpg","articlesCount":19},"state":"accepted","targetPost":{"titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5022c810d5368fc75d86d7c2b2dc0860_r.jpg","lastUpdated":1515405248,"imagePath":"v2-5022c810d5368fc75d86d7c2b2dc0860.jpg","permission":"ARTICLE_PUBLIC","topics":[89794,3084,162939],"summary":"在入门深度学习时，梯度下降、反向传播、激活函数这三个概念是绕不过的知识点，如果不能好好理解这些点那么深度学习可能就入不了门；如果不能好好的将这些点联系起来，我觉得对深度神经网络的理解也会很迷惑。网上介绍这些概念的文章有很多，但是往往都是单…","copyPermission":"ARTICLE_COPYABLE","translatedCommentPermission":"all","likes":0,"origAuthorId":0,"publishedTime":"2018-01-08T17:54:08+08:00","sourceUrl":"","urlToken":32714733,"id":5275206,"withContent":false,"slug":32714733,"bigTitleImage":false,"title":"深度学习的前戏--梯度下降、反向传播、激活函数","url":"\u002Fp\u002F32714733","commentPermission":"ARTICLE_ALL_CAN_COMMENT","snapshotUrl":"","created":1515405248,"comments":0,"columnId":66046,"content":"","parentId":0,"state":"ARTICLE_PUBLISHED","imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5022c810d5368fc75d86d7c2b2dc0860_r.jpg","author":{"bio":"计算机硕士在读","isFollowing":true,"hash":"1b7326a564c30474efbc1f85c5af40a9","uid":61977426657280,"isOrg":false,"slug":"wangle-nlp","isFollowed":false,"description":"要心平气和、不要争执","name":"王乐","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fwangle-nlp","avatar":{"id":"v2-5a76beb4edb4fee8e8b6026394b47368","template":"https:\u002F\u002Fpic1.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},"memberId":8416570,"excerptTitle":"","voteType":"ARTICLE_VOTE_CLEAR"},"id":1032414}],"title":"深度学习的前戏--梯度下降、反向传播、激活函数","author":"wangle-nlp","content":"\u003Cp\u003E在入门深度学习时，梯度下降、反向传播、激活函数这三个概念是绕不过的知识点，如果不能好好理解这些点那么深度学习可能就入不了门；如果不能好好的将这些点联系起来，我觉得对深度神经网络的理解也会很迷惑。网上介绍这些概念的文章有很多，但是往往都是单独介绍的，因为每一个概念要介绍起来到需要很多笔墨；然而为了更好的理解这些概念，为了知其然并且知其所以然，我觉得有必要将它们串起来讲一讲。\u003C\u002Fp\u003E\u003Cp\u003E为什么说它们是深度学习的前戏呢，只因为它们是深度学习的基础之中的基础概念。在我看来，深度学习包含两方面内容：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E更好的训练深度神经网络。神经网络隐藏层超过两层就算深度神经网络，三层的NN的训练还好说，但是如果NN很多层数呢？那将会面临梯度弥散和梯度爆炸等问题。所以为了让训练的DNN取得好的效果，就有了一些训练DNN的技巧，比如反向传播算法、激活函数、批量归一化、dropout等技术的发明；而梯度下降是为了更好的优化代价函数，不管是机器学习还是深度学习，总会需要优化代价函数（损失函数）。\u003C\u002Fli\u003E\u003Cli\u003E设计网络结构以更好的提取特征。增加神经网络隐藏层就能提取更高层次特征，卷积神经网络能提取空间上的特征，循环神经网络能够提取时间序列特征，等等；于是各种网络结构被发明出来，比如AlexNet，LeNet，GooleNet，Inception系列，ResNet等等，另外还有LSTM等等。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E网络结构再美，如果不能训练到收敛，就是不work。所以我们今天介绍的这些技术就是为了更好的训练DNN，它们是保证能够训练好的DNN的基础，所以它们叫深度学习的前戏！！\u003C\u002Fp\u003E\u003Chr\u003E\u003Ch2\u003E1、梯度下降\u003C\u002Fh2\u003E\u003Cp\u003E假设有输入 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bx_1%2Cx_2%2C...%2Cx_n%5C%7D\" alt=\"\\{x_1,x_2,...,x_n\\}\" eeimg=\"1\"\u003E ，对应的输出为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7By_1%2Cy_2%2C..%2Cy_n%5C%7D\" alt=\"\\{y_1,y_2,..,y_n\\}\" eeimg=\"1\"\u003E ，我们希望神经网络的输出f(x)可以拟合所有训练输入xi，为此，我们需要定义一个代价函数：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=C%28v_1%2Cv_2%29%5Cequiv+%5Cfrac%7B1%7D%7B2n%7D%5Csum_%7Bi%7D%5E%7Bn%7D%7B%28f%28x_i%29-y_i%29%5E2%7D\" alt=\"C(v_1,v_2)\\equiv \\frac{1}{2n}\\sum_{i}^{n}{(f(x_i)-y_i)^2}\" eeimg=\"1\"\u003E \u003C\u002Fp\u003E\u003Cp\u003E要找到一组合适的参数（v1,v2）最小化上述代价函数，只要用微积分的知识解出上述代价函数右边部分的极值点就行了，也就是求导就够了；然而求导的方法在参数较少的时候行的通，但是参数数量一旦多了就不好办了。正好，深度学习中神经网络参数动辄几百万几千万个参数，所以直接计算倒数求极值行不通。\u003C\u002Fp\u003E\u003Cp\u003E为了解决这个问题，我们以上述例子来介绍一下梯度下降算法是如何求得极值的。\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2db5731c0563357c3205c0d1bc05bb08_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"482\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb\" width=\"482\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2db5731c0563357c3205c0d1bc05bb08_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='482'%20height='342'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"482\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"482\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2db5731c0563357c3205c0d1bc05bb08_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2db5731c0563357c3205c0d1bc05bb08_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E如上图所示，首先我们初始化v1,v2，假如现在（v1,v2）的取值如上图小球所在的位置，我们要做的就是寻找最佳v1,v2的取值使得代价函数最小，也就是使上图上的小球从山坡上移动到谷底。这里有两个方向v1,v2，也就是两个变量，想象一下小球分别往两个方向移动很小的量，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+v_1%2C%5CDelta+v_2\" alt=\"\\Delta v_1,\\Delta v_2\" eeimg=\"1\"\u003E ,那么小球移动的大小将为：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+C%5Capprox+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+v_1%7D%5CDelta+v_1+%2B+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+v_2%7D%5CDelta+v_2\" alt=\"\\Delta C\\approx \\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2}\\Delta v_2\" eeimg=\"1\"\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+v_1%7D\" alt=\"\\frac{\\partial C}{\\partial v_1}\" eeimg=\"1\"\u003E 表示函数C对变量v1的偏导数，也就是代价函数在v1上的变化速率，乘以变量的变化量就是代价函数自身的变化量了。\u003C\u002Fp\u003E\u003Cp\u003E定义倒三角形C为梯度向量，即：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f21461ba8f1b6b914210086467d7c749_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"67\" class=\"content_image\" width=\"202\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='202'%20height='67'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"67\" class=\"content_image lazy\" width=\"202\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f21461ba8f1b6b914210086467d7c749_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E那么小球移动的量可以表示为：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2445bfa0ff00e95eb688425b08542b22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"151\" data-rawheight=\"32\" class=\"content_image\" width=\"151\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='151'%20height='32'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"151\" data-rawheight=\"32\" class=\"content_image lazy\" width=\"151\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2445bfa0ff00e95eb688425b08542b22_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E为了使得 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+C\" alt=\"\\Delta C\" eeimg=\"1\"\u003E 为负数，也就是C能够逐渐变小，我们可以取\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9a1c82722d2cc5da6a54904f93e29ddc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"128\" data-rawheight=\"28\" class=\"content_image\" width=\"128\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='128'%20height='28'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"128\" data-rawheight=\"28\" class=\"content_image lazy\" width=\"128\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9a1c82722d2cc5da6a54904f93e29ddc_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这里 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"\u003E 称为学习率，一般是一个很小的正数，于是有了\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ec183811ed393478b409beeba8103a8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"30\" class=\"content_image\" width=\"257\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='257'%20height='30'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"30\" class=\"content_image lazy\" width=\"257\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ec183811ed393478b409beeba8103a8a_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样保证了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+C\" alt=\"\\Delta C\" eeimg=\"1\"\u003E 为负数。因此我们得到了变量v的变动方式了：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-90789d1af40ae9346593e0de29f195c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"30\" class=\"content_image\" width=\"171\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='171'%20height='30'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"30\" class=\"content_image lazy\" width=\"171\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-90789d1af40ae9346593e0de29f195c1_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E总结起来，梯度下降算法的工作方式就是重复计算梯度，然后沿着相反的方向移动，使得小球沿着山谷“滚动”。\u003C\u002Fp\u003E\u003Cp\u003E由于考虑到梯度下降算法的性能各方面因素，后来又有了随机梯度下降算法等；但是我这里不是介绍优化算法有什么，而是告诉大家，\u003Cb\u003E在优化代价函数的时候需要计算“梯度”，也就是代价函数必须可导，且代价函数在变量（参数）上的导数不能为0，不然就不能通过改变变量来优化代价函数了。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Chr\u003E\u003Ch2\u003E2、反向传播\u003C\u002Fh2\u003E\u003Cp\u003E回忆一下线性回归， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y+%3D+f%28%5Ctheta%3Bx%29\" alt=\"y = f(\\theta;x)\" eeimg=\"1\"\u003E ，x是输入，y是输出， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u003E 是参数，梯度下降算法就是用来求得最优参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u003E 的。在这里最初始的输入x和最终的输出y是直接关联的，如果将线性回归看做一个神经网络的话，那这个网络就只有输入层和输出层，而没有隐藏层。在深度神经网络中，隐藏层可能有多层，那么它的初始输入和最终输出是如何关联的呢？怎么应用梯度下降到神经网络中呢？\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d24339c0b303f6e1a0100492a82f2cd0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"281\" class=\"origin_image zh-lightbox-thumb\" width=\"499\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d24339c0b303f6e1a0100492a82f2cd0_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='499'%20height='281'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"281\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"499\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d24339c0b303f6e1a0100492a82f2cd0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d24339c0b303f6e1a0100492a82f2cd0_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\u003Cp\u003E如上图所示，该网络有两个隐藏层，每个隐藏层 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=layer_i\" alt=\"layer_i\" eeimg=\"1\"\u003E 的输入其实就是上一层 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=layer_%7Bi-1%7D\" alt=\"layer_{i-1}\" eeimg=\"1\"\u003E 的输出，而它的输出又是下一层 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=layer_%7Bi%2B1%7D\" alt=\"layer_{i+1}\" eeimg=\"1\"\u003E 的输入；反向传播的思想其实就是，对于每一个训练实例，将它传入神经网络，计算它的输出；然后测量网络的输出误差（即期望输出和实际输出之间的差异），并\u003Cb\u003E计算出上一个隐藏层中各神经元为该输出结果贡献了多少的误差\u003C\u002Fb\u003E；反复一直从后一层计算到前一层，直到算法到达初始的输入层为止。此反向传递过程有效地测量网络中所有连接权重的误差梯度，最后\u003Cb\u003E通过在每一个隐藏层中应用梯度下降算法来优化该层的参数\u003C\u002Fb\u003E（反向传播算法的名称也因此而来）。\u003C\u002Fp\u003E\u003Cp\u003E上面中文看不懂没关系，可以参考一下英文“ for each training instance the backpropagation algorithm first makes a prediction (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally slightly tweaks the connection weights to reduce the error (Gradient Descent step).”\u003C\u002Fp\u003E\u003Cp\u003E上面的描述仅仅是反向传播算法的思想，工作原理；那么具体反向传播算法是怎样计算出每个神经元的误差，并且将这个误差关联到“梯度”上的呢？有兴趣的可以看看下面的数学描述，没兴趣的可以略过……\u003C\u002Fp\u003E\u003Cp\u003E引入一个中间量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta+_j%5E%7Bl%7D\" alt=\"\\delta _j^{l}\" eeimg=\"1\"\u003E ，将其称为神经网络中在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l%5E%7Bth%7D\" alt=\"l^{th}\" eeimg=\"1\"\u003E 层第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j%5E%7Bth%7D\" alt=\"j^{th}\" eeimg=\"1\"\u003E 个神经元上的误差。如下图所示：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e87901a4f6c43398b747034ef0bd288b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"609\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"609\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e87901a4f6c43398b747034ef0bd288b_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='609'%20height='306'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"609\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"609\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e87901a4f6c43398b747034ef0bd288b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e87901a4f6c43398b747034ef0bd288b_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E反向传播将给出计算误差 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta+_j%5E%7Bl%7D\" alt=\"\\delta _j^{l}\" eeimg=\"1\"\u003E 的流程，然后将其关联到“梯度”（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpartial+C%2F%5Cpartial+w_%7Bjk%7D%5El%2C%5Cpartial+C%2F%5Cpartial+b_%7Bj%7D%5El\" alt=\"\\partial C\u002F\\partial w_{jk}^l,\\partial C\u002F\\partial b_{j}^l\" eeimg=\"1\"\u003E ）的计算上。\u003C\u002Fp\u003E\u003Cp\u003E想象一下，当每一层的输入进入到该层的每一个神经元，由于前后两层的神经元之间是由权重w连接的，而这个权重在最开始是由我们人为随机设置的，肯定不是最优的权重，所以必然会给上一层的输出带来一个误差，我们将这个误差记为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+z_j%5El\" alt=\"\\Delta z_j^l\" eeimg=\"1\"\u003E ，它是的神经元的输出从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma+%28z_j%5El%29\" alt=\"\\sigma (z_j^l)\" eeimg=\"1\"\u003E 变成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%28%5CDelta+z_j%5El+%2Bz_j%5El%29\" alt=\"\\sigma(\\Delta z_j^l +z_j^l)\" eeimg=\"1\"\u003E ，这个变化会向网络后面的层进行传播，最终导致整个代价产生 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5El%7D%5CDelta+z_j%5El\" alt=\"\\frac{\\partial C}{\\partial z_j^l}\\Delta z_j^l\" eeimg=\"1\"\u003E 的改变。我们的梯度下降算法正是用来优化代价的，为了使得 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+z_j%5El\" alt=\"\\Delta z_j^l\" eeimg=\"1\"\u003E 更小，假设 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5El%7D\" alt=\"\\frac{\\partial C}{\\partial z_j^l}\" eeimg=\"1\"\u003E 有一个很大的值（或正或负），那么梯度下降将会选择与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5El%7D\" alt=\"\\frac{\\partial C}{\\partial z_j^l}\" eeimg=\"1\"\u003E 符号相反的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+z_j%5El\" alt=\"\\Delta z_j^l\" eeimg=\"1\"\u003E 来降低代价。而如果 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5El%7D\" alt=\"\\frac{\\partial C}{\\partial z_j^l}\" eeimg=\"1\"\u003E 接近0，那么无论如何也不能优化代价函数了。因此，我们直觉的认为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5El%7D\" alt=\"\\frac{\\partial C}{\\partial z_j^l}\" eeimg=\"1\"\u003E 是神经元误差的度量。因此，我们有：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4d70a9281504497f472e0e24cd13a3ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"112\" data-rawheight=\"68\" class=\"content_image\" width=\"112\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='112'%20height='68'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"112\" data-rawheight=\"68\" class=\"content_image lazy\" width=\"112\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4d70a9281504497f472e0e24cd13a3ba_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E 我们使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta+%5El\" alt=\"\\delta ^l\" eeimg=\"1\"\u003E 表示关联于 l 层的误差向量。\u003C\u002Fp\u003E\u003Cp\u003E由于我们每一层神经网络的输出和输入之间使用了激活函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"\u003E（这里提到激活函数，先不管为什么有激活函数，后面再讲） ，我们使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_j+%5El\" alt=\"a_j ^l\" eeimg=\"1\"\u003E 表示 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z_j%5El\" alt=\"z_j^l\" eeimg=\"1\"\u003E 的激活值，那么我们可以使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_j%5El%7D\" alt=\"\\frac{\\partial C}{\\partial a_j^l}\" eeimg=\"1\"\u003E 作为度量误差的方法。\u003C\u002Fp\u003E\u003Cp\u003E有了以上认识之后，我们来描述反向传播算法：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E计算输出层误差的方程\u003C\u002Fb\u003E，δ^L: 每个元素定义如下:\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6cbf465e93a1371bbb6019961231a738_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6cbf465e93a1371bbb6019961231a738_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='496'%20height='67'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6cbf465e93a1371bbb6019961231a738_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6cbf465e93a1371bbb6019961231a738_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E右式第一个项 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpartial+_%7Ba_j%5EL%7D\" alt=\"\\partial _{a_j^L}\" eeimg=\"1\"\u003E 表示代价随着 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j%5E%7Bth%7D\" alt=\"j^{th}\" eeimg=\"1\"\u003E 输出激活值的变化而变化的速度。假如 C 不太依赖一个特定的输出神经元 j，那么 δ_j^L 就会很小，这也是我们想要的效果。右式第二项 σ′(zjL) 刻画了在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z_j%5EL\" alt=\"z_j^L\" eeimg=\"1\"\u003E 处激活函数 σ 变化的速度。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E使用下一层的误差 δ^{l+1} 来表示当前层的误差 δ^l:\u003C\u002Fb\u003E 特别地，\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8dcc9d0f61140415cbdfec4cfb286bb4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"519\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb\" width=\"519\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8dcc9d0f61140415cbdfec4cfb286bb4_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='519'%20height='56'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"519\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"519\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8dcc9d0f61140415cbdfec4cfb286bb4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8dcc9d0f61140415cbdfec4cfb286bb4_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中 (w^{l+1})^T 是 (l + 1)^{th} 层权重矩阵 w^{l+1} 的转置。这个公式看上去有些复杂，但每一个元素有很好的解释。假设我们知道 l + 1^{th} 层的误差 δ^{l+1}。当我们应用转置的权重矩阵 (w^{l+1})^T ，我们可以凭直觉地把它看作是在沿着网络反向移动误差，给了我们度量在 lth 层输出的误差方法。然后，我们进行 Hadamard 乘积运算 ⊙σ′(z^l)。这会让误差通过 l 层的激活函数反向传递回来并给出在第 l 层的带权输入的误差 δ。\u003Cbr\u003E通过组合 (BP1) 和 (BP2)，我们可以计算任何层的误差 δ^l。首先使用 (BP1) 计算 δ^L，然后应用方程 (BP2) 来计算 δ^{L−1}，然后再次用方程 (BP2) 来计算 δ^{L−2}，如此一步一步地反向传播完整个网络。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E代价函数关于网络中任意偏置的改变率:\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b7f8fd04de05c5c9936de0d4f1635ab4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"462\" data-rawheight=\"79\" class=\"origin_image zh-lightbox-thumb\" width=\"462\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b7f8fd04de05c5c9936de0d4f1635ab4_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='462'%20height='79'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"462\" data-rawheight=\"79\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"462\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b7f8fd04de05c5c9936de0d4f1635ab4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b7f8fd04de05c5c9936de0d4f1635ab4_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E代价函数关于任何一个权重的改变率:\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8efddaee9d75bcf4a9f33a3d891becb5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"473\" data-rawheight=\"83\" class=\"origin_image zh-lightbox-thumb\" width=\"473\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8efddaee9d75bcf4a9f33a3d891becb5_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='473'%20height='83'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"473\" data-rawheight=\"83\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"473\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8efddaee9d75bcf4a9f33a3d891becb5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8efddaee9d75bcf4a9f33a3d891becb5_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E将上式简化可以写成如下形式：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4d10beb82b17a72dc7c731548eec880c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"60\" class=\"content_image\" width=\"138\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='138'%20height='60'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"138\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4d10beb82b17a72dc7c731548eec880c_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中 a_{in} 是输入给权重 w 的神经元的激活值，δ_{out} 是输出自权重 w 的神经元的误差。从上式很直观的可以看到，当激活值很小时，梯度也会很小，趋近于0，这样，我们就说权重缓慢学习，表示梯度下降的时候，这个权重改变不多；这样的情形我们称之为神经元已经饱和了，最终层的权重学习也会终止（或缓慢）。这种现象也称为梯度弥散（消失），这很不利于深度神经网络的学习。\u003C\u002Fp\u003E\u003Chr\u003E\u003Ch2\u003E3、激活函数\u003C\u002Fh2\u003E\u003Cp\u003E激活函数的作用网上有很多帖子介绍，也不需我多言。总而言之，它能使得神经网络的每层输出结果变得非线性化，为什么神经网络的输出结果需要非线性化？因为只有如此，神经网络才能拟合任意函数（线性函数、非线性函数），\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F22334626\" class=\"internal\"\u003E点这里详细了解\u003C\u002Fa\u003E；其实非线性化的作用还有这一点：保持每一层的输出具有“梯度”，只有有了“梯度”我们才能应用反向传播算法、梯度下降算法来优化代价函数（正如我们在前文看到的那样），从而训练出更深的神经网络。\u003C\u002Fp\u003E\u003Cp\u003E在讲反向传播的时候我们已经说过，误差需要从输出层一层一层的传递到输入层的，然而在传递过程中你会发现梯度越来越小，甚至都没有梯度了（这种现象称为梯度弥散问题 \u003Ci\u003Evanishing gradients \u003C\u002Fi\u003Eproblem）；而又存在再一些相反的案例，比如循环神经网络中，梯度会越来越大，这样权重始终在更新，因而训练一直得不到收敛，这种现象称为梯度爆炸问题（explod\u003Ci\u003Eing gradients \u003C\u002Fi\u003Eproblem）。\u003C\u002Fp\u003E\u003Cp\u003E最开始作为激活函数的函数是sigmod函数，如下图所示：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d3d6983a63be8a2ba0b260e23e3bee68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"535\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d3d6983a63be8a2ba0b260e23e3bee68_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='535'%20height='326'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"535\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d3d6983a63be8a2ba0b260e23e3bee68_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d3d6983a63be8a2ba0b260e23e3bee68_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从图中可以看到，当输入的值比较大时（负值或者正值），sigmod函数的导数趋近于0，也就是梯度饱和了。当反向传播kicks in时，它几乎没有梯度通过神经网络传播到输入层，即使有小梯度存在，也会不断的被稀释在顶层；所以到了低层时，已经没有什么梯度剩下了，也就是权重将不能得到更新。为了避免梯度小时或者爆炸，有人认为神经网络的每一层的输出的方差必须等于该层的输入值的方差（ we need the variance of the outputs of each layer to be equal to the variance of its inputs），于是他们针对sigmod激活函数发明了一种初始化权重的技术，具体公式如下：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b867e77353360e0a93c85d9d7683b913_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"659\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"659\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b867e77353360e0a93c85d9d7683b913_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='659'%20height='132'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"659\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"659\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b867e77353360e0a93c85d9d7683b913_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b867e77353360e0a93c85d9d7683b913_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这个初始化技术称为  \u003Ci\u003EXavier initialization或者 Glorot initialization，\u003C\u002Fi\u003E\u003Cb\u003E这能加速神经网络的训练速度，这也是导致当前训练深度神经网络成功的诀窍之一。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E针对不同激活函数，不同论文也给出了它们相应的权重初始化策略：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e54a3fc34175798d4dd6b8c7ceab6b70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"560\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e54a3fc34175798d4dd6b8c7ceab6b70_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='560'%20height='243'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"560\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e54a3fc34175798d4dd6b8c7ceab6b70_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e54a3fc34175798d4dd6b8c7ceab6b70_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在tensorflow中的全连接函数 fully_connected()中默认使用的是\u003Ci\u003EXavier initialization\u003C\u002Fi\u003E，你也可以手动替换为 He initialization，通过使用 variance_scaling_initializer()函数：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-python3\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Ehe_init\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Etf\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Econtrib\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Elayers\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Evariance_scaling_initializer\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E()\u003C\u002Fspan\u003E\n\u003Cspan class=\"n\"\u003Ehidden1\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Efully_connected\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003EX\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003En_hidden1\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eweights_initializer\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Ehe_init\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Escope\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E\"h1\"\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E)\u003C\u002Fspan\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E但是因为存在梯度饱和的问题，所以sigmod作为激活函数始终不怎么好，于是有了ReLU激活函数，它不存在饱和现象（当值为正数的时候），如下图红色虚线所示：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9c55ebd7d1fd06db5b176f9184c3cd7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"240\" class=\"content_image\" width=\"341\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='341'%20height='240'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"240\" class=\"content_image lazy\" width=\"341\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9c55ebd7d1fd06db5b176f9184c3cd7e_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E然而，当输入为负值的时候神经网络的经过ReLU激活之后输出就变成0了，这一定程度上加速了神经网络的计算速度，但是却也使得一大半的神经元“死了”，因为这些神经元不输出任何值（只有0），而当学习率比较大时，你会发现有更多的神经元“死了”，这不是我们想要的结果。为了解决这个问题，于是有了LeakyReLU激活函数：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b11400dc9a6cb8051d3cbbb7de13e362_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"31\" class=\"content_image\" width=\"257\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='257'%20height='31'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"257\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b11400dc9a6cb8051d3cbbb7de13e362_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c8270560c16ee0c9c1fc9069f6b13ee5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c8270560c16ee0c9c1fc9069f6b13ee5_r.jpg\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='440'%20height='258'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c8270560c16ee0c9c1fc9069f6b13ee5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c8270560c16ee0c9c1fc9069f6b13ee5_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当输入z小于0时不再直接输出0，而是输出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha+z\" alt=\"\\alpha z\" eeimg=\"1\"\u003E ，这里 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u003E 通常设置为0.01，这样神经元就不会“死”，而有机会“复苏”起来。有人发现使用这个变体激活函数往往比原激活函数效果更好，而设置 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha+%3D+0.2\" alt=\"\\alpha = 0.2\" eeimg=\"1\"\u003E （大的leaky）效果比设置为0.01更好。\u003C\u002Fp\u003E\u003Cp\u003E有时，也可以不将 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u003E 设置为一个固定的值，而是在训练神经网络时将其作为一个参数进行学习，这在大数据集上表现良好，然而对于小数据集却容易过拟合。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E指数线性单元激活函数  \u003Ci\u003Eexponential linear unit \u003C\u002Fi\u003E(ELU)\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E2015年， Djork-Arné Clevert等人提出这个激活函数，表示它表任何ReLU及其变体激活函数效果都要好，ELU具体定义如下：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-70c2143940263d230bd578b22ac6f8f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"332\" data-rawheight=\"63\" class=\"content_image\" width=\"332\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='332'%20height='63'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"332\" data-rawheight=\"63\" class=\"content_image lazy\" width=\"332\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-70c2143940263d230bd578b22ac6f8f6_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-746dfb0cf0d49ee84d6104ad955a2b83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"393\" data-rawheight=\"249\" class=\"content_image\" width=\"393\"\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&amp;lt;svg%20xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg'%20width='393'%20height='249'&amp;gt;&amp;lt;\u002Fsvg&amp;gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"393\" data-rawheight=\"249\" class=\"content_image lazy\" width=\"393\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-746dfb0cf0d49ee84d6104ad955a2b83_b.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以看到它的主要改变就是在输出值z&amp;lt;0时，将其替换为一个系数为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u003E 的指数函数。当负值很大时，超参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u003E 一般设置为1，当然也可以在训练时通过调参去设置它。\u003C\u002Fp\u003E\u003Cp\u003EELU的缺点是计算度慢，但在训练过程中，更快的收敛速度补偿了这一点。\u003C\u002Fp\u003E\u003Cp\u003E在训练神经网络需要选择激活函数时，你一般可以按照这个顺序取选择：ELU &amp;gt; leaky ReLU (and its variants) &amp;gt; ReLU &amp;gt; tanh &amp;gt; logistic。  \u003C\u002Fp\u003E\u003Cblockquote\u003EIf you care a lot about runtime performance, then you may prefer leaky ReLUs over ELUs. If you don’t want to tweak yet another hyperparameter, you may just use the default \u003Ci\u003Eα \u003C\u002Fi\u003Evalues suggested earlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set.\u003C\u002Fblockquote\u003E\u003Cp\u003Etensorflow里面有ELU激活函数。可以这样调用：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-python3\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Ehidden1\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Efully_connected\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003EX\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003En_hidden1\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eactivation_fn\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Etf\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Enn\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Eelu\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E)\u003C\u002Fspan\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E你也可以自定义  leaky ReLUs激活函数：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-python3\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"k\"\u003Edef\u003C\u002Fspan\u003E \u003Cspan class=\"nf\"\u003Eleaky_relu\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Ez\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Ename\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"kc\"\u003ENone\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E):\u003C\u002Fspan\u003E\n    \u003Cspan class=\"k\"\u003Ereturn\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Etf\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Emaximum\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mf\"\u003E0.01\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E*\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Ez\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Ez\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Ename\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Ename\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E)\u003C\u002Fspan\u003E\n\n\u003Cspan class=\"n\"\u003Ehidden1\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Efully_connected\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003EX\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003En_hidden1\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eactivation_fn\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Eleaky_relu\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E)\u003C\u002Fspan\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E好了，本文到此结束，如有纰漏敬请斧正；另外希望在过年前我能写完Dropout，Softmax， Batch Normalization等深度学习基础概念的介绍。\u003C\u002Fp\u003E\u003Ch2\u003E4、参考文献：\u003C\u002Fh2\u003E\u003Cblockquote\u003E1、Aurélien Géron，《 Hands-On Machine Learning with Scikit-Learn and TensoFlow》\u003Cbr\u003E2、Michael Nielsen，《Neural Networks and Deep Learning》\u003Cbr\u003E3、Ian Goodfellow et. al，《Deep Learning》\u003C\u002Fblockquote\u003E\u003Cp\u003E禁止转载。\u003C\u002Fp\u003E","updated":new Date("2018-01-08T09:54:08.000Z"),"canComment":true,"commentPermission":"anyone","commentCount":4,"collapsedCount":0,"likeCount":81,"state":"published","isLiked":false,"slug":"32714733","lastestTipjarors":[],"isTitleImageFullScreen":false,"rating":"none","titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5022c810d5368fc75d86d7c2b2dc0860_r.jpg","links":{"comments":"\u002Fapi\u002Fposts\u002F32714733\u002Fcomments"},"reviewers":[],"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19813032","id":"19813032","name":"深度学习（Deep Learning）"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19559450","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F20032249","id":"20032249","name":"TensorFlow"}],"adminClosedComment":false,"titleImageSize":{"width":888,"height":439},"href":"\u002Fapi\u002Fposts\u002F32714733","excerptTitle":"","column":{"slug":"leemoo","name":"每天都要机器学习哦"},"tipjarState":"activated","tipjarTagLine":"真诚赞赏，手留余香","sourceUrl":"","pageCommentsCount":4,"tipjarorCount":0,"annotationAction":[],"hasPublishingDraft":false,"snapshotUrl":"","publishedTime":"2018-01-08T17:54:08+08:00","url":"\u002Fp\u002F32714733","lastestLikers":[{"bio":"机器学习 | iOS ","isFollowing":false,"hash":"688f16917cf6c30c8f1e65817de6ea9d","uid":694161005151801300,"isOrg":false,"slug":"lin-guo-sheng-97","isFollowed":false,"description":"","name":"BearLin","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Flin-guo-sheng-97","avatar":{"id":"v2-f29e42630818810e2852028bc75852a3","template":"https:\u002F\u002Fpic3.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},{"bio":"炼金术士=w=","isFollowing":false,"hash":"a2804eb50280280933ca9da112ed1ef3","uid":39482954874880,"isOrg":false,"slug":"anakin_tatooine","isFollowed":false,"description":"Python是个好语言","name":"方恨水","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fanakin_tatooine","avatar":{"id":"c58a09446c06989c3f98668e034696ec","template":"https:\u002F\u002Fpic2.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},{"bio":"期货交易公司码农","isFollowing":false,"hash":"d9532c59157576c842b3ee5ee518056b","uid":27891400179712,"isOrg":false,"slug":"wang-wei-77-36","isFollowed":false,"description":"爱编程，爱数学，爱交易， quant一枚，码农一个","name":"星澜海","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fwang-wei-77-36","avatar":{"id":"fa59702d0fac718e41f820b22d27effe","template":"https:\u002F\u002Fpic4.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},{"bio":"学生","isFollowing":false,"hash":"c85cc4c814874f256f608ac72e1e701a","uid":69471247007744,"isOrg":false,"slug":"wang-si-76-53","isFollowed":false,"description":"","name":"王思","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fwang-si-76-53","avatar":{"id":"a05dab5d5e204d8176df3b1ba204081b","template":"https:\u002F\u002Fpic1.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},{"bio":"神经网络在读，电子音乐爱好者","isFollowing":false,"hash":"1f1e5edbb48be0790b636501fb9f9ccc","uid":645356550483611600,"isOrg":false,"slug":"liu-xu-song-40","isFollowed":false,"description":"","name":"刘旭松","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fliu-xu-song-40","avatar":{"id":"e5294395d5fa49c26cd5846b5053fff2","template":"https:\u002F\u002Fpic3.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false}],"summary":"\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2db5731c0563357c3205c0d1bc05bb08_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"482\" data-rawheight=\"342\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2db5731c0563357c3205c0d1bc05bb08_r.jpg\"\u003E在入门深度学习时，梯度下降、反向传播、激活函数这三个概念是绕不过的知识点，如果不能好好理解这些点那么深度学习可能就入不了门；如果不能好好的将这些点联系起来，我觉得对深度神经网络的理解也会很迷惑。网上介绍这些概念的文章有很多，但是往往都是单…","reviewingCommentsCount":0,"meta":{"previous":{"isTitleImageFullScreen":false,"rating":"none","titleImage":"","links":{"comments":"\u002Fapi\u002Fposts\u002F32354021\u002Fcomments"},"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19559450","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19552832","id":"19552832","name":"Python"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19559424","id":"19559424","name":"数据分析"}],"adminClosedComment":false,"href":"\u002Fapi\u002Fposts\u002F32354021","excerptTitle":"","author":{"bio":"计算机硕士在读","isFollowing":true,"hash":"1b7326a564c30474efbc1f85c5af40a9","uid":61977426657280,"isOrg":false,"slug":"wangle-nlp","isFollowed":false,"description":"要心平气和、不要争执","name":"王乐","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fwangle-nlp","avatar":{"id":"v2-5a76beb4edb4fee8e8b6026394b47368","template":"https:\u002F\u002Fpic1.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},"content":"\u003Cp\u003E十一月底到十二月中旬的这一段时间参加了京东举办的京东金融信贷需求预测竞赛，致使那段时间我都没有好好准备考六级：），当然有可能好好准备了还是过不了的：）；虽然六级没有好好准备，但是我竞赛也没好好做呀：），所以成绩最终排名到50几名了，本也没脸写什么经验分享，而且决赛都结束10天了；但是不总结一下吧，感觉不利于进步，所以打算把这个总结当做汲取经验教训，同时也分享给其他想参加竞赛的同学，做一点微小的贡献。\u003C\u002Fp\u003E\u003Cp\u003E因为我的层次呢只有这么点高，可能只适合那些不知机器学习是何物，竞赛baseliine都不知怎么写的同学参考；欢迎有经验的大佬随时指点我的不足之处，点拨一下我- -。\u003C\u002Fp\u003E\u003Cp\u003E这个信贷需求预测的竞赛应该算是我正式的参加的第一个竞赛吧，之前玩过一些小比赛，限于能力有限也只是随便写个baseline提交就不管了。最后发现如果真的要认真去参加比赛的话，真的是太耗时间太耗精力了，这个过程真是太虐心了。毕竟这是一个实际业务场景，京东公司里可能都有一个小组专门花时间来研究，靠自己单打独斗要做好可见还是挺难的。当然如果认真去做的话，能学到的东西也很多，比如和其他选手的交流，自己置身在实际业务场景去解决实际问题，这不是从书上能学习到的；而且另一方面因为竞争意识，我又不得不去对之前学习的知识去再一次仔细再学习一遍，以便借助对算法的更深入的理解来改进模型，比如在参加竞赛的阶段我仔细研读了XGBoost，lightgbm算法系统，也写了两篇文章发在这个专栏；另外通过比赛也深刻的认识了自己的不足之处，为以后的学习找到一些方向。所以说参加这个竞赛还是有一些收获的。\u003C\u002Fp\u003E\u003Cp\u003E对于机器学习竞赛所需要的知识做一个优先级的，或者说你最先应该掌握的知识，做一个排序的话，我觉得应该这样排：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E机器学习任务的基本概念理解（比如回归，分类，排序的应用场景；特征的概念，等）\u003C\u002Fli\u003E\u003Cli\u003E编程语言的基本知识（比如Python）\u003C\u002Fli\u003E\u003Cli\u003E数据处理和科学计算的库（比如NumPy，pandas）\u003C\u002Fli\u003E\u003Cli\u003E机器学习算法库（比如sklearn）\u003C\u002Fli\u003E\u003Cli\u003E竞赛所选择算法的原理（比如xgboost）\u003C\u002Fli\u003E\u003Cli\u003E竞赛题目中的业务逻辑知识（比如信贷业务）\u003C\u002Fli\u003E\u003Cli\u003E对数据的敏感性（能分辩哪些数据有用，哪些没用）\u003C\u002Fli\u003E\u003Cli\u003E机器学习领域知识的综合能力（比如能熟练运用各种算法模型做融合，根据任务修改、创造算法）\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Chr\u003E\u003Cp\u003E理解第1点就等于确定了你所需要解决的任务的方向，方向至关重要，方向不对，后面所做的一切都将白做。以信贷需求预测这个场景为例，题目给定的任务是一个回归预测，也就是训练模型预测每一个用户在12月份的贷款金额；但是经过分析，我发现总共90000多个用户，而每个月只有20000左右的用户会贷款，其他用户都不会贷款；将这些用户的贷款金额画一个分布图如下所示：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-261001a6a1a979d79c5fbb8588f99504_b.jpg\" data-size=\"normal\" data-rawwidth=\"436\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"436\" data-original=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-261001a6a1a979d79c5fbb8588f99504_r.jpg\"\u003E\u003Cfigcaption\u003E11月份19520个用户的贷款金额分布示意图\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E对于有贷款行为的用户，他们的贷款金额分布大致符合正态分布。\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-6f06cab411177db9bb06589c90f9d121_b.jpg\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"504\" data-original=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-6f06cab411177db9bb06589c90f9d121_r.jpg\"\u003E\u003Cfigcaption\u003E11月份19520个用户的贷款金额升序排列\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从上图来说贷款用户的贷款金额基本上可以用线性模型就可以很好的拟合了。因此可以说对于贷款的用户确实很适合做回归预测。然而任务是要预测所有90000多个用户的贷款需求，那么把所有用户的贷款金额分布画出来再看一下。\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-e3d5f282dc08ec44a2bdb8db5510c855_b.jpg\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb\" width=\"522\" data-original=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-e3d5f282dc08ec44a2bdb8db5510c855_r.jpg\"\u003E\u003Cfigcaption\u003E所有90993个用户在11月的贷款金额分布\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以看到大部分贷款金额是0，他们的分布可以说非常不适合全部拿来做回归预测；凭着这个分析我想当然的以为可以先做一个二分类，先把会贷款和不会贷款分出来，然后对会贷款的用户做一个回顾预测，这样想可以说很完美了，然而事实却是，由于数据有限，分类准确率一直很低，而如果分类第一步都做不好，后面的回归也是扯淡了。所以做用户分类浪费了很多时间精力，这都是因为没有理解第1点所导致的。而且题目的排名计算准则是RMSE最小化，题目描述里也明确说了“开展信贷业务除了评估用户风险之外，还需要预测用户的借款需求，只有尽可能的给有借款需求的用户分配合适的额度才能\u003Cb\u003E最大限额的增加资金利用率，降低成本并增加收益\u003C\u002Fb\u003E”，所以任务的本质并不是预测具体哪个用户要贷款哪个用户不要贷款，也不是具体某个用户要贷款多少，而是要在总体水平上预测公司要在下一个月往信贷业务这个资金池里准备多少现金才能最大限额的增加资金利用率、降低成本并增加收益，资金池里现金放多了影响资金利用率，放少了不能满足借款需求从而收益减少。所以本质上只需要通过已有数据来预测下月的总借款需求，对所有用户都是用回归预测，将所有用户的贷款金额加起来就是总的需求啊，问题就解决了。\u003C\u002Fp\u003E\u003Cp\u003E因此，对于信贷需求预测这个场景，我们可以很坚定的使用回归模型，不要再花精力做什么分类了。\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E对于第2，3，4点，这是能保证你写出baseline的前提，即使你不懂任何机器学习算法，你只是刚接触入门，但是也足以写一个初级版本的竞赛代码，然后提交结果了。第2，3点保证你能将各类原始数据处理成训练模型所需要的数据形式，第4点很简单，调用API就够，刚开始甚至都不需要调参数，直接使用默认参数就好。很多人都说机器学习就是调参，但是我觉得大部分人还达不到调参的地步；倒不如说机器学习就是\u003Cb\u003E处理数据（提特征）\u003C\u002Fb\u003E更为贴切，当你到了调参这一步时，你的工作也差不多做完了。\u003C\u002Fp\u003E\u003Cp\u003E以这个信贷需求预测竞赛为例，讲一下它的数据是怎样的。先看看给的数据：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-b70bac6a2f4ad84abed7958b4b5dac20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"363\" data-rawheight=\"209\" class=\"content_image\" width=\"363\"\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-072ae546535d5aa491dbd37d2081f2a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"337\" data-rawheight=\"205\" class=\"content_image\" width=\"337\"\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic4.zhimg.com\u002Fv2-3c6a46008fe18e469d386ce04414780f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"228\" class=\"content_image\" width=\"278\"\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-5e6daf518b6cd6ad8c9d40f67012b214_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"441\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"441\" data-original=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-5e6daf518b6cd6ad8c9d40f67012b214_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-51b3242577b0512923e0147cb619a56d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb\" width=\"438\" data-original=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-51b3242577b0512923e0147cb619a56d_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E给的数据包含5张表，每张表列数比较少，行数从几万行到一千万行不等；这样的数据和我们在一些机器学习的课堂上所看到的数据不同，通常我们学习算法所用的数据都是很完美的数据，你只要划分一下训练集，验证集，测试集就行，然后将数据做一些基本的归一化等处理再输入算法接口，等待结果就行。然而看看这5张表，我们要如何动手把这些数据输入算法接口呢？\u003C\u002Fp\u003E\u003Cp\u003E仔细观察一下这些表，可以看到每张表都有uid这列，也就是用户ID，我们可以通过用户ID将所有表进行拼接组成一张表，然后做一些处理；这里的工作使用pandas基本可以搞定。然而直接拼接就行了吗？我们看一下每张表都有一些列，我们可以将这些列都看训练样本的属性，也就是特征，加起来也有10几个特征了，然而就这些特征够了吗？显然不够，显然也没有这么简单，所以这里就涉及特征工程了。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E数据和特征工程决定了机器学习的上限，算法模型只是用来逼近这个上限而已。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E时刻谨记上面的机器学习信条，去做特征工程吧！\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E第6，7点，为什么要理解数据，理解业务逻辑？因为这能让我们更好的取做特征啊。我们看看上述表格，基本上都是有时间这一列的，而且我们的任务目标是通过8，9，10，11月的用户借款历史记录，购买行为记录，用户点击行为记录，用户固有属性记录去预测12月份的借款需求，这明显涉及到时间序列问题。所以有经验的人自然一开始仅使用历史借款记录的那张表，然后使用时间滑动窗口去自动提取特征就能取得不错的成绩；而我，直到比赛后期才知道\u003Cb\u003E原来还有这样的操作。。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E另外，每个用户的行为都是和时间有关的，每个时间段他的行为都有所区别，这也是有些用户在这个月借款了在下个月可能又不会借款，随着时间的移动，用户的借款需求也是不同的，所以他的一些行为也是变动的，这些不同时间段的不同行为使得不同用户（不同训练样本）具有唯一性，但是我们机器学习就是去学习一些数据分布，如果所有用户的分布没有共性，只有个性，那么学习到的模型又怎么能去预测其他用户呢？因为每个用户都是独一无二的，预测是不可能的。所以除了找到用户的个性特征之外，我们还需要找到用户和用户之间的共性，只有这样，机器学习的模型才可以通过这些共性去预测其他用户样本。那么什么是共性？这里就用到统计特征了，用户在某个时间点可能行为各不相同，但是在某个时间段的行为却是相关的，比如，在同一个星期张三星期1，3，5借款了，李四星期2，4，6借款了，那么他们在这一周内的借款频率，借款金额的平均值，总额，中间值，最大值，最小值，方差等是不是有可能一样呢？这样就找到共性了。\u003C\u002Fp\u003E\u003Cp\u003E那么业务逻辑有什么用呢？\u003C\u002Fp\u003E\u003Cp\u003E当然了解业务逻辑也是为了提特征，上面讲的时间滑动窗口特征只是纯粹从数字上去构建特征的；对于一些具体的业务，我们也可以人为通过各列数据去生成一些有利于预测的特征。比如借款业务，有借款就要有还款，通过借款金额和还款期数，我们可以构造每月的还款特征；每个用都有一个借款限额，我们可以检查用户在某月的贷款金额是不是超过限额了从而构造特征；等等。\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E第7点，对数据的敏感性。\u003C\u002Fp\u003E\u003Cp\u003E有时候我们的数据很乱很脏，并不是所有数据都是有利于我们的模型构建，另外往往存在一些缺失值和异常值，以及训练集合测试集分布不一致的数据。因此我们有必要对数据进行检查，排除不好的数据，保留好的数据；这个就需要一些统计学知识了，更需要有对数据的敏感性认知。下面以正在进行的天池AI智能制造竞赛这个数据具体提出一些问题供大家参考。\u003C\u002Fp\u003E\u003Cp\u003E天池AI智能制造这个竞赛我是前两天才正式看数据动手写代码的，目前写了2个版本的baseline，排名top95\u002F1952（估计明天就掉出100名了，因为昨天我还是71.。看来还是有挺多大牛往上冲啊）\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-41975ab883ea70c1c4a61f2f16d1de0e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"678\" data-original=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-41975ab883ea70c1c4a61f2f16d1de0e_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E我们看看数据：\u003C\u002Fp\u003E\u003Cp\u003E这个比赛的数据有8027个属性，但训练集只有500个样本，线下甚至可以随便丢特征选特征，线下成绩都不会太差，因为毫无疑问肯定会过拟合。相比信贷需求预测的数据需要自己生成特征不同，这个数据需要删除特征，就是降维，很多人想到PCA，然而PCA真的适合吗？我们需要去看看数据和实际进行试验。\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-f129c19413feec9eef96f5ccad53309e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"258\" class=\"content_image\" width=\"420\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E看上图这个属性，如果做异常值检查，那么取值300的可能被做异常值处理了，然而它真是异常值吗？实际上它是这样的：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-6db81b8001fdf701ae501c9a517be4a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"447\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb\" width=\"447\" data-original=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-6db81b8001fdf701ae501c9a517be4a4_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E那么对于这样的特征如何处理呢？直接归一化？\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-28de1c1bc90b4f5c9851daa080c0d726_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"252\" class=\"content_image\" width=\"394\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样的特征又该如何处理呢？用线性模型时怎么处理，树形模型呢？\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-271bba82d92cd09d794875039f107432_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-271bba82d92cd09d794875039f107432_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样的呢？\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-f5791b08fa27d8b83132e0ae24d3c078_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"436\" data-rawheight=\"267\" class=\"origin_image zh-lightbox-thumb\" width=\"436\" data-original=\"http:\u002F\u002Fpic1.zhimg.com\u002Fv2-f5791b08fa27d8b83132e0ae24d3c078_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样的呢？训练集和测试集分布很不一致，是直接丢掉还是做一些处理？\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-706f5bf65ac4e44646990ebeff6382dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"437\" data-original=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-706f5bf65ac4e44646990ebeff6382dd_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这个是异常值吗？\u003C\u002Fp\u003E\u003Cp\u003E等等情况，总之机器学习之前对数据进行分析是必不可少的，训练好模型之后经过模型性能的反馈我们还需要回过头来继续分析数据，所以哪有机会给你调参呢？调参是前十名才需要做的吧……，所以至少如果你还没没有进入前二十，你都不需要去特意调参，选特征要紧（一家之言）。。。\u003C\u002Fp\u003E\u003Cp\u003E数据分析，数据与处理的帖子网上很多，随便搜一下就有了，我就不多说啦。\u003C\u002Fp\u003E\u003Cp\u003E对于机器学习竞赛，如果有大佬看到这里，欢迎指点指点我- -。。。。。闭门造车不可取，特别希望有高人指点，可能这就是环境和平台的缺陷吧，如果你身边都没有大牛，那你的水平天花板就只有那么高了；所以读书读一个好大学是有多么重要。。。当然也不是否定我湖，只是读研了活动范围窄接触的人太少，能认识大牛的机会更少。。。。。。\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E文末做一个预告，说了要做一个总结，这篇文章只是泛泛而谈，没什么干货，所以我会在最近几天选一个下午直播撸代码（免费参观，普通话不标准，对口音敏感的慎重），参考别人的代码的同时将这次比赛的代码做一个重构。会讲一些基本的数据处理和模型搭建，以及算法原理吧，就是上面讲到的第2，3，4，5，7点这些相关内容。将近2000行的代码，重构加讲解估计得要一个下午，所以这个直播质量节奏啥的不做保证，只是做一个记录，有兴趣的同学欢迎去围观，到时在QQ群里公布直播时间和平台网址，扫描下方二维码进群。。\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"http:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fqm.qq.com\u002Fcgi-bin\u002Fqm\u002Fqr%3Fk%3DghJMKlf1hd4n8djXuQmIEBSWMF5bVNnW\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eqm.qq.com\u002Fcgi-bin\u002Fqm\u002Fqr?\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ek=ghJMKlf1hd4n8djXuQmIEBSWMF5bVNnW\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E (二维码自动识别)\u003C\u002Fp\u003E\u003Cp\u003E群里也上传了信贷需求预测的数据文件，有兴趣的同学可以下载下来去玩玩。\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E更新\u003C\u002Fp\u003E\u003Cp\u003E本次竞赛代码重构视频\u003C\u002Fp\u003E\u003Ca href=\"http:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.bilibili.com\u002Fvideo\u002Fav17780394\u002F\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8b4967b0907cff06f6907b70d389349e_180x120.jpg\" data-image-width=\"1440\" data-image-height=\"900\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E京东金融信贷需求预测机器学习竞赛--代码重构_演讲•公开课_科技_bilibili_哔哩哔哩\u003C\u002Fa\u003E\u003Cp\u003E重构代码和原始代码均上传github\u003C\u002Fp\u003E\u003Ca href=\"http:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002Fwangle1218\u002F2017JDD--loan-forecasting\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0954b8e746ceb91e4447e0ec0f4937cc_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003Ewangle1218\u002F2017JDD--loan-forecasting\u003C\u002Fa\u003E\u003Cp\u003E\u003C\u002Fp\u003E","state":"published","sourceUrl":"","pageCommentsCount":0,"canComment":true,"snapshotUrl":"","slug":32354021,"publishedTime":"2017-12-26T16:43:21+08:00","url":"\u002Fp\u002F32354021","title":"关于机器学习（数据科学）竞赛的一些小思考","summary":"十一月底到十二月中旬的这一段时间参加了京东举办的京东金融信贷需求预测竞赛，致使那段时间我都没有好好准备考六级：），当然有可能好好准备了还是过不了的：）；虽然六级没有好好准备，但是我竞赛也没好好做呀：），所以成绩最终排名到50几名了，本也没脸…","reviewingCommentsCount":0,"meta":{"previous":null,"next":null},"commentPermission":"anyone","commentsCount":19,"likesCount":49},"next":{"isTitleImageFullScreen":false,"rating":"none","titleImage":"","links":{"comments":"\u002Fapi\u002Fposts\u002F32822550\u002Fcomments"},"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19560026","id":"19560026","name":"自然语言处理"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19559424","id":"19559424","name":"数据分析"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19866282","id":"19866282","name":"知乎专栏"}],"adminClosedComment":false,"href":"\u002Fapi\u002Fposts\u002F32822550","excerptTitle":"","author":{"bio":"计算机硕士在读","isFollowing":true,"hash":"1b7326a564c30474efbc1f85c5af40a9","uid":61977426657280,"isOrg":false,"slug":"wangle-nlp","isFollowed":false,"description":"要心平气和、不要争执","name":"王乐","profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fwangle-nlp","avatar":{"id":"v2-5a76beb4edb4fee8e8b6026394b47368","template":"https:\u002F\u002Fpic1.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false},"content":"\u003Cp\u003E灵机一动，于是爬了机器学习、自然语言处理、深度学习相关的558个知乎专栏，经过一番操作，竟然让我发现这里这些事！！\u003C\u002Fp\u003E\u003Ch2\u003E1、专栏发文数量\u003C\u002Fh2\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-8afe283d6209d40463a2b6f95db071de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb\" width=\"793\" data-original=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-8afe283d6209d40463a2b6f95db071de_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E大部分专栏的文章数量在10篇以下，达418个，占比75%；其中一文没发的有122个。\u003C\u002Fp\u003E\u003Ch2\u003E2、专栏关注人数\u003C\u002Fh2\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-b8a2d1cab91b939df5f9ed09521a8fd5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb\" width=\"785\" data-original=\"http:\u002F\u002Fpic2.zhimg.com\u002Fv2-b8a2d1cab91b939df5f9ed09521a8fd5_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E大部分专栏的关注人数在1000人以下，达445个，占比79.75%；少数关注人数有一万个以上，数量为7个。\u003C\u002Fp\u003E\u003Cp\u003E关注人数超8000个以上的有以下专栏：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cimg src=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-2ee0f70410dc5f679ff63b33852604b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"778\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"http:\u002F\u002Fpic3.zhimg.com\u002Fv2-2ee0f70410dc5f679ff63b33852604b2_r.jpg\"\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E完整版及利于阅读的版本移步这里：\u003C\u002Fp\u003E\u003Ca href=\"http:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002Fwangle1218\u002Fzhihu_zhuanlan_recommend\u002Fblob\u002Fmaster\u002Fsort_by_followers.md\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0954b8e746ceb91e4447e0ec0f4937cc_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Egithub.com\u002Fwangle1218\u002Fz\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ehihu_zhuanlan_recommend\u002Fblob\u002Fmaster\u002Fsort_by_followers.md\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cp\u003E后期所有新增的内容都会及时提交到这个仓库，欢饮持续关注……\u003C\u002Fp\u003E\u003Ch2\u003E3、专栏描述关键词\u003C\u002Fh2\u003E\u003Cp\u003E558个专栏有307个写了专栏介绍，占比55%，专栏介绍里的关键词频数如下:\u003C\u002Fp\u003E\u003Cp\u003E('学习', 303), ('机器', 135), ('深度', 125), ('分享', 52), ('算法', 45), ('人工智能', 44), ('技术', 39), ('相关', 35), ('专栏', 32), ('知识', 31)\u003C\u002Fp\u003E\u003Cp\u003E看来jieba分词还是不够好啊，机器学习分词成机器和学习了，同样的深度学习、自然语言处理也没分对。。\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E你以为就这么些了吗，错错错，今天时间有限，先弄到这里；后期打算使用NLP分析一下这558个专栏的所有文章，我目前想做的有：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E提取所有专栏的所有文章的发文时间分析发文周期(总共6559篇文章)；\u003C\u002Fli\u003E\u003Cli\u003E提取所有文章的赞同数、评论数；\u003C\u002Fli\u003E\u003Cli\u003E针对ML、NLP、DL等细分领域做文章主题分类；\u003C\u002Fli\u003E\u003Cli\u003E提取每篇文章的精简摘要、关键词；\u003C\u002Fli\u003E\u003Cli\u003E相似文章整合；\u003C\u002Fli\u003E\u003Cli\u003E综合以上所有数据做一个推荐榜，推荐每个细分领域最值得读的文章。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E希望以上工作能让你在浩如烟海的学习资料中快速精准的找到想要看的高质量文章。\u003C\u002Fp\u003E\u003Cp\u003E当然如果你有其他建议，也欢饮你评论告知，毕竟这个工作还是要面向读者老爷的，读者老爷的意见很重要。\u003C\u002Fp\u003E\u003Chr\u003E\u003Cp\u003E另外，在爬专栏信息的时候，我是使用的搜索关键词来找要爬的专栏，我发现JSON数据里面还有 score 这个信息，这应该就是知乎给的搜索关键词和专栏的匹配得分吧， score 分值越高，排序越靠前，初步猜测是根据搜索词和专栏名称、专栏描述以及专栏里面的文章标签做的一个简单的关键词匹配来打分排序的。然而这个JSON格式的页面我不会爬。。。。最后爬的是HTML，然后用的BeautifulSoup解析的。。。如果有爬虫牛人能给我把JSON格式的爬下来就好了。。。这样我或许可以使用这个score来做标签，然后做一个回归的机器学习，或许能找到知乎的搜索排序规则，或许还能优化知乎的专栏搜索排序。。。当然有兴趣的同学也可以自己去做做。\u003C\u002Fp\u003E","state":"published","sourceUrl":"","pageCommentsCount":0,"canComment":true,"snapshotUrl":"","slug":32822550,"publishedTime":"2018-01-10T23:46:04+08:00","url":"\u002Fp\u002F32822550","title":"分析了558个知乎专栏，竟然让我发现这样的事","summary":"灵机一动，于是爬了机器学习、自然语言处理、深度学习相关的558个知乎专栏，经过一番操作，竟然让我发现这里这些事！！1、专栏发文数量大部分专栏的文章数量在10篇以下，达418个，占比75%；其中一文没发的有122个。2、专栏关注人数大部分专栏的关注人数在10…","reviewingCommentsCount":0,"meta":{"previous":null,"next":null},"commentPermission":"anyone","commentsCount":18,"likesCount":29}},"annotationDetail":null,"commentsCount":4,"likesCount":81,"FULLINFO":true}},"User":{"yue-liang-he-xiao":{"isFollowed":false,"name":"sike","headline":"","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fe519a5bfbe9e58ce6098cf1d858dc7d0_s.jpg","isFollowing":false,"type":"people","slug":"yue-liang-he-xiao","bio":"程序员","hash":"bf0143c6ffc95233089b5ff6ed647a40","uid":611143972933079000,"links":{"columns":"\u002Fapi\u002Fme\u002Fcolumns"},"isOrg":false,"pendingColumns":[],"activated":true,"allowShareDaily":false,"isBindPhone":true,"mutedInfo":{"muted":false,"reason":null,"leftBanDay":-1,"banReason":"OTHER"},"description":"","muted":false,"profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fyue-liang-he-xiao","avatar":{"id":"e519a5bfbe9e58ce6098cf1d858dc7d0","template":"https:\u002F\u002Fpic4.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false,"email":null,"columns":[]},"wangle-nlp":{"isFollowed":false,"name":"王乐","headline":"要心平气和、不要争执","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5a76beb4edb4fee8e8b6026394b47368_s.jpg","isFollowing":true,"type":"people","slug":"wangle-nlp","bio":"计算机硕士在读","hash":"1b7326a564c30474efbc1f85c5af40a9","uid":61977426657280,"isOrg":false,"description":"要心平气和、不要争执","badge":{"identity":null,"bestAnswerer":null},"profileUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fwangle-nlp","avatar":{"id":"v2-5a76beb4edb4fee8e8b6026394b47368","template":"https:\u002F\u002Fpic1.zhimg.com\u002F{id}_{size}.jpg"},"isOrgWhiteList":false,"isBanned":false}},"Comment":{},"favlists":{}},"me":{"slug":"yue-liang-he-xiao"},"global":{"experimentFeatures":{"ge3":"ge3_9","ge2":"ge2_1","androidPassThroughPush":"all","nwebQAGrowth":"experiment","qawebRelatedReadingsContentControl":"close","marketTabBanner":"market_tab_banner_show","liveStore":"ls_a3_b2_c2_f2","qawebThumbnailAbtest":"new","nwebSearch":"nweb_search_heifetz","searchHybridTabs":"without-tabs","enableVoteDownReasonMenu":"enable","showVideoUploadAttention":"true","isOffice":"false","enableTtsPlay":"post","newQuestionDiversion":"true","recomAnswerRec":"recom_answer_lastn","wechatShareModal":"wechat_share_modal_show","newLiveFeedMediacard":"old","nwebFeedUi":"expand","recommendEbookAc":"ebook_new","hybridZhmoreVideo":"no","recommendLiveGuessLike":"live_guess_gbdt","nwebGrowthPeople":"default","nwebSearchSuggest":"experiment","qrcodeLogin":"qrcode","androidDbFollowRecommendHide":"open","isShowUnicomFreeEntry":"unicom_free_entry_off","newMobileColumnAppheader":"new_header","androidDbCommentWithRepinRecord":"open","feedHybridTopicRecomButtonIcon":"yes","androidDbRecommendAction":"open","zcmLighting":"zcm","androidDbFeedHashTagStyle":"button","appStoreRateDialog":"close","recommendQuestion":"rec_question_new","topWeightSearch":"new_top_search","default":"None","isNewNotiPanel":"no","androidDbRepinSelection":"open","growthBanner":"default","androidProfilePanel":"panel_b","recommendLiveDetail":"live_detail_no_rerank_v2"}},"columns":{"next":{},"leemoo":{"following":false,"canManage":false,"href":"\u002Fapi\u002Fcolumns\u002Fleemoo","name":"每天都要机器学习哦","creator":{"slug":"wangle-nlp"},"url":"\u002Fleemoo","slug":"leemoo","avatar":{"id":"v2-8456af8026d9b1d75719eab1971a3662","template":"https:\u002F\u002Fpic2.zhimg.com\u002F{id}_{size}.jpg"}}},"columnPosts":{},"columnSettings":{"colomnAuthor":[],"uploadAvatarDetails":"","contributeRequests":[],"contributeRequestsTotalCount":0,"inviteAuthor":""},"postComments":{},"postReviewComments":{"comments":[],"newComments":[],"hasMore":true},"favlistsByUser":{},"favlistRelations":{},"promotions":{},"switches":{"couldSetPoster":false},"draft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null}},"drafts":{"draftsList":[],"next":{}},"config":{"userNotBindPhoneTipString":{}},"recommendPosts":{"articleRecommendations":[],"columnRecommendations":[]},"env":{"edition":{"baidu":false,"yidianzixun":false,"qqnews":false},"isAppView":false,"appViewConfig":{"content_padding_top":128,"content_padding_bottom":56,"content_padding_left":16,"content_padding_right":16,"title_font_size":22,"body_font_size":16,"is_dark_theme":false,"can_auto_load_image":true,"app_info":"OS=iOS"},"isApp":false,"userAgent":{"ua":"Mozilla\u002F5.0 (Windows NT 10.0; WOW64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F63.0.3239.132 Safari\u002F537.36","browser":{"name":"Chrome","version":"63.0.3239.132","major":"63"},"engine":{"version":"537.36","name":"WebKit"},"os":{"name":"Windows","version":"10"},"device":{},"cpu":{"architecture":"amd64"}}},"message":{"newCount":0},"pushNotification":{"newCount":0}}</textarea><script src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/common.5f727ff076db70d3161d.js.下载"></script><script src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/app.806d4fd41664cb95419d.js.下载"></script><script src="./深度学习的前戏--梯度下降、反向传播、激活函数_files/raven.f2839fd2e428f2fb2218.js.下载" async="" defer=""></script><div><div data-reactroot=""><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><div class="Editable-languageSuggestionsInput Input-wrapper"><input value="" autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplet-51418-10508-0" id="Popover-51418-98349-toggle" aria-haspopup="true" aria-owns="Popover-51418-98349-content" class="Input" placeholder="选择语言"><div class="Input-after"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></div></div><!-- react-empty: 10 --></div></div></div></div></div></body></html>