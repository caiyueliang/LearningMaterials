<!DOCTYPE html>
<!-- saved from url=(0039)https://www.zhihu.com/question/39022858 -->
<html lang="zh" data-hairline="true" data-theme="light" data-focus-method="pointer" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>卷积神经网络工作原理直观的解释？ - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link href="./卷积神经网络工作原理直观的解释？ - 知乎_files/main.app.e052b57f9ecda6cf7626.css" rel="stylesheet"><meta name="apple-itunes-app" content="app-id=432274380, app-argument=zhihu://questions/39022858" data-react-helmet="true"><script type="text/javascript" charset="utf-8" async="" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/main.modals.f83b9535b20ed594c996.js.下载"></script><script type="text/javascript" charset="utf-8" async="" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/main.richinput.cf0c31502c4ce9bb70a1.js.下载"></script><script type="text/javascript" charset="utf-8" async="" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/main.signflow.b42112b76d3e185c9364.js.下载"></script><style type="text/css">.CloseIcon-icon-2xww{transition:opacity .3s ease-out}.CloseIcon-icon-2xww:hover{opacity:.8}</style><style type="text/css">.animations-fadeIn-1aFv{animation:animations-fadeIn-1aFv .3s ease-out both}@keyframes animations-fadeIn-1aFv{0%{opacity:0}to{opacity:1}}.animations-fadeOut-3XSQ{animation:animations-fadeOut-3XSQ .3s ease-out both}@keyframes animations-fadeOut-3XSQ{0%{opacity:1}to{opacity:0}}.animations-fadeInUp-3KKK{animation:animations-fadeInUp-3KKK .3s cubic-bezier(.25,.1,.35,1) both}@keyframes animations-fadeInUp-3KKK{0%{opacity:0;transform:translateY(20px)}to{opacity:1;transform:translateY(0)}}.animations-fadeOutDown-r_A_{animation:animations-fadeOutDown-r_A_ .3s cubic-bezier(.25,.1,.35,1) both}@keyframes animations-fadeOutDown-r_A_{0%{transform:translateY(0)}to{opacity:0;transform:translateY(20px)}}</style><style type="text/css">.Modal-backdrop-2ksh{background-color:rgba(0,0,0,.65)}.Modal-backdrop-2ksh,.Modal-modalWrapper-56Mq{position:fixed;top:0;right:0;bottom:0;left:0;z-index:10010}.Modal-modalWrapper-56Mq{display:-ms-flexbox;display:flex;-ms-flex-align:center;align-items:center;-ms-flex-pack:center;justify-content:center}.Modal-modal-wf58{position:relative;z-index:10011;background:#fff;border-radius:2px}.Modal-content-3JxL{width:588px;max-height:calc(100vh - 24px * 2);overflow-x:hidden;overflow-y:auto;-webkit-overflow-scrolling:touch}.Modal-closeButton-3JkR{position:absolute;top:4px;right:-44px;padding:12px;width:40px;height:40px;cursor:pointer;box-sizing:border-box;background:none;outline:none;border:none}</style><style type="text/css">.FeedbackButton-button-3waL{position:fixed;z-index:10000;bottom:40px;right:40px;width:40px;height:40px;cursor:pointer;border-radius:50%;background-color:#fff;border:none;outline:none;box-shadow:0 0 10px rgba(0,0,0,.15);font-weight:700;line-height:normal}.FeedbackButton-icon-1Rgw{display:inline-block;vertical-align:middle;width:18px;height:18px;background-image:url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzUiIGhlaWdodD0iMzYiIHZpZXdCb3g9IjAgMCAzNSAzNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48dGl0bGU+R3JvdXAgNjwvdGl0bGU+PGcgZmlsbD0iIzAwOEZFQiIgZmlsbC1ydWxlPSJldmVub2RkIj48cGF0aCBkPSJNMTMgMTguM2MwLS40IDAtLjcuNC0xIC4yLS4yLjUtLjMuOC0uMy40IDAgLjcgMCAxIC40LjIuMi4zLjUuMyAxIDAgLjIgMCAuNS0uNC43IDAgLjQtLjQuNS0uOC41LS4zIDAtLjYgMC0uOC0uNC0uMyAwLS40LS40LS40LS43ek0xMCAxMC43di0xYy40LTEgMS41LTIuNyA0LjItMi43IDIgMCAzLjggMS40IDMuOCAzcy0xLjQgMi43LTIgMy4zYy0uOC42LTEgMS0xLjIgMS43LS4yLjYtLjYgMS0xLjMgMS0uMy0uMi0uNi0uMy0uNy0uNXYtLjhjMC0uMiAwLS43LjMtMS4ybDEuNC0xLjVjMS40LTEuMiAxLjYtMiAuOC0yLjgtLjUtLjQtMS42LS41LTIuMiAwLS44LjQtMSAxLTEuMiAxLjMtLjIuNS0uMyAxLTEuMyAxLS4zLS4yLS40LS40LS41LS44eiIvPjxwYXRoIGQ9Ik0yOS44IDEwLjJ2M2MxLjQgMS44IDIuMyA0IDIuMyA2LjMgMCAzLjgtMi4yIDctNS41IDl2My44bC0zLjctMi41LTMgLjNjLTIuOCAwLTQuMy0uOC02LjQtMi4yaC0zLjFDMTMgMzAuNCAxNS42IDMyIDIwIDMyYy44IDAgMS43IDAgMi42LS4ybDYgNHYtNi40YzMuNS0yLjQgNS43LTYgNS43LTEwIDAtMy42LTEuNy03LTQuNS05LjJ6TTE0LjQgMjUuNmM4IDAgMTMuMi02IDEzLjItMTNTMjEgMCAxNC40IDBDNi40IDAgMCA1LjcgMCAxMi43YzAgNCAxLjUgNy41IDQuNCAxMFYyOWw2LjMtMy42IDMuNy4yek0xNC4yIDJjNi41IDAgMTEuNSA1LjMgMTEuNSAxMUMyNS43IDE5IDIxIDI0IDE0LjQgMjRjLTEgMC0zLjYtLjMtNC41LS41TDYgMjUuN3YtNGMtMi43LTIuMi00LTUtNC04LjZDMiA3IDcgMiAxNCAyeiIvPjwvZz48L3N2Zz4=);background-repeat:no-repeat;background-size:contain}.FeedbackButton-button-3waL:hover .FeedbackButton-icon-1Rgw{background-image:url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzUiIGhlaWdodD0iMzYiIHZpZXdCb3g9IjAgMCAzNSAzNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48dGl0bGU+R3JvdXAgMTE8L3RpdGxlPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCI+PHBhdGggZD0iTTI5LjQgMTNjMCAyLS4zIDguMi01IDExLjQtNC4zIDMtOSAzLjMtMTEuMyAzLjNoLTNjMi44IDIuNyA2LjYgNC44IDEyLjIgNCAxIDAgNi4zIDQgNi4zIDRWMjljMy41LTIuMyA1LjMtNS4yIDUuNi05LjUuMy0zLjctMS42LTctNC41LTkuNXYzem0tMTUgMTIuNmM4IDAgMTMuMi02IDEzLjItMTNTMjEgMCAxNC40IDBDNi40IDAgMCA1LjcgMCAxMi43YzAgNCAxLjUgNy41IDQuNCAxMFYyOWw2LjMtMy42IDMuNy4zeiIgZmlsbD0iIzAwOEZFQiIvPjxwYXRoIGQ9Ik0xMyAxOC4zYzAtLjQgMC0uNy40LTFzLjUtLjMuOC0uM2MuNCAwIC43IDAgMSAuNC4yLjIuMy41LjMgMSAwIC4yIDAgLjUtLjQuNyAwIC40LS40LjUtLjguNS0uMyAwLS42IDAtLjgtLjQtLjMgMC0uNC0uNC0uNC0uN3ptLTMtNy42di0xYy40LTEgMS40LTIuNyA0LjItMi43IDIgMCAzLjggMS40IDMuOCAzcy0xLjQgMi43LTIgMy4zYy0uOC42LTEgMS0xLjIgMS43LS4yLjYtLjYgMS0xLjMgMS0uMy0uMi0uNi0uMy0uNy0uNXYtLjhjMC0uMiAwLS43LjMtMS4ybDEuNC0xLjVjMS40LTEuMiAxLjYtMiAuOC0yLjgtLjUtLjQtMS42LS41LTIuMiAwLS44LjQtMSAxLTEuMiAxLjMtLjIuNS0uMyAxLTEuMyAxLS4zLS4yLS40LS40LS41LS44eiIgZmlsbD0iI0ZGRiIvPjwvZz48L3N2Zz4=)}</style><style type="text/css">.DrawingExample-svg-30WA{position:absolute;top:30px;right:0;left:0;margin:auto;transform:rotate(-44deg)}.DrawingExample-ellipse-26bv{stroke-dasharray:520;transform-origin:center;animation:DrawingExample-drawingExample-3Bm3 .6s linear both}@keyframes DrawingExample-drawingExample-3Bm3{0%{stroke-dashoffset:520}50%{stroke-dashoffset:1000;opacity:1}to{stroke-dashoffset:1000;opacity:0}}</style><style type="text/css">.Spinner-spinner-2PGn{position:absolute;width:30px;height:30px;top:50%;left:50%;margin:-15px 0 0 -15px;animation:Spinner-rotate-RMMJ .8s linear infinite}.Spinner-spinner-2PGn .Spinner-circle-teFy{stroke:#4197ff;stroke-dasharray:187;stroke-dashoffset:46.75;transform-origin:center}@keyframes Spinner-rotate-RMMJ{0%{transform:rotate(0deg)}to{transform:rotate(1turn)}}</style><style type="text/css">.FeedbackForm-form-1uUg{padding:40px 24px 32px;width:100%;font-size:14px;line-height:1.5;font-family:HelveticaNeue-Light,Helvetica,PingFangSC-Light,Hiragino Sans GB,Microsoft YaHei,Arial,sans-serif;color:#404040;box-sizing:border-box}.FeedbackForm-header-3hQI{margin-bottom:26px;text-align:center}.FeedbackForm-title-2uCC{font-size:24px;font-weight:500;line-height:33px;font-family:Helvetica,PingFang SC,Hiragino Sans GB,Microsoft YaHei,Arial,sans-serif}.FeedbackForm-inputBox-15yJ{display:block;padding:12px;width:100%;height:auto;font-size:15px;border:1px solid #e7eaf1;border-radius:3px;box-sizing:border-box;resize:none;outline:none;color:inherit;transition:box-shadow .15s ease-out;overflow:auto}.FeedbackForm-inputBox-15yJ::-webkit-input-placeholder{color:#9aaabf;transition:color .15s ease-out}.FeedbackForm-inputBox-15yJ:-ms-input-placeholder{color:#9aaabf;transition:color .15s ease-out}.FeedbackForm-inputBox-15yJ::placeholder{color:#9aaabf;transition:color .15s ease-out}.FeedbackForm-inputBox-15yJ:hover::-webkit-input-placeholder{color:rgba(154,170,191,.8)}.FeedbackForm-inputBox-15yJ:hover:-ms-input-placeholder{color:rgba(154,170,191,.8)}.FeedbackForm-inputBox-15yJ:hover::placeholder{color:rgba(154,170,191,.8)}.FeedbackForm-inputBox-15yJ:focus{box-shadow:0 0 5px #e7eaf1}.FeedbackForm-inputBox-15yJ:focus::-webkit-input-placeholder{color:rgba(154,170,191,.8)}.FeedbackForm-inputBox-15yJ:focus:-ms-input-placeholder{color:rgba(154,170,191,.8)}.FeedbackForm-inputBox-15yJ:focus::placeholder{color:rgba(154,170,191,.8)}.FeedbackForm-inputBox-15yJ.FeedbackForm-isWarning-2ds-{border-color:#f75659}.FeedbackForm-inputBox-15yJ.FeedbackForm-isWarning-2ds-::-webkit-input-placeholder{color:#f75659}.FeedbackForm-inputBox-15yJ.FeedbackForm-isWarning-2ds-:-ms-input-placeholder{color:#f75659}.FeedbackForm-inputBox-15yJ.FeedbackForm-isWarning-2ds-::placeholder{color:#f75659}.FeedbackForm-inputBox-15yJ.FeedbackForm-isWarning-2ds-:focus{box-shadow:none}.FeedbackForm-screenShot--Gsn{overflow:hidden;box-sizing:border-box;transition:max-height .3s ease,opacity .3s ease}.FeedbackForm-screenShotLabel-2Sgh{padding-top:22px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.FeedbackForm-canvasContainer-mrde{margin-top:8px;position:relative;background-color:#f6f7f9}.FeedbackForm-canvas-tSGI{display:block;max-width:100%;cursor:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAABGdBTUEAALGPC/xhBQAAARpJREFUOBGdkr1KA0EUhTcxEkSQQIpAihQWPoedD2BnIVsZQiBVEkiR7SwtBDsJqXwBLQwEgoWNjY2FjYUQ38Ei5Oc7sBeGLTKze+Djzsyecxh2N4ryq0zkHpYQQ2H1SW5TNsxukaZzQsewACvTvIBg3eFUaABHMEv3OruBIN3icm8wZl+FZ3iBQ/AqweGW2FrlKlChV0McFsxOvaegm/T2lLzzTC/dqzaO7A1s/8GzE28Dhhj0f1jQnZ+c18CrKxxrcMO2/uK87m3AcAkrsKA7vzlvQJAmuNywrX84bwY1YBrBATyCFWj+QguCJKNCU1DZQ7r/Y55CsK5x2i2eWFcggTPIJd3Eiv5Zd3KlU3OJOQd9lVd4A5Xl1g4YG2GGhwRfegAAAABJRU5ErkJggg==) 0 17,default;cursor:-webkit-image-set(url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAABGdBTUEAALGPC/xhBQAAARpJREFUOBGdkr1KA0EUhTcxEkSQQIpAihQWPoedD2BnIVsZQiBVEkiR7SwtBDsJqXwBLQwEgoWNjY2FjYUQ38Ei5Oc7sBeGLTKze+Djzsyecxh2N4ryq0zkHpYQQ2H1SW5TNsxukaZzQsewACvTvIBg3eFUaABHMEv3OruBIN3icm8wZl+FZ3iBQ/AqweGW2FrlKlChV0McFsxOvaegm/T2lLzzTC/dqzaO7A1s/8GzE28Dhhj0f1jQnZ+c18CrKxxrcMO2/uK87m3AcAkrsKA7vzlvQJAmuNywrX84bwY1YBrBATyCFWj+QguCJKNCU1DZQ7r/Y55CsK5x2i2eWFcggTPIJd3Eiv5Zd3KlU3OJOQd9lVd4A5Xl1g4YG2GGhwRfegAAAABJRU5ErkJggg==) 1x,url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAABGdBTUEAALGPC/xhBQAAAn9JREFUWAnFlz1IV2EUxs2+ixCCrCCrIaiGGnLIRdBacqkhtCEbqqVaoqmacpIma6rFHHITh1pKMKFaInATkb4hKChKIwj7rt8D74XDy3tfKM69Hng4zz3nvc9zfH3/93//DQ31xWqsBsEnMAU6wILFSpwnwB+Db/BDoPZYjuMYsMMU/Af1nron2o7hh5KBNNhP0AtqjV24vQPFzsT5F70jVU7UiPhNcNyY7IC/AfEwxfUsvcVmvRtdhNIQkNFvcAoUsQ3yGhRDxLmpWOiZryUMzxqDrfCXiTXjZo0bvZIwKnbhvHHZBH9q1s7Am03fhV42BsUQcb5knDbAp8EzsNHUXWgfKrF52XW/cVwH1265xgXUyszL6gOuExixc/8xjIZ8C9YbHRd6GpWyHcjV33PfTpcJjMgJuJ4xOeNU7yP37DY6LvQoKnrUpwxzNb12tLpMYES64foyzBmnep+5p83ouNCDqHwHKcNc7Qv3tLtMYEQOwPVClTNO9ea5Z7/RcaH7UJFwyjBX0x/Q5TKBEdFWa8tzxqme3gTdX0/3IqrDmDLM1XTodfhdYw9qcyBnnOrpcdDrOglienDpAZYyzNX0oDwJ3GMUxZxxWe+M+yRBcBX53j8OpS9Z9ziM4tKguoJ8B5Tthq1fDPe4p+co3gbLgrLyLWDNY94X1rqnFmN8F64dUiwBIyAeRNd6Za0sjqFsTXWGdJYU+q00DGz/qhpVxg3EraH4Q7AmmDaSB4Hq10Ot0vQimMVDPaLeFJz1A1DvQsqVxmbU40Hs9ST9tZVOYMT1r+gw1zF9ReEx2BI3qrrWp6jTiH+FPwD6pI2BJ6DW0HkYBzNAQ9wH82DB4i/kUnkzGX+skQAAAABJRU5ErkJggg==) 2x) 0 17,default}.FeedbackForm-canvas-tSGI.FeedbackForm-isCapturing-3UFp{display:none}.FeedbackForm-checkLabelWrapper-3B7w{margin-top:12px}.FeedbackForm-checkLabel-2VTb{cursor:pointer;color:#9aaabf;transition:color .15s ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.FeedbackForm-checkLabel-2VTb:hover{color:rgba(154,170,191,.8)}.FeedbackForm-checkLabel-2VTb input{margin-right:.5em;vertical-align:1px}.FeedbackForm-actions-dJ87{margin-top:40px;text-align:center}.FeedbackForm-submitButton-1oKQ{display:inline-block;min-width:220px;padding:8px 1em;background-color:#0f88eb;border:1px solid #0f88eb;border-radius:3px;font:inherit;color:#fff;transition:background-color .15s ease-out,opacity .15s ease-out;cursor:pointer;outline:none}.FeedbackForm-submitButton-1oKQ[disabled]{opacity:.8;cursor:default}.FeedbackForm-submitButton-1oKQ:hover{background-color:#0d79d1}.FeedbackForm-submitButton-1oKQ:active{opacity:.8}.FeedbackForm-successMask-34go{display:-ms-flexbox;display:flex;-ms-flex-pack:center;justify-content:center;-ms-flex-align:center;align-items:center;position:absolute;top:0;left:0;width:100%;height:100%;background-color:#fff}.FeedbackForm-successTitle-1Y6p{font-size:24px;font-weight:500;line-height:33px;font-family:Helvetica,PingFang SC,Hiragino Sans GB,Microsoft YaHei,Arial,sans-serif;text-align:center}.FeedbackForm-successSubtitle-A_aP{margin-top:10px;font-size:18px;line-height:33px;text-align:center}</style></head><body class="Entry-body"><div id="root"><div data-zop-usertoken="{}" data-reactroot="" data-reactid="1"><!-- react-empty: 2 --><div class="LoadingBar" data-reactid="3"></div><!-- react-empty: 4 --><div data-reactid="5"><header role="banner" class="Sticky AppHeader is-fixed" data-za-module="TopNavBar" data-reactid="6" style="width: 1519.2px; top: 0px; left: 0px;"><!-- react-empty: 393 --><div class="AppHeader-inner" data-reactid="8"><a href="https://www.zhihu.com/" aria-label="知乎" data-reactid="9"><svg viewBox="0 0 200 91" class="Icon Icon--logo" style="fill:#0f88eb;height:30px;width:64px;" width="64" height="30" aria-hidden="true" data-reactid="10"><title data-reactid="11"></title><g data-reactid="12"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><nav role="navigation" class="AppHeader-nav" data-reactid="13"><a class="AppHeader-navItem" href="https://www.zhihu.com/" data-reactid="14">首页</a><a class="AppHeader-navItem" href="https://www.zhihu.com/explore" data-reactid="15">发现</a><a href="https://www.zhihu.com/topic" class="AppHeader-navItem" data-reactid="16">话题</a></nav><div class="SearchBar" role="search" data-reactid="17"><div class="SearchBar-toolWrapper" data-reactid="18"><form class="SearchBar-tool" data-reactid="19"><div data-reactid="20"><div class="Popover" data-reactid="21"><div class="SearchBar-input Input-wrapper Input-wrapper--grey" data-reactid="22"><input type="text" maxlength="100" value="" autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplet-84528-22300--1" id="Popover-84528-3220-toggle" aria-haspopup="true" aria-owns="Popover-84528-3220-content" class="Input" placeholder="搜索你感兴趣的内容…" data-reactid="23"><div class="Input-after" data-reactid="24"><button class="Button SearchBar-searchIcon Button--primary" aria-label="搜索" type="button" data-reactid="25"><svg viewBox="0 0 16 16" class="Icon Icon--search" style="height:16px;width:16px;" width="16" height="16" aria-hidden="true" data-reactid="26"><title data-reactid="27"></title><g data-reactid="28"><path d="M12.054 10.864c.887-1.14 1.42-2.57 1.42-4.127C13.474 3.017 10.457 0 6.737 0S0 3.016 0 6.737c0 3.72 3.016 6.737 6.737 6.737 1.556 0 2.985-.533 4.127-1.42l3.103 3.104c.765.46 1.705-.37 1.19-1.19l-3.103-3.104zm-5.317.925c-2.786 0-5.053-2.267-5.053-5.053S3.95 1.684 6.737 1.684 11.79 3.95 11.79 6.737 9.522 11.79 6.736 11.79z"></path></g></svg></button></div></div><!-- react-empty: 29 --></div></div></form></div></div><div class="AppHeader-userInfo" data-reactid="30"><!-- react-empty: 31 --><div class="AppHeader-profile" data-reactid="32"><div data-reactid="33"><button class="Button AppHeader-login Button--blue" type="button" data-reactid="34"><!-- react-text: 35 -->登录<!-- /react-text --></button><button class="Button Button--primary Button--blue" type="button" data-reactid="36"><!-- react-text: 37 -->加入知乎<!-- /react-text --></button></div></div></div></div><div class="PageHeader"><div data-reactroot="" class="QuestionHeader-content"><div class="QuestionHeader-main"><h1 class="QuestionHeader-title">卷积神经网络工作原理直观的解释？</h1></div><div class="QuestionHeader-side" data-za-detail-view-path-module="ToolBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;39022858&quot;}}}"><div class="QuestionButtonGroup"><button class="Button FollowButton Button--primary Button--blue" type="button"><!-- react-text: 7 -->关注问题<!-- /react-text --></button><button class="Button Button--blue" type="button"><svg viewBox="0 0 12 12" class="Icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg><!-- react-text: 12 -->写回答<!-- /react-text --></button></div></div></div></div></header><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div><!-- react-empty: 38 --><main role="main" class="App-main" data-reactid="39"><div class="QuestionPage" itemscope="" itemtype="http://schema.org/Question" data-reactid="40"><!-- react-empty: 41 --><meta itemprop="name" content="卷积神经网络工作原理直观的解释？" data-reactid="42"><meta itemprop="url" content="https://www.zhihu.com/question/39022858" data-reactid="43"><meta itemprop="keywords" content="机器学习,神经网络,卷积神经网络（CNN）" data-reactid="44"><meta itemprop="answerCount" content="42" data-reactid="45"><meta itemprop="commentCount" content="4" data-reactid="46"><meta itemprop="dateCreated" content="2015-12-30T10:31:10.000Z" data-reactid="47"><meta itemprop="dateModified" content="2015-12-30T13:59:36.000Z" data-reactid="48"><meta itemprop="zhihu:visitsCount" data-reactid="49"><meta itemprop="zhihu:followerCount" content="5746" data-reactid="50"><div data-zop-question="{&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;机器学习&quot;,&quot;id&quot;:&quot;19559450&quot;},{&quot;name&quot;:&quot;神经网络&quot;,&quot;id&quot;:&quot;19607065&quot;},{&quot;name&quot;:&quot;卷积神经网络（CNN）&quot;,&quot;id&quot;:&quot;20043586&quot;}],&quot;id&quot;:39022858,&quot;isEditable&quot;:false}" data-reactid="51" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;39022858&quot;}}}"><div class="QuestionStatus"><!-- react-empty: 395 --><!-- react-empty: 396 --><!-- react-empty: 397 --></div><div class="QuestionHeader" data-reactid="53" data-za-detail-view-path-module="QuestionDescription" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;39022858&quot;}}}"><div class="QuestionHeader-content" data-reactid="54"><div class="QuestionHeader-main" data-reactid="55"><div class="QuestionHeader-tags" data-reactid="56"><div class="QuestionHeader-topics" data-reactid="57"><div class="Tag QuestionTopic" data-reactid="58" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content" data-reactid="59"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" data-reactid="60"><div class="Popover" data-reactid="61"><div id="Popover-84538-2171-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84538-2171-content" data-reactid="62">机器学习</div><!-- react-empty: 63 --></div></a></span></div><div class="Tag QuestionTopic" data-reactid="64" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19607065&quot;}}}"><span class="Tag-content" data-reactid="65"><a class="TopicLink" href="https://www.zhihu.com/topic/19607065" data-reactid="66"><div class="Popover" data-reactid="67"><div id="Popover-84539-59468-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84539-59468-content" data-reactid="68">神经网络</div><!-- react-empty: 69 --></div></a></span></div><div class="Tag QuestionTopic" data-reactid="70" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20043586&quot;}}}"><span class="Tag-content" data-reactid="71"><a class="TopicLink" href="https://www.zhihu.com/topic/20043586" data-reactid="72"><div class="Popover" data-reactid="73"><div id="Popover-84539-98814-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84539-98814-content" data-reactid="74">卷积神经网络（CNN）</div><!-- react-empty: 75 --></div></a></span></div></div></div><h1 class="QuestionHeader-title" data-reactid="76"><!-- react-text: 77 -->卷积神经网络工作原理直观的解释？<!-- /react-text --></h1><div data-reactid="78"><!-- react-text: 79 --><!-- /react-text --><div class="QuestionHeader-detail" data-reactid="80"><div class="QuestionRichText QuestionRichText--expandable" data-reactid="81"><div data-reactid="82"><span class="RichText" itemprop="text" data-reactid="83">我目前大四，在用CNN做手写识别毕业设计，已经接触机器学习4个月了。但CNN是目前最让我困惑的，其简直就像黑匣子，只有输入输出，然后看看准确率，完全不清楚其内部如何由初始状态一步步走向结果，还请能指点我几个问题，万分感谢！<br><br>cnn的论文却让我读得稀里糊涂。在读过相关的LeCun的论文中，有以下几点。<br>1.发现其并没有详细的解释内部变化。<br>2.对于局部到整体的变化，通过降低空间分辨率来进行，其不在乎特征的精度，而是特征的相对位置。其由边，角的组合从而形成特征的相对位置（其实有N多的组合，但训练的目的就是选择合适的），是否是在LeNet－5的第三层和后面的全连接层进行？<br>3.其采用卷积核共享参数，减少了需要训练的参数，这是主要的。但其同样对变形、移动的图片有很好的识别，而且还能侦查所谓的特征，我定义6个卷积核就相当于侦查了图片中的6个特征，那我12个就是12个特征了，这些特征是什么样子的？在LeNet的演示页面，我看到第一层是卷积后完整的图片，那边，角，在哪里？另外，如果有，侦查的特征和后面特征的组合有何关系？在读过的论文中，也只是说，实验后这样是可行的，但没有具体解释。去掉卷积，直接进行减少空间分辨率，然后上全连接，这样可行吗？<br><br>我也实现了一个和LeNet－5相似的模型，但想来想去还是觉得只是造了个轮子，并不能理解。想把实验里每个过程的图片都拿出来看，可后面几层完全不知道怎么看。关于神经网络的数学表达能力的文章我没有读过，但就LeNet而言，还是上面几个问题。我题目是自命，论文只读过<br><br>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.Proceedings of the IEEE, november 1998<br><br>Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a back-propagation network. In David Touretzky, editor, Advances in Neural Information Processing Systems 2 (NIPS*89), Denver, CO, 1990. Morgan Kaufman</span></div></div></div></div></div><div class="QuestionHeader-side" data-reactid="89"><div class="QuestionHeader-follow-status" data-reactid="90"><div class="QuestionFollowStatus" data-reactid="91"><div class="NumberBoard QuestionFollowStatus-counts NumberBoard--divider" data-reactid="92"><div class="NumberBoard-item" data-reactid="93"><div class="NumberBoard-itemInner" data-reactid="94"><div class="NumberBoard-itemName" data-reactid="95">关注者</div><strong class="NumberBoard-itemValue" title="5746" data-reactid="96">5,746</strong></div></div><div class="NumberBoard-item" data-reactid="97"><div class="NumberBoard-itemInner" data-reactid="98"><div class="NumberBoard-itemName" data-reactid="99">被浏览</div><strong class="NumberBoard-itemValue" title="455534" data-reactid="100">455,534</strong></div></div></div><!-- react-empty: 101 --></div></div></div></div><div><div class="Sticky QuestionHeader-footer is-bottom" style=""><div class="QuestionHeader-footer-inner"><div class="QuestionHeader-main QuestionHeader-footer-main"><div class="QuestionButtonGroup"><button class="Button FollowButton Button--primary Button--blue" type="button"><!-- react-text: 2680 -->关注问题<!-- /react-text --></button><button class="Button Button--blue" type="button"><svg viewBox="0 0 12 12" class="Icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg><!-- react-text: 2685 -->写回答<!-- /react-text --></button></div><div class="QuestionHeaderActions"><div class="QuestionHeader-Comment"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2690 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2693 -->4 条评论<!-- /react-text --><!-- react-empty: 2694 --></button></div><div class="Popover ShareMenu"><div class="" id="Popover-97886-72506-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-97886-72506-content"><img class="ShareMenu-fakeQRCode" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/qrcode" alt="微信二维码"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2699 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2702 -->分享<!-- /react-text --></button></div><!-- react-empty: 2703 --></div><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2706 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2709 -->邀请回答<!-- /react-text --></button><div class="Popover"><button class="Button Button--plain Button--withIcon Button--iconOnly" aria-label="更多" type="button" id="Popover-97886-9403-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-97886-9403-content"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2713 -->​<!-- /react-text --><svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button><!-- react-empty: 2716 --></div><!-- react-empty: 2728 --><!-- react-empty: 2718 --><!-- react-empty: 2719 --><!-- react-empty: 2720 --><!-- react-empty: 2721 --></div><div class="QuestionHeader-actions"><button class="Button Button--plain" type="button"><!-- react-text: 2724 -->收起<!-- /react-text --><svg viewBox="0 0 10 6" class="Icon QuestionHeader-collapse-icon Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div></div></div></div></div><div data-reactid="150"><div data-reactid="151"><div class="Sticky is-fixed" data-reactid="152" style="width: 1519.2px; top: 52px; left: 0px;"></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 0px;"></div></div><!-- react-empty: 3270 --></div></div><div class="Question-main" data-reactid="153"><div class="Question-mainColumn" data-reactid="154"><div data-reactid="155"><!-- react-empty: 156 --><div id="QuestionAnswers-answers" class="QuestionAnswers-answers" data-zop-feedlistmap="0,0,1,0" data-reactid="157" data-za-detail-view-path-module="ContentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card" data-reactid="158"><div class="List" data-reactid="159"><div class="List-header" data-reactid="160"><h4 class="List-headerText" data-reactid="161"><span data-reactid="162"><!-- react-text: 163 -->42<!-- /react-text --><!-- react-text: 164 --> 个回答<!-- /react-text --></span></h4><div class="List-headerOptions" data-reactid="165"><div class="Popover" data-reactid="166"><button class="Button Select-button Select-plainButton Button--plain" role="combobox" aria-expanded="false" type="button" id="Popover-84545-74251-toggle" aria-haspopup="true" aria-owns="Popover-84545-74251-content" data-reactid="167"><!-- react-text: 168 -->默认排序<!-- /react-text --><span style="display:inline-flex;align-items:center;" data-reactid="169"><!-- react-text: 170 -->​<!-- /react-text --><svg class="Zi Zi--Select Select-arrow" fill="#9fadc7" viewBox="0 0 24 24" width="24" height="24" data-reactid="171"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd" data-reactid="172"></path></svg></span></button><!-- react-empty: 173 --></div></div></div><div data-reactid="174"><div class="" data-reactid="175"><div class="List-item" data-reactid="176"><div class="ContentItem AnswerItem" data-za-index="0" data-zop="{&quot;authorName&quot;:&quot;Owl of Minerva&quot;,&quot;itemId&quot;:224446917,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="224446917" itemprop="acceptedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-reactid="177" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;224446917&quot;,&quot;upvote_num&quot;:938,&quot;comment_num&quot;:49,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;bfd75d28af9e975dec51ec8ae0527fe7&quot;}}}"><div class="ContentItem-meta" data-reactid="178"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person" data-reactid="179"><meta itemprop="name" content="Owl of Minerva" data-reactid="180"><meta itemprop="image" content="https://pic4.zhimg.com/623cb8e61da80bbae706f30642b2a145_is.jpg" data-reactid="181"><meta itemprop="url" content="https://www.zhihu.com/people/OwlofMinerva" data-reactid="182"><meta itemprop="zhihu:followerCount" content="55216" data-reactid="183"><span class="UserLink AuthorInfo-avatarWrapper" data-reactid="184"><div class="Popover" data-reactid="185"><div id="Popover-84545-8387-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84545-8387-content" data-reactid="186"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/OwlofMinerva" data-reactid="187"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/623cb8e61da80bbae706f30642b2a145_xs.jpg" srcset="https://pic4.zhimg.com/623cb8e61da80bbae706f30642b2a145_l.jpg 2x" alt="Owl of Minerva" data-reactid="188"></a></div><!-- react-empty: 189 --></div></span><div class="AuthorInfo-content" data-reactid="190"><div class="AuthorInfo-head" data-reactid="191"><span class="UserLink AuthorInfo-name" data-reactid="192"><div class="Popover" data-reactid="193"><div id="Popover-84545-21252-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84545-21252-content" data-reactid="194"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/OwlofMinerva" data-reactid="195">Owl of Minerva</a></div><!-- react-empty: 2673 --></div><a class="UserLink-badge" href="https://www.zhihu.com/question/48509984" target="_blank" data-tooltip="优秀回答者" data-reactid="197"><svg viewBox="0 0 20 20" class="Icon Icon--badgeGlorious" style="height:16px;width:16px;" width="16" height="16" aria-hidden="true" data-reactid="198"><title data-reactid="199">用户标识</title><g data-reactid="200"><g fill="none" fill-rule="evenodd">     <path d="M.64 11.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.12 3.21.656 4.083 1.735l-.125-.154c.876 1.085 2.304 1.093 3.195.028l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.28-2.238l.016.197c-.12-1.388.656-3.212 1.735-4.084l-.154.125c1.084-.876 1.093-2.304.028-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.28l.198-.016c-1.387.122-3.21-.655-4.083-1.734l.125.153C10.802-.265 9.374-.274 8.483.79L8.61.64c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.28 2.24L2.4 4.363c.12 1.387-.656 3.21-1.735 4.084l.154-.126C-.265 9.2-.274 10.626.79 11.517L.64 11.39z" fill="#FF9500"></path>     <path d="M10.034 12.96L7.38 14.58c-.47.286-.747.09-.618-.45l.72-3.024-2.36-2.024c-.418-.357-.318-.68.235-.725l3.1-.25 1.195-2.87c.21-.508.55-.513.763 0l1.195 2.87 3.1.25c.547.043.657.365.236.725l-2.362 2.024.72 3.025c.13.535-.143.74-.616.45l-2.654-1.62z" fill="#FFF"></path>   </g></g></svg></a></span></div><div class="AuthorInfo-detail" data-reactid="201"><div class="AuthorInfo-badge" data-reactid="202"><div class="AuthorInfo-badgeText" data-reactid="203"><span data-reactid="204"><span data-reactid="205"><a href="https://www.zhihu.com/people/OwlofMinerva/answers/topic/19619550" data-reactid="206">神经科学</a><!-- react-text: 207 -->、<!-- /react-text --></span><span data-reactid="208"><a href="https://www.zhihu.com/people/OwlofMinerva/answers/topic/19620291" data-reactid="209">脑科学</a><!-- react-text: 210 --> <!-- /react-text --></span><!-- react-text: 211 -->话题<!-- /react-text --></span><!-- react-text: 212 -->的<!-- /react-text --><!-- react-text: 213 -->优秀回答者<!-- /react-text --></div></div></div></div></div><div class="AnswerItem-extraInfo" data-reactid="214"><span data-reactid="215"><!-- react-text: 216 -->收录于 <!-- /react-text --><span class="AnswerItem-markInfoItem" data-reactid="217">编辑推荐</span></span><!-- react-text: 218 --> · <!-- /react-text --><span class="Voters" data-reactid="219"><button class="Button Button--plain" type="button" data-reactid="220"><!-- react-text: 221 -->938 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 222 --></span></div></div><meta itemprop="image" content="https://pic3.zhimg.com/v2-6c3ecd5935b257fbd52cd57d0ac2402b_200x112.jpg" data-reactid="223"><meta itemprop="upvoteCount" content="938" data-reactid="224"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/224446917" data-reactid="225"><meta itemprop="dateCreated" content="2017-09-02T13:47:26.000Z" data-reactid="226"><meta itemprop="dateModified" content="2017-09-02T13:54:18.000Z" data-reactid="227"><meta itemprop="commentCount" content="49" data-reactid="228"><div class="RichContent RichContent--unescapable" data-reactid="229"><div class="RichContent-inner" data-reactid="230"><span class="RichText CopyrightRichText-richText" itemprop="text" data-reactid="231"><p>从最基础的开始</p><p>对二维数字信号（图像）的操作，可以写成矩阵形式。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-6c3ecd5935b257fbd52cd57d0ac2402b_hd.jpg" data-rawwidth="389" data-rawheight="377" class="content_image" width="389"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-6c3ecd5935b257fbd52cd57d0ac2402b_hd.jpg" data-rawwidth="389" data-rawheight="377" class="content_image lazy" width="389" data-actualsrc="https://pic3.zhimg.com/50/v2-6c3ecd5935b257fbd52cd57d0ac2402b_hd.jpg"></figure><p>比如对图像做平滑，一个典型的8领域平滑，其结果中的每个值都来源于原对应位置和其周边8个元素与一个3X3矩阵的乘积：</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-b92779a7dba6c9105b2c86dff70fef65_hd.jpg" data-rawwidth="703" data-rawheight="368" class="origin_image zh-lightbox-thumb" width="703" data-original="https://pic3.zhimg.com/v2-b92779a7dba6c9105b2c86dff70fef65_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-b92779a7dba6c9105b2c86dff70fef65_hd.jpg" data-rawwidth="703" data-rawheight="368" class="origin_image zh-lightbox-thumb lazy" width="703" data-original="https://pic3.zhimg.com/v2-b92779a7dba6c9105b2c86dff70fef65_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-b92779a7dba6c9105b2c86dff70fef65_hd.jpg"></figure><p>也就相当于对原矩阵，按照顺序将各区域元素与W矩阵相乘，W 矩阵为</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-837dfd79c625629a0f04e57c0ffb10ed_hd.jpg" data-rawwidth="277" data-rawheight="130" class="content_image" width="277"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-837dfd79c625629a0f04e57c0ffb10ed_hd.jpg" data-rawwidth="277" data-rawheight="130" class="content_image lazy" width="277" data-actualsrc="https://pic3.zhimg.com/50/v2-837dfd79c625629a0f04e57c0ffb10ed_hd.jpg"></figure><p>这也被称作核(Kernel, 3X3)</p><p>其处理效果如下：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-ccdd4c56e745b3c4b279f3dc52a88737_hd.jpg" data-rawwidth="900" data-rawheight="411" class="origin_image zh-lightbox-thumb" width="900" data-original="https://pic2.zhimg.com/v2-ccdd4c56e745b3c4b279f3dc52a88737_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ccdd4c56e745b3c4b279f3dc52a88737_hd.jpg" data-rawwidth="900" data-rawheight="411" class="origin_image zh-lightbox-thumb lazy" width="900" data-original="https://pic2.zhimg.com/v2-ccdd4c56e745b3c4b279f3dc52a88737_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-ccdd4c56e745b3c4b279f3dc52a88737_hd.jpg"></figure><p>也就是，这个核对图像进行操作，相当于对图像进行了低通滤波。因此这个核也被称为滤波器，整个操作过程按照概念称为卷积。</p><p>扩展来讲，对二维图像的滤波操作可以写成卷积，比如常见的高斯滤波、拉普拉斯滤波（算子）等。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-6cc428eebcf692486a8e6b52d3a558c0_hd.jpg" data-rawwidth="275" data-rawheight="307" class="content_image" width="275"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-6cc428eebcf692486a8e6b52d3a558c0_hd.jpg" data-rawwidth="275" data-rawheight="307" class="content_image lazy" width="275" data-actualsrc="https://pic4.zhimg.com/50/v2-6cc428eebcf692486a8e6b52d3a558c0_hd.jpg"></figure><p>滤波器跟卷积神经网络有什么关系呢。不如我们预想一个识别问题：我们要识别图像中的某种特定曲线，也就是说，这个滤波器要对这种曲线有很高的输出，对其他形状则输出很低，这也就像是神经元的<b>激活</b>。</p><p>我们设计的滤波器和想要识别的曲线如下：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-c5d4c0c3fac68a7a711b3e31b552313f_hd.jpg" data-rawwidth="615" data-rawheight="271" class="origin_image zh-lightbox-thumb" width="615" data-original="https://pic2.zhimg.com/v2-c5d4c0c3fac68a7a711b3e31b552313f_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c5d4c0c3fac68a7a711b3e31b552313f_hd.jpg" data-rawwidth="615" data-rawheight="271" class="origin_image zh-lightbox-thumb lazy" width="615" data-original="https://pic2.zhimg.com/v2-c5d4c0c3fac68a7a711b3e31b552313f_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-c5d4c0c3fac68a7a711b3e31b552313f_hd.jpg"></figure><p>假设上面的核（滤波器）按照卷积顺序沿着下图移动：</p><p><br></p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-d2b0e3f9fa705eef0551033f4e47d413_hd.jpg" data-rawwidth="634" data-rawheight="211" class="origin_image zh-lightbox-thumb" width="634" data-original="https://pic4.zhimg.com/v2-d2b0e3f9fa705eef0551033f4e47d413_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-d2b0e3f9fa705eef0551033f4e47d413_hd.jpg" data-rawwidth="634" data-rawheight="211" class="origin_image zh-lightbox-thumb lazy" width="634" data-original="https://pic4.zhimg.com/v2-d2b0e3f9fa705eef0551033f4e47d413_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-d2b0e3f9fa705eef0551033f4e47d413_hd.jpg"></figure><p>那么当它移动到上面的位置时，按照矩阵操作，将这个区域的图像像素值与滤波器相乘，我们得到一个很大的值（6600）：</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-8b9d835be42ae0253c20108dbb976f3d_hd.jpg" data-rawwidth="705" data-rawheight="261" class="origin_image zh-lightbox-thumb" width="705" data-original="https://pic1.zhimg.com/v2-8b9d835be42ae0253c20108dbb976f3d_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-8b9d835be42ae0253c20108dbb976f3d_hd.jpg" data-rawwidth="705" data-rawheight="261" class="origin_image zh-lightbox-thumb lazy" width="705" data-original="https://pic1.zhimg.com/v2-8b9d835be42ae0253c20108dbb976f3d_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-8b9d835be42ae0253c20108dbb976f3d_hd.jpg"></figure><p>而当这个滤波器移动到其他区域时，我们得到一个相对很小的值：</p><figure><noscript>&lt;img src="https://pic7.zhimg.com/50/v2-ff07f869ed9446ee048ac218e6c77bce_hd.jpg" data-rawwidth="702" data-rawheight="251" class="origin_image zh-lightbox-thumb" width="702" data-original="https://pic7.zhimg.com/v2-ff07f869ed9446ee048ac218e6c77bce_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ff07f869ed9446ee048ac218e6c77bce_hd.jpg" data-rawwidth="702" data-rawheight="251" class="origin_image zh-lightbox-thumb lazy" width="702" data-original="https://pic7.zhimg.com/v2-ff07f869ed9446ee048ac218e6c77bce_r.jpg" data-actualsrc="https://pic7.zhimg.com/50/v2-ff07f869ed9446ee048ac218e6c77bce_hd.jpg"></figure><p>如此，我们对整个原图进行一次卷积，得到的结果中，在那个特定曲线和周边区域，值就很高，在其他区域，值相对低。这就是一张<b>激活图</b>。对应的高值区域就是我们所要检测曲线的位置。</p><p>在训练卷积审计网络（CNN）的某一个卷积层时，我们实际上是在训练一系列的滤波器(filter)。比如，对于一个32x32x3（宽32像素x高32像素xRGB三通道）的图像，如果我们在CNN的第一个卷积层定义训练12个滤波器，那就这一层的输出便是32X32X12.按照不同的任务，我们可以对这个输出做进一步的处理，这包括激活函数，池化，全连接等。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-4d49151943dd467c46ea9e055e82f9c1_hd.jpg" data-rawwidth="552" data-rawheight="420" class="origin_image zh-lightbox-thumb" width="552" data-original="https://pic4.zhimg.com/v2-4d49151943dd467c46ea9e055e82f9c1_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-4d49151943dd467c46ea9e055e82f9c1_hd.jpg" data-rawwidth="552" data-rawheight="420" class="origin_image zh-lightbox-thumb lazy" width="552" data-original="https://pic4.zhimg.com/v2-4d49151943dd467c46ea9e055e82f9c1_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-4d49151943dd467c46ea9e055e82f9c1_hd.jpg"></figure><p>简单来说，训练CNN在相当意义上是在训练每一个卷积层的滤波器。让这些滤波器组对特定的模式有高的激活，以达到CNN网络的分类/检测等目的。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-c29a52fa109192d2f6df1869bd78f8b0_hd.jpg" data-rawwidth="560" data-rawheight="558" class="origin_image zh-lightbox-thumb" width="560" data-original="https://pic4.zhimg.com/v2-c29a52fa109192d2f6df1869bd78f8b0_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c29a52fa109192d2f6df1869bd78f8b0_hd.jpg" data-rawwidth="560" data-rawheight="558" class="origin_image zh-lightbox-thumb lazy" width="560" data-original="https://pic4.zhimg.com/v2-c29a52fa109192d2f6df1869bd78f8b0_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-c29a52fa109192d2f6df1869bd78f8b0_hd.jpg"></figure><p>&lt;Fig. 一个实际CNN（AlexNet）第一个卷积层的滤波器&gt;</p><p>卷积神经网络的第一个卷积层的滤波器用来检测低阶特征，比如边、角、曲线等。随着卷积层的增加，对应滤波器检测的特征就更加复杂（理性情况下，也是我们想要的情况）。比如第二个卷积层的输入实际上是第一层的输出（滤波器激活图），这一层的滤波器便是用来检测低价特征的组合等情况（半圆、四边形等），如此累积，以检测越来越复杂的特征。实际上，我们的人类大脑的视觉信息处理也遵循这样的低阶特征到高阶特征的模式（<a href="https://www.zhihu.com/question/27380522/answer/36794240" class="internal">Owl of Minerva：为什么无彩色系（黑白灰色）在色彩搭配中可以和谐地与任何彩色搭配？</a>）。最后一层的滤波器按照训练CNN目的的不同，可能是在检测到人脸、手写字体等时候激活[1]。</p><p>所以，在相当程度上，构建卷积神经网络的任务就在于构建这些滤波器。也就是，将这些滤波器变成这样(改变滤波器矩阵的值，也就是Weight)的——能识别特定的特征。这个过程叫做<b>训练</b>。</p><p>在训练开始之时，卷积层的滤波器是完全随机的，它们不会对任何特征激活（不能检测任何特征）。这就像刚出生的孩子，TA不知道什么是人脸、什么是狗，什么是上下左右。TA需要学习才知道这些概念，也就是通过接触人脸、狗、上下左右，并被告知这些东西分别是人脸、狗、上下左右。然后TA才能在头脑中记住这些概念，并在之后的某一次见到之后能准确的给出结果。</p><p>把一个空白的滤波其，修改其权重(weights)以使它能检测特定的模式，整个过程就如工程里面的反馈。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-36a36bcf10325397b6d8ea62d3acd629_hd.jpg" data-rawwidth="800" data-rawheight="329" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic1.zhimg.com/v2-36a36bcf10325397b6d8ea62d3acd629_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-36a36bcf10325397b6d8ea62d3acd629_hd.jpg" data-rawwidth="800" data-rawheight="329" class="origin_image zh-lightbox-thumb lazy" width="800" data-original="https://pic1.zhimg.com/v2-36a36bcf10325397b6d8ea62d3acd629_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-36a36bcf10325397b6d8ea62d3acd629_hd.jpg"></figure><p>想想一下，如果有一只无意识的猴子，完全随机的修改一个5X5滤波器矩阵的25个值，那完全可能经过一定的轮次之后，这个滤波器能够检测棱角等特征。这是一种无反馈的训练情况。对神经网络的训练当然不能如此，我们不可能靠运气去做这件事情。</p><p>举个例子，我们要训练一个用于分类的神经网络，让它能判定输入图像中的物体最可能是十个类别的哪一类。那么，训练过程就是这样的：</p><p>第一次训练，输入一张图像，这个图像通过各层卷积处理输出量一组向量[1,1,1,1,1,1,1,1,1,1], 也就是，对于完全由随机滤波器构建的网络，其输出认为这张图等概率的是十个类别中的某一种。但是对于训练，我们有一个Gound Thuth, 也就是这张图中物体所属的类别：[0，0，1，0，0，0，0，0，0，0]，也就是属于第三类。这时候我们可以定义一个损失函数，比如常见的MSE（mean squared error）.我们假定L是这个损失函数的输出。这时候我们的目的就是，让L的值反馈(这种神经网络概念下称为 back propagation， 反向传输)给整个卷积神经网络，以修改各个滤波器的权重，使得损失值L最小。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-c2e7f995f2ece459495df5a0cdf4ff43_hd.jpg" data-rawwidth="304" data-rawheight="238" class="content_image" width="304"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c2e7f995f2ece459495df5a0cdf4ff43_hd.jpg" data-rawwidth="304" data-rawheight="238" class="content_image lazy" width="304" data-actualsrc="https://pic1.zhimg.com/50/v2-c2e7f995f2ece459495df5a0cdf4ff43_hd.jpg"></figure><p>这是一个典型的最优化问题。当然地，在工程上我们几乎不可能一次就把滤波器的权重W修改到使L最小的情况，而是需要多次训练和多次修改。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-5b17d0411f1b341a3f08ed035956a253_hd.jpg" data-rawwidth="284" data-rawheight="208" class="content_image" width="284"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-5b17d0411f1b341a3f08ed035956a253_hd.jpg" data-rawwidth="284" data-rawheight="208" class="content_image lazy" width="284" data-actualsrc="https://pic2.zhimg.com/50/v2-5b17d0411f1b341a3f08ed035956a253_hd.jpg"></figure><p>如果情况理想的话，权重修改的方向是使得L的变化收敛的。这也就是说很可能达到了我们训练这个神经网络的目的——让各个卷积层的滤波器能够组合起来最优化的检测特定的模式。</p><p>--------</p><p>[1] Zeiler, M. D., &amp; Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In <i>European conference on computer vision</i> (pp. 818-833). Springer, Cham.</p></span><!-- react-empty: 3268 --></div><div data-reactid="233"><div class="ContentItem-time" data-reactid="234"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/224446917" data-reactid="235"><span data-tooltip="发布于 2017-09-02" data-reactid="236"><!-- react-text: 237 -->编辑于 <!-- /react-text --><!-- react-text: 238 -->2017-09-02<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 3278 -->938<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3285 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3288 -->49 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-42556-34598-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-42556-34598-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3293 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3296 -->分享<!-- /react-text --></button></div><!-- react-empty: 3297 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3300 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3303 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3306 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3309 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 282 --><!-- react-empty: 629 --><!-- react-empty: 284 --><!-- react-empty: 285 --><!-- react-empty: 630 --><!-- react-empty: 287 --></div></div><div class="List-item" data-reactid="288"><div class="ContentItem AnswerItem" data-za-index="1" data-zop="{&quot;authorName&quot;:&quot;周思捷&quot;,&quot;itemId&quot;:106162086,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="106162086" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-reactid="289" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;106162086&quot;,&quot;upvote_num&quot;:539,&quot;comment_num&quot;:100,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;f4cbdf8951f19c65e927086b0a9c073b&quot;}}}"><div class="ContentItem-meta" data-reactid="290"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person" data-reactid="291"><meta itemprop="name" content="周思捷" data-reactid="292"><meta itemprop="image" content="https://pic4.zhimg.com/1ec95fd0a29b649d295203abf68f2721_is.jpg" data-reactid="293"><meta itemprop="url" content="https://www.zhihu.com/people/zhou-si-jie-44" data-reactid="294"><meta itemprop="zhihu:followerCount" content="137" data-reactid="295"><span class="UserLink AuthorInfo-avatarWrapper" data-reactid="296"><div class="Popover" data-reactid="297"><div id="Popover-84560-87009-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84560-87009-content" data-reactid="298"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/zhou-si-jie-44" data-reactid="299"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/1ec95fd0a29b649d295203abf68f2721_xs.jpg" srcset="https://pic4.zhimg.com/1ec95fd0a29b649d295203abf68f2721_l.jpg 2x" alt="周思捷" data-reactid="300"></a></div><!-- react-empty: 301 --></div></span><div class="AuthorInfo-content" data-reactid="302"><div class="AuthorInfo-head" data-reactid="303"><span class="UserLink AuthorInfo-name" data-reactid="304"><div class="Popover" data-reactid="305"><div id="Popover-84560-50434-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84560-50434-content" data-reactid="306"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/zhou-si-jie-44" data-reactid="307">周思捷</a></div><!-- react-empty: 308 --></div></span></div><div class="AuthorInfo-detail" data-reactid="309"><div class="AuthorInfo-badge" data-reactid="310"><div class="RichText AuthorInfo-badgeText" data-reactid="311">表达不好，思路不清，还喜欢吵架</div></div></div></div></div><div class="AnswerItem-extraInfo" data-reactid="312"><span class="Voters" data-reactid="313"><button class="Button Button--plain" type="button" data-reactid="314"><!-- react-text: 315 -->539 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 316 --></span></div></div><meta itemprop="image" content="" data-reactid="317"><meta itemprop="upvoteCount" content="539" data-reactid="318"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/106162086" data-reactid="319"><meta itemprop="dateCreated" content="2016-06-15T14:18:15.000Z" data-reactid="320"><meta itemprop="dateModified" content="2016-06-15T14:22:21.000Z" data-reactid="321"><meta itemprop="commentCount" content="100" data-reactid="322"><div class="RichContent RichContent--unescapable" data-reactid="323"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text">这条链接改变了我的人生……<br><a href="https://link.zhihu.com/?target=http%3A//scs.ryerson.ca/%7Eaharley/vis/conv/" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">scs.ryerson.ca/~aharley</span><span class="invisible">/vis/conv/</span><span class="ellipsis"></span></a><br>不过需要电脑才能打开，不方便的童鞋呐，可以看下边的文章预热一下～<br><a href="https://link.zhihu.com/?target=http%3A//m.huxiu.com/article/138857/1.html" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">m.huxiu.com/article/138</span><span class="invisible">857/1.html</span><span class="ellipsis"></span></a></span><!-- react-empty: 400 --></div><div data-reactid="327"><div class="ContentItem-time" data-reactid="328"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/106162086" data-reactid="329"><span data-tooltip="发布于 2016-06-15" data-reactid="330"><!-- react-text: 331 -->编辑于 <!-- /react-text --><!-- react-text: 332 -->2016-06-15<!-- /react-text --></span></a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 407 -->539<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 414 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 417 -->100 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-84603-86156-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84603-86156-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 422 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 425 -->分享<!-- /react-text --></button></div><!-- react-empty: 426 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 429 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 432 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 435 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 438 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 376 --><!-- react-empty: 631 --><!-- react-empty: 378 --><!-- react-empty: 379 --><!-- react-empty: 632 --><!-- react-empty: 381 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="2" data-zop="{&quot;authorName&quot;:&quot;雨宫夏一&quot;,&quot;itemId&quot;:81026163,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="81026163" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;81026163&quot;,&quot;upvote_num&quot;:1071,&quot;comment_num&quot;:58,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;24d3215758247a73f33018923f68c023&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="雨宫夏一"><meta itemprop="image" content="https://pic2.zhimg.com/v2-6ee66c21921303977a301f04ec65ffc7_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/sherwood-zheng"><meta itemprop="zhihu:followerCount" content="2132"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-84603-37687-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84603-37687-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/sherwood-zheng"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-6ee66c21921303977a301f04ec65ffc7_xs.jpg" srcset="https://pic2.zhimg.com/v2-6ee66c21921303977a301f04ec65ffc7_l.jpg 2x" alt="雨宫夏一"></a></div><!-- react-empty: 452 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-84603-53322-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-84603-53322-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/sherwood-zheng">雨宫夏一</a></div><!-- react-empty: 459 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">XMU（厦门大学）硕士在读，移动视觉搜索</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 466 -->1,071 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 467 --></span></div></div><meta itemprop="image" content="https://pic2.zhimg.com/d68d271178e7dd3a0fd47ec44e7fefa0_200x112.jpg"><meta itemprop="upvoteCount" content="1071"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/81026163"><meta itemprop="dateCreated" content="2016-01-11T06:56:42.000Z"><meta itemprop="dateModified" content="2017-08-30T05:21:31.000Z"><meta itemprop="commentCount" content="58"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>开始正式的答题，学历尚浅，多有偏颇之处，还望指正</p><p>来源资料：PRML，cs231n，以及部分论文</p><p>其实我们在做线性回归也好，分类（逻辑斯蒂回归）也好，本质上来讲，就是把数据进行映射，要么映射到一个多个离散的标签上，或者是连续的空间里面，一般简单的数据而言，我们很好拟合，只要线性变化一下，然后学习出最好的W就可以了，但是对于一些比较复杂的数据怎么办呢？比如说，对于一个二分类问题，特别是高纬度复杂化之后，数据不一定是线性可分的，这个时候，我们的basis function隆重登场，我们可以把数据进行一定的映射，转变，非线性的线性的，转变之后，就可以进行分类，最明显的例子在andrew NG在讲SVM里面的例子就很好的说明了，但是这个时候问题来了，对于一个很复杂，高维度的数据，我们如何才能找到最好的basis function呢？这个时候，神经网络隆重登场，我们把我们的basis function打开来，我们把误差转递到basis function的里面，通过这样的方式，来得到最好的basis function，同理，我们可以无限打开basis function，一直打开，对应的也就是一层神经网络（具体出自于prml关于神经网络的章节最开始简介的部分），但是问题来了，对于图片怎么办？我们知道，对于图片而言，图片是一个二维度的数据，我们怎样才能通过学习图片正确的模式来对于一张图片有正确的对于图片分类呢？这个时候，有人就提出了一个观点，我们可以这样，对于所有的像素，全部都连接上一个权值，我们也分很多层，然后最后进行分类，这样也可以，但是对于一张图片来说，像素点太多，参数太多了。然后就有人提出来，我们只看一部分怎么样，就是对于一张图片来说，我们只看一个小窗口就可以了，对于其他的地方，我们也提供类似的小窗口，我们知道，当我们对图片进行卷积的时候，我们可以对图片进行很多操作，比如说图片整体模糊，或者是边缘的提取，卷积操作对于图片来说可以很好的提取到特征，而且通过BP误差的传播，我们可以根据不同任务，得到对于这个任务最好的一个参数，学习出相对于这个任务的最好的卷积核，之所以权值共享的逻辑是：如果说一个卷积核在图片的一小块儿区域可以得到很好的特征，那么在其他的地方，也可以得到很好的特征。</p><p>这就有了alex net的提出，通过对图片进行五层（不知道有没有记忆错误）的卷积，然后后面三层的全连接，我们可以得到一个很好的结果，特别的相对于更大的数据集而言，最好参数越多越好，也就是网络最好更加深，更加的宽。</p><p>但是神经网络到底是什么？对于一批数据我们有很多的问题，为什么设置五层最好，batchsize多少比较好，每一层多少个卷积核（这个到现在我依旧没有一个更好的解释，每一个应该多少卷积核），宽度多少？要不要LRN？每一层都代表了什么？</p><p>这些的解释，就要好好看看今年CVPR的文章Visualizing and Understanding Convolutional Networks  这篇文章写的很棒，而且2015 CVPR出了很多对于卷积神经网络理解的文章，这篇文章提出了一个反卷积的方法(De-convolution)的方法，这样我们就可以好好看看每一层卷积神经网络到底做了什么事情：</p><p>首先第一层的返卷积（上面是反卷积的图片，下面对于第一层来说，激活值最大的图片）：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/d68d271178e7dd3a0fd47ec44e7fefa0_hd.jpg" data-rawwidth="403" data-rawheight="552" class="content_image" width="403"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/d68d271178e7dd3a0fd47ec44e7fefa0_hd.jpg" data-rawwidth="403" data-rawheight="552" class="content_image lazy" width="403" data-actualsrc="https://pic2.zhimg.com/50/d68d271178e7dd3a0fd47ec44e7fefa0_hd.jpg"></figure><p>我们看到，第一个卷积层只是表达了简单的图片的边缘而已，我们来看第二层：</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/8906f1eb5cc9bc86d15d1630c40a5b9f_hd.jpg" data-rawwidth="1147" data-rawheight="562" class="origin_image zh-lightbox-thumb" width="1147" data-original="https://pic3.zhimg.com/8906f1eb5cc9bc86d15d1630c40a5b9f_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/8906f1eb5cc9bc86d15d1630c40a5b9f_hd.jpg" data-rawwidth="1147" data-rawheight="562" class="origin_image zh-lightbox-thumb lazy" width="1147" data-original="https://pic3.zhimg.com/8906f1eb5cc9bc86d15d1630c40a5b9f_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/8906f1eb5cc9bc86d15d1630c40a5b9f_hd.jpg"></figure><p>第二层稍稍复杂了一点点，可以包含的不仅仅是一个边缘，可以是几个边缘的组合</p><p>第三层：</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/6f600422e7aeb19850b19c55aea04a11_hd.jpg" data-rawwidth="1472" data-rawheight="549" class="origin_image zh-lightbox-thumb" width="1472" data-original="https://pic1.zhimg.com/6f600422e7aeb19850b19c55aea04a11_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/6f600422e7aeb19850b19c55aea04a11_hd.jpg" data-rawwidth="1472" data-rawheight="549" class="origin_image zh-lightbox-thumb lazy" width="1472" data-original="https://pic1.zhimg.com/6f600422e7aeb19850b19c55aea04a11_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/6f600422e7aeb19850b19c55aea04a11_hd.jpg"></figure><p>第四层：</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/f9c5fe43099bde52bd9673db4383e10f_hd.jpg" data-rawwidth="666" data-rawheight="833" class="origin_image zh-lightbox-thumb" width="666" data-original="https://pic1.zhimg.com/f9c5fe43099bde52bd9673db4383e10f_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/f9c5fe43099bde52bd9673db4383e10f_hd.jpg" data-rawwidth="666" data-rawheight="833" class="origin_image zh-lightbox-thumb lazy" width="666" data-original="https://pic1.zhimg.com/f9c5fe43099bde52bd9673db4383e10f_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/f9c5fe43099bde52bd9673db4383e10f_hd.jpg"></figure><p>第五层：</p><figure><noscript>&lt;img src="https://pic7.zhimg.com/50/074357f5b07e0c14ecf740ca47a3c728_hd.jpg" data-rawwidth="667" data-rawheight="823" class="origin_image zh-lightbox-thumb" width="667" data-original="https://pic7.zhimg.com/074357f5b07e0c14ecf740ca47a3c728_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/074357f5b07e0c14ecf740ca47a3c728_hd.jpg" data-rawwidth="667" data-rawheight="823" class="origin_image zh-lightbox-thumb lazy" width="667" data-original="https://pic7.zhimg.com/074357f5b07e0c14ecf740ca47a3c728_r.jpg" data-actualsrc="https://pic7.zhimg.com/50/074357f5b07e0c14ecf740ca47a3c728_hd.jpg"></figure><p>我们看到，每一层都是对于一张图片从最基础的边缘，不断到最复杂的图片自己本身。</p><p>同时在进行反卷积的时候M.D. Zeiler and R. Fergus也发现，对于第一层的alexnet，会得到频度很高的像素（也就是颜色很深），所以他们也提出了应该要减小窗口，这样可以得到频度比较适中的像素：</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/1f0c506afdf7c4493d398bb9c2d20408_hd.jpg" data-rawwidth="561" data-rawheight="232" class="origin_image zh-lightbox-thumb" width="561" data-original="https://pic2.zhimg.com/1f0c506afdf7c4493d398bb9c2d20408_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/1f0c506afdf7c4493d398bb9c2d20408_hd.jpg" data-rawwidth="561" data-rawheight="232" class="origin_image zh-lightbox-thumb lazy" width="561" data-original="https://pic2.zhimg.com/1f0c506afdf7c4493d398bb9c2d20408_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/1f0c506afdf7c4493d398bb9c2d20408_hd.jpg"></figure><p>当图片卷积完之后，会把一个图片对于这一类本身最独特的部分凸显出来，然后来进行判断，这一类到底是什么？有下面的实验截图：</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/4ef49fb833cc7a3510f81da4343ed1ae_hd.jpg" data-rawwidth="1448" data-rawheight="262" class="origin_image zh-lightbox-thumb" width="1448" data-original="https://pic4.zhimg.com/4ef49fb833cc7a3510f81da4343ed1ae_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/4ef49fb833cc7a3510f81da4343ed1ae_hd.jpg" data-rawwidth="1448" data-rawheight="262" class="origin_image zh-lightbox-thumb lazy" width="1448" data-original="https://pic4.zhimg.com/4ef49fb833cc7a3510f81da4343ed1ae_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/4ef49fb833cc7a3510f81da4343ed1ae_hd.jpg"></figure><p>最左边的图像是原图像，作者盖住不同的区域，来分析对于一张图片，经过五次卷积之后，到底是如何判断的，我们看到卷积到最后（左三），比较凸显出来的是狗的头部，左二和右二的意思是，当我们遮住不同的区域，判断是狗的几率，红色区域代表概率很高，蓝色区域代表概率很低，我们发现，当我们遮挡住狗的头的地方的时候，我们得到这个物体时狗的概率最低，这个侧面证明了，<b>所谓卷积神经网络，就是会自动的对于一张图片学习出最好的卷积核以及这些卷积核的组合方式，也就是对于一张图片的任务来说，求出最好的图片对于本任务的特征的表达，然后来进行判断</b></p><p>还有一篇文章也助于理解，</p><p>UnderstandingDeepImageRepresentationsbyInvertingThem</p><p>这篇对于卷积每一层都不断的还原到最原始的图片：</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/c3292580aa367cb7d07d5409d3d76f1c_hd.jpg" data-rawwidth="1759" data-rawheight="366" class="origin_image zh-lightbox-thumb" width="1759" data-original="https://pic3.zhimg.com/c3292580aa367cb7d07d5409d3d76f1c_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/c3292580aa367cb7d07d5409d3d76f1c_hd.jpg" data-rawwidth="1759" data-rawheight="366" class="origin_image zh-lightbox-thumb lazy" width="1759" data-original="https://pic3.zhimg.com/c3292580aa367cb7d07d5409d3d76f1c_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/c3292580aa367cb7d07d5409d3d76f1c_hd.jpg"></figure><p>越是到后面，图片越模糊，但是它自己独特的部分，却凸显了出来。（也就是这个猩猩还是狒狒的头的部分）</p><p>还望指正，多谢</p><p>反正我写的也没什么人看，随意干啥都行，标明作者就好了。</p><p><br></p><p>最后打一个小广告，有兴趣报考人工智能/CS硕士的童鞋，可以看看我的live</p><p><a href="https://www.zhihu.com/lives/885940369227907072" class="internal">一百天，从二本到厦大 985</a></p></span><!-- react-empty: 477 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/81026163"><span data-tooltip="发布于 2016-01-11"><!-- react-text: 482 -->编辑于 <!-- /react-text --><!-- react-text: 483 -->2017-08-30<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 3675 -->1.1K<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3682 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3685 -->58 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-98458-72998-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-98458-72998-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3690 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3693 -->分享<!-- /react-text --></button></div><!-- react-empty: 3694 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3697 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3700 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3703 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3706 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 527 --><!-- react-empty: 633 --><!-- react-empty: 529 --><!-- react-empty: 530 --><!-- react-empty: 634 --><!-- react-empty: 532 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="3" data-zop="{&quot;authorName&quot;:&quot;机器之心&quot;,&quot;itemId&quot;:203073911,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="203073911" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;203073911&quot;,&quot;upvote_num&quot;:550,&quot;comment_num&quot;:38,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;06a67981ced7a2e9f07005185605685c&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="机器之心"><meta itemprop="image" content="https://pic2.zhimg.com/v2-dd115d399e55c37e3312c8ee4713890e_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/ji-qi-zhi-xin-65"><meta itemprop="zhihu:followerCount" content="78925"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85314-47005-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85314-47005-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/org/ji-qi-zhi-xin-65"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-dd115d399e55c37e3312c8ee4713890e_xs.jpg" srcset="https://pic2.zhimg.com/v2-dd115d399e55c37e3312c8ee4713890e_l.jpg 2x" alt="机器之心"></a></div><!-- react-empty: 724 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85314-84719-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85314-84719-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/org/ji-qi-zhi-xin-65">机器之心</a></div><!-- react-empty: 731 --></div><a class="UserLink-badge" href="https://www.zhihu.com/question/48510028" target="_blank" data-tooltip="已认证的官方帐号"><svg viewBox="0 0 20 20" class="Icon Icon--badgeCert" width="16" height="16" aria-hidden="true" style="height: 16px; width: 16px;"><title>用户标识</title><g><g fill="none" fill-rule="evenodd">     <path d="M.64 11.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.12 3.21.656 4.083 1.735l-.125-.154c.876 1.085 2.304 1.093 3.195.028l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.28-2.238l.016.197c-.12-1.388.656-3.212 1.735-4.084l-.154.125c1.084-.876 1.093-2.304.028-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.28l.198-.016c-1.387.122-3.21-.655-4.083-1.734l.125.153C10.802-.265 9.374-.274 8.483.79L8.61.64c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.28 2.24L2.4 4.363c.12 1.387-.656 3.21-1.735 4.084l.154-.126C-.265 9.2-.274 10.626.79 11.517L.64 11.39z" fill="#0F88EB"></path>     <path d="M7.78 13.728l-2.633-3s-.458-.704.242-1.36c.7-.658 1.327-.22 1.327-.22L8.67 11.28l4.696-4.93s.663-.35 1.3.197c.635.545.27 1.382.27 1.382s-3.467 3.857-5.377 5.78c-.98.93-1.78.018-1.78.018z" fill="#FFF"></path>1   </g></g></svg></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">欢迎关注我们的微信公众号：机器之心（almosthuman2014）</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 742 -->550 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 743 --></span></div></div><meta itemprop="image" content="https://pic3.zhimg.com/v2-ce7584843d74f0c90b60b5439250635a_200x112.jpg"><meta itemprop="upvoteCount" content="550"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/203073911"><meta itemprop="dateCreated" content="2017-07-24T10:01:56.000Z"><meta itemprop="dateModified" content="2017-07-25T02:35:43.000Z"><meta itemprop="commentCount" content="38"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><blockquote>近日，Dishashree Gupta 在 Analyticsvidhya 上发表了一篇题为《Architecture of Convolutional Neural Networks (CNNs) demystified》的文章，对用于图像识别和分类的卷积神经网络架构作了深度揭秘；作者在文中还作了通盘演示，期望对 CNN 的工作机制有一个深入的剖析。机器之心对本文进行了编译，<a href="https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/" class=" wrap external" target="_blank" rel="nofollow noreferrer">原文链接在此</a>，希望对你有帮助。</blockquote><h2><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650728746%26idx%3D1%26sn%3D61e9cb824501ec7c505eb464e8317915%26scene%3D0%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">机器视角：长文揭秘图像处理和卷积神经网络架构</a></h2><p><br></p><h2><b>引言</b></h2><p>先坦白地说，有一段时间我无法真正理解深度学习。我查看相关研究论文和文章，感觉深度学习异常复杂。我尝试去理解神经网络及其变体，但依然感到困难。</p><p><br></p><p>接着有一天，我决定一步一步，从基础开始。我把技术操作的步骤分解开来，并手动执行这些步骤（和计算），直到我理解它们如何工作。这相当费时，且令人紧张，但是结果非凡。</p><p><br></p><p>现在，我不仅对深度学习有了全面的理解，还在此基础上有了好想法，因为我的基础很扎实。随意地应用神经网络是一回事，理解它是什么以及背后的发生机制是另外一回事。</p><p><br></p><p>今天，我将与你共享我的心得，展示我如何上手卷积神经网络并最终弄明白了它。我将做一个通盘的展示，从而使你对 CNN 的工作机制有一个深入的了解。</p><p><br></p><p>在本文中，我将会讨论 CNN 背后的架构，其设计初衷在于解决图像识别和分类问题。同时我也会假设你对神经网络已经有了初步了解。</p><p><br></p><h2><b>目录</b></h2><p>1.机器如何看图？</p><p>2.如何帮助神经网络识别图像？</p><p>3.定义卷积神经网络</p><ul><li>卷积层</li><li>池化层</li><li>输出层</li></ul><p>4.小结</p><p>5.使用 CNN 分类图像</p><p><br></p><h2><b>1. 机器如何看图？</b></h2><p>人类大脑是一非常强大的机器，每秒内能看（捕捉）多张图，并在意识不到的情况下就完成了对这些图的处理。但机器并非如此。机器处理图像的第一步是理解，理解如何表达一张图像，进而读取图片。</p><p><br></p><p>简单来说，每个图像都是一系列特定排序的图点（像素）。如果你改变像素的顺序或颜色，图像也随之改变。举个例子，存储并读取一张上面写着数字 4 的图像。</p><p><br></p><p>基本上，机器会把图像打碎成像素矩阵，存储每个表示位置像素的颜色码。在下图的表示中，数值 1 是白色，256 是最深的绿色（为了简化，我们示例限制到了一种颜色）。</p><p><br></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-ce7584843d74f0c90b60b5439250635a_hd.jpg" data-rawwidth="387" data-rawheight="380" class="content_image" width="387"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ce7584843d74f0c90b60b5439250635a_hd.jpg" data-rawwidth="387" data-rawheight="380" class="content_image lazy" width="387" data-actualsrc="https://pic3.zhimg.com/50/v2-ce7584843d74f0c90b60b5439250635a_hd.jpg"></figure><p><br></p><p>一旦你以这种格式存储完图像信息，下一步就是让神经网络理解这种排序与模式。</p><p><br></p><h2><b>2. 如何帮助神经网络识别图像？</b></h2><p>表征像素的数值是以特定的方式排序的。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-d6e748862f4f995047b53a87009b3fb5_hd.jpg" data-rawwidth="848" data-rawheight="808" class="origin_image zh-lightbox-thumb" width="848" data-original="https://pic4.zhimg.com/v2-d6e748862f4f995047b53a87009b3fb5_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-d6e748862f4f995047b53a87009b3fb5_hd.jpg" data-rawwidth="848" data-rawheight="808" class="origin_image zh-lightbox-thumb lazy" width="848" data-original="https://pic4.zhimg.com/v2-d6e748862f4f995047b53a87009b3fb5_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-d6e748862f4f995047b53a87009b3fb5_hd.jpg"></figure><p>假设我们尝试使用全连接网络识别图像，该如何做？</p><p><br></p><p>全连接网络可以通过平化它，把图像当作一个数组，并把像素值当作预测图像中数值的特征。明确地说，让网络理解理解下面图中发生了什么，非常的艰难。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-25be163b4687fa9f6bf174bc95613a21_hd.jpg" data-rawwidth="1340" data-rawheight="140" class="origin_image zh-lightbox-thumb" width="1340" data-original="https://pic2.zhimg.com/v2-25be163b4687fa9f6bf174bc95613a21_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-25be163b4687fa9f6bf174bc95613a21_hd.jpg" data-rawwidth="1340" data-rawheight="140" class="origin_image zh-lightbox-thumb lazy" width="1340" data-original="https://pic2.zhimg.com/v2-25be163b4687fa9f6bf174bc95613a21_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-25be163b4687fa9f6bf174bc95613a21_hd.jpg"></figure><p>即使人类也很难理解上图中表达的含义是数字 4。我们完全丢失了像素的空间排列。</p><p>我们能做什么呢？可以尝试从原图像中提取特征，从而保留空间排列。</p><p><br></p><h2><b>案例 1</b></h2><p>这里我们使用一个权重乘以初始像素值。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-97541d570892f2b8580758655b2b737c_hd.jpg" data-rawwidth="1326" data-rawheight="604" class="origin_image zh-lightbox-thumb" width="1326" data-original="https://pic1.zhimg.com/v2-97541d570892f2b8580758655b2b737c_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-97541d570892f2b8580758655b2b737c_hd.jpg" data-rawwidth="1326" data-rawheight="604" class="origin_image zh-lightbox-thumb lazy" width="1326" data-original="https://pic1.zhimg.com/v2-97541d570892f2b8580758655b2b737c_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-97541d570892f2b8580758655b2b737c_hd.jpg"></figure><p>现在裸眼识别出这是「4」就变得更简单了。但把它交给全连接网络之前，还需要平整化（flatten) 它，要让我们能够保留图像的空间排列。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-ecb23f7cce51ed444ceefa1d35bd4240_hd.jpg" data-rawwidth="1354" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="1354" data-original="https://pic4.zhimg.com/v2-ecb23f7cce51ed444ceefa1d35bd4240_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ecb23f7cce51ed444ceefa1d35bd4240_hd.jpg" data-rawwidth="1354" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="1354" data-original="https://pic4.zhimg.com/v2-ecb23f7cce51ed444ceefa1d35bd4240_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-ecb23f7cce51ed444ceefa1d35bd4240_hd.jpg"></figure><p><br></p><h2><b>案例 2</b></h2><p>现在我们可以看到，把图像平整化完全破坏了它的排列。我们需要想出一种方式在没有平整化的情况下把图片馈送给网络，并且还要保留空间排列特征，也就是需要馈送像素值的 2D/3D 排列。</p><p>我们可以尝试一次采用图像的两个像素值，而非一个。这能给网络很好的洞见，观察邻近像素的特征。既然一次采用两个像素，那也就需要一次采用两个权重值了</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-5a8235754cd0ae57f475683ed6cba2b5_hd.gif" data-rawwidth="994" data-rawheight="383" data-thumbnail="https://pic3.zhimg.com/50/v2-5a8235754cd0ae57f475683ed6cba2b5_hd.jpg" class="origin_image zh-lightbox-thumb" width="994" data-original="https://pic3.zhimg.com/v2-5a8235754cd0ae57f475683ed6cba2b5_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-5a8235754cd0ae57f475683ed6cba2b5_hd.jpg" data-thumbnail="https://pic3.zhimg.com/50/v2-5a8235754cd0ae57f475683ed6cba2b5_hd.jpg"></div></div></figure><p><br></p><p>希望你能注意到图像从之前的 4 列数值变成了 3 列。因为我们现在一次移用两个像素（在每次移动中像素被共享），图像变的更小了。虽然图像变小了，我们仍能在很大程度上理解这是「4」。而且，要意识到的一个重点是，我们采用的是两个连贯的水平像素，因此只会考虑水平的排列。</p><p><br></p><p>这是我们从图像中提取特征的一种方式。我们可以看到左边和中间部分，但右边部分看起来不那么清楚。主要是因为两个问题：</p><p><br></p><p>1. 图片角落左边和右边是权重相乘一次得到的。</p><p>2. 左边仍旧保留，因为权重值高；右边因为略低的权重，有些丢失。</p><p><br></p><p>现在我们有两个问题，需要两个解决方案。</p><p><br></p><h2><b>案例 3</b></h2><p>遇到的问题是图像左右两角只被权重通过一次。我们需要做的是让网络像考虑其他像素一样考虑角落。我们有一个简单的方法解决这一问题：把零放在权重运动的两边。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-0014d5f30b87ae12947cffa0f429d5f1_hd.gif" data-rawwidth="1225" data-rawheight="389" data-thumbnail="https://pic2.zhimg.com/50/v2-0014d5f30b87ae12947cffa0f429d5f1_hd.jpg" class="origin_image zh-lightbox-thumb" width="1225" data-original="https://pic2.zhimg.com/v2-0014d5f30b87ae12947cffa0f429d5f1_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-0014d5f30b87ae12947cffa0f429d5f1_hd.jpg" data-thumbnail="https://pic2.zhimg.com/50/v2-0014d5f30b87ae12947cffa0f429d5f1_hd.jpg"></div></div></figure><p><br></p><p>你可以看到通过添加零，来自角落的信息被再训练。图像也变得更大。这可被用于我们不想要缩小图像的情况下。</p><p><br></p><h2><b>案例 4</b></h2><p>这里我们试图解决的问题是右侧角落更小的权重值正在降低像素值，因此使其难以被我们识别。我们所能做的是采取多个权重值并将其结合起来。</p><p><br></p><p>(1,0.3) 的权重值给了我们一个输出表格</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-ca6e6b4afb696bccdcb67687a04c44da_hd.jpg" data-rawwidth="333" data-rawheight="376" class="content_image" width="333"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ca6e6b4afb696bccdcb67687a04c44da_hd.jpg" data-rawwidth="333" data-rawheight="376" class="content_image lazy" width="333" data-actualsrc="https://pic2.zhimg.com/50/v2-ca6e6b4afb696bccdcb67687a04c44da_hd.jpg"></figure><p><br></p><p>同时表格 (0.1,5) 的权重值也将给我们一个输出表格。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-cf8ff64d38ce55d2c1d1513b08e079e2_hd.jpg" data-rawwidth="331" data-rawheight="396" class="content_image" width="331"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-cf8ff64d38ce55d2c1d1513b08e079e2_hd.jpg" data-rawwidth="331" data-rawheight="396" class="content_image lazy" width="331" data-actualsrc="https://pic2.zhimg.com/50/v2-cf8ff64d38ce55d2c1d1513b08e079e2_hd.jpg"></figure><p>两张图像的结合版本将会给我们一个清晰的图片。因此，我们所做的是简单地使用多个权重而不是一个，从而再训练图像的更多信息。最终结果将是上述两张图像的一个结合版本。</p><p><br></p><h2><b>案例 5</b></h2><p>我们到现在通过使用权重，试图把水平像素（horizontal pixel）结合起来。但是大多数情况下我们需要在水平和垂直方向上保持空间布局。我们采取 2D 矩阵权重，把像素在水平和垂直方向上结合起来。同样，记住已经有了水平和垂直方向的权重运动，输出会在水平和垂直方向上低一个像素。</p><p><br></p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-f1206f480077d6064e2a445ebef4ea66_hd.gif" data-rawwidth="1002" data-rawheight="363" data-thumbnail="https://pic1.zhimg.com/50/v2-f1206f480077d6064e2a445ebef4ea66_hd.jpg" class="origin_image zh-lightbox-thumb" width="1002" data-original="https://pic1.zhimg.com/v2-f1206f480077d6064e2a445ebef4ea66_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-f1206f480077d6064e2a445ebef4ea66_hd.jpg" data-thumbnail="https://pic1.zhimg.com/50/v2-f1206f480077d6064e2a445ebef4ea66_hd.jpg"></div></div></figure><p><br></p><p>特别感谢 Jeremy Howard 启发我创作了这些图像。</p><p><br></p><h2><b>因此我们做了什么？</b></h2><p>上面我们所做的事是试图通过使用图像的空间的安排从图像中提取特征。为了理解图像，理解像素如何安排对于一个网络极其重要。上面我们所做的也恰恰是一个卷积网络所做的。我们可以采用输入图像，定义权重矩阵，并且输入被卷积以从图像中提取特殊特征而无需损失其有关空间安排的信息。</p><p><br></p><p>这个方法的另一个重大好处是它可以减少图像的参数数量。正如所见，卷积图像相比于原始图像有更少的像素。</p><p><br></p><h2><b>3.定义一个卷积神经网络</b></h2><p>我们需要三个基本的元素来定义一个基本的卷积网络</p><p><br></p><p>1. 卷积层</p><p>2. 池化层（可选）</p><p>3. 输出层</p><p><br></p><h2><b>卷积层</b></h2><p>在这一层中，实际所发生的就像我们在上述案例 5 中见到的一样。假设我们有一个 6*6 的图像。我们定义一个权值矩阵，用来从图像中提取一定的特征。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-1e95f13c1f0d650e826e813c0e1f493a_hd.jpg" data-rawwidth="425" data-rawheight="159" class="origin_image zh-lightbox-thumb" width="425" data-original="https://pic1.zhimg.com/v2-1e95f13c1f0d650e826e813c0e1f493a_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-1e95f13c1f0d650e826e813c0e1f493a_hd.jpg" data-rawwidth="425" data-rawheight="159" class="origin_image zh-lightbox-thumb lazy" width="425" data-original="https://pic1.zhimg.com/v2-1e95f13c1f0d650e826e813c0e1f493a_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-1e95f13c1f0d650e826e813c0e1f493a_hd.jpg"></figure><p>我们把权值初始化成一个 3*3 的矩阵。这个权值现在应该与图像结合，所有的像素都被覆盖至少一次，从而来产生一个卷积化的输出。上述的 429，是通过计算权值矩阵和输入图像的 3*3 高亮部分以元素方式进行的乘积的值而得到的。</p><p><br></p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-f9dab2820dc8a941e62c1e5c63e418f1_hd.gif" data-rawwidth="487" data-rawheight="168" data-thumbnail="https://pic4.zhimg.com/50/v2-f9dab2820dc8a941e62c1e5c63e418f1_hd.jpg" class="origin_image zh-lightbox-thumb" width="487" data-original="https://pic4.zhimg.com/v2-f9dab2820dc8a941e62c1e5c63e418f1_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer is-playing" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-f9dab2820dc8a941e62c1e5c63e418f1_hd.gif" data-thumbnail="https://pic4.zhimg.com/50/v2-f9dab2820dc8a941e62c1e5c63e418f1_hd.jpg"></div></div></figure><p><br></p><p>现在 6*6 的图像转换成了 4*4 的图像。想象一下权值矩阵就像用来刷墙的刷子。首先在水平方向上用这个刷子进行刷墙，然后再向下移，对下一行进行水平粉刷。当权值矩阵沿着图像移动的时候，像素值再一次被使用。实际上，这样可以使参数在卷积神经网络中被共享。</p><p><br></p><p>下面我们以一个真实图像为例。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-56acff7742146f37317e4c36aba47234_hd.jpg" data-rawwidth="640" data-rawheight="249" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-56acff7742146f37317e4c36aba47234_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-56acff7742146f37317e4c36aba47234_hd.jpg" data-rawwidth="640" data-rawheight="249" class="origin_image zh-lightbox-thumb lazy" width="640" data-original="https://pic2.zhimg.com/v2-56acff7742146f37317e4c36aba47234_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-56acff7742146f37317e4c36aba47234_hd.jpg"></figure><p>权值矩阵在图像里表现的像一个从原始图像矩阵中提取特定信息的过滤器。一个权值组合可能用来提取边缘（edge）信息，另一个可能是用来提取一个特定颜色，下一个就可能就是对不需要的噪点进行模糊化。</p><p>先对权值进行学习，然后损失函数可以被最小化，类似于多层感知机（MLP）。因此需要通过对参数进行学习来从原始图像中提取信息，从而来帮助网络进行正确的预测。当我们有多个卷积层的时候，初始层往往提取较多的一般特征，随着网络结构变得更深，权值矩阵提取的特征越来越复杂，并且越来越适用于眼前的问题。</p><p><br></p><h2><b>步长（stride）和边界（padding）的概念</b></h2><p>像我们在上面看到的一样，过滤器或者说权值矩阵，在整个图像范围内一次移动一个像素。我们可以把它定义成一个超参数（hyperparameter），从而来表示我们想让权值矩阵在图像内如何移动。如果权值矩阵一次移动一个像素，我们称其步长为 1。下面我们看一下步长为 2 时的情况。</p><p><br></p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-2d5ce7b1af041dab1b4019cc2776b71b_hd.gif" data-rawwidth="421" data-rawheight="150" data-thumbnail="https://pic4.zhimg.com/50/v2-2d5ce7b1af041dab1b4019cc2776b71b_hd.jpg" class="origin_image zh-lightbox-thumb" width="421" data-original="https://pic4.zhimg.com/v2-2d5ce7b1af041dab1b4019cc2776b71b_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-2d5ce7b1af041dab1b4019cc2776b71b_hd.jpg" data-thumbnail="https://pic4.zhimg.com/50/v2-2d5ce7b1af041dab1b4019cc2776b71b_hd.jpg"></div></div></figure><p><br></p><p>你可以看见当我们增加步长值的时候，图像的规格持续变小。在输入图像四周填充 0 边界可以解决这个问题。我们也可以在高步长值的情况下在图像四周填加不只一层的 0 边界。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-67c54ac5b709f0d13ce78e9e17c8991f_hd.jpg" data-rawwidth="246" data-rawheight="188" class="content_image" width="246"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-67c54ac5b709f0d13ce78e9e17c8991f_hd.jpg" data-rawwidth="246" data-rawheight="188" class="content_image lazy" width="246" data-actualsrc="https://pic3.zhimg.com/50/v2-67c54ac5b709f0d13ce78e9e17c8991f_hd.jpg"></figure><p>我们可以看见在我们给图像填加一层 0 边界后，图像的原始形状是如何被保持的。由于输出图像和输入图像是大小相同的，所以这被称为 same padding。</p><p><br></p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-19f50c58341de1d5c4700972a718b8e1_hd.gif" data-rawwidth="419" data-rawheight="189" data-thumbnail="https://pic4.zhimg.com/50/v2-19f50c58341de1d5c4700972a718b8e1_hd.jpg" class="content_image" width="419"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-19f50c58341de1d5c4700972a718b8e1_hd.jpg" data-thumbnail="https://pic4.zhimg.com/50/v2-19f50c58341de1d5c4700972a718b8e1_hd.jpg"></div></div></figure><p><br></p><p>这就是 same padding（意味着我们仅考虑输入图像的有效像素）。中间的 4*4 像素是相同的。这里我们已经利用边界保留了更多信息，并且也已经保留了图像的原大小。</p><p><br></p><p><b>多过滤与激活图</b></p><p>需要记住的是权值的纵深维度（depth dimension）和输入图像的纵深维度是相同的。权值会延伸到输入图像的整个深度。因此，和一个单一权值矩阵进行卷积会产生一个单一纵深维度的卷积化输出。大多数情况下都不使用单一过滤器（权值矩阵），而是应用维度相同的多个过滤器。</p><p><br></p><p>每一个过滤器的输出被堆叠在一起，形成卷积图像的纵深维度。假设我们有一个 32*32*3 的输入。我们使用 5*5*3，带有 valid padding 的 10 个过滤器。输出的维度将会是 28*28*10。</p><p><br></p><p>如下图所示：</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-defbc123f52a27bddd902761243121f8_hd.jpg" data-rawwidth="640" data-rawheight="220" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-defbc123f52a27bddd902761243121f8_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-defbc123f52a27bddd902761243121f8_hd.jpg" data-rawwidth="640" data-rawheight="220" class="origin_image zh-lightbox-thumb lazy" width="640" data-original="https://pic3.zhimg.com/v2-defbc123f52a27bddd902761243121f8_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-defbc123f52a27bddd902761243121f8_hd.jpg"></figure><p><br></p><p>激活图是卷积层的输出。</p><p><br></p><h2><b>池化层</b></h2><p>有时图像太大，我们需要减少训练参数的数量，它被要求在随后的卷积层之间周期性地引进池化层。池化的唯一目的是减少图像的空间大小。池化在每一个纵深维度上独自完成，因此图像的纵深保持不变。池化层的最常见形式是最大池化。</p><p><br></p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-cae74f34159e48d581156d80e8e12ec6_hd.jpg" data-rawwidth="259" data-rawheight="110" class="content_image" width="259"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-cae74f34159e48d581156d80e8e12ec6_hd.jpg" data-rawwidth="259" data-rawheight="110" class="content_image lazy" width="259" data-actualsrc="https://pic1.zhimg.com/50/v2-cae74f34159e48d581156d80e8e12ec6_hd.jpg"></figure><p>在这里，我们把步幅定为 2，池化尺寸也为 2。最大化执行也应用在每个卷机输出的深度尺寸中。正如你所看到的，最大池化操作后，4*4 卷积的输出变成了 2*2。</p><p>让我们看看最大池化在真实图片中的效果如何。</p><p><br></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-e62be992ba35c0f79a2f54191da1defb_hd.jpg" data-rawwidth="640" data-rawheight="246" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-e62be992ba35c0f79a2f54191da1defb_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-e62be992ba35c0f79a2f54191da1defb_hd.jpg" data-rawwidth="640" data-rawheight="246" class="origin_image zh-lightbox-thumb lazy" width="640" data-original="https://pic3.zhimg.com/v2-e62be992ba35c0f79a2f54191da1defb_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-e62be992ba35c0f79a2f54191da1defb_hd.jpg"></figure><p><br></p><p>正如你看到的，我们卷积了图像，并最大池化了它。最大池化图像仍然保留了汽车在街上的信息。如果你仔细观察的话，你会发现图像的尺寸已经减半。这可以很大程度上减少参数。</p><p><br></p><p>同样，其他形式的池化也可以在系统中应用，如平均池化和 L2 规范池化。</p><p><br></p><h2><b>输出维度</b></h2><p>理解每个卷积层输入和输出的尺寸可能会有点难度。以下三点或许可以让你了解输出尺寸的问题。有三个超参数可以控制输出卷的大小。</p><p><br></p><p>1. 过滤器数量-输出卷的深度与过滤器的数量成正比。请记住该如何堆叠每个过滤器的输出以形成激活映射。激活图的深度等于过滤器的数量。</p><p>2. 步幅（Stride）-如果步幅是 1，那么我们处理图片的精细度就进入单像素级别了。更高的步幅意味着同时处理更多的像素，从而产生较小的输出量。</p><p>3. 零填充（zero padding）-这有助于我们保留输入图像的尺寸。如果添加了单零填充，则单步幅过滤器的运动会保持在原图尺寸。</p><p><br></p><p>我们可以应用一个简单的公式来计算输出尺寸。输出图像的空间尺寸可以计算为（[W-F + 2P] / S）+1。在这里，W 是输入尺寸，F 是过滤器的尺寸，P 是填充数量，S 是步幅数字。假如我们有一张 32*32*3 的输入图像，我们使用 10 个尺寸为 3*3*3 的过滤器，单步幅和零填充。</p><p><br></p><p>那么 W=32，F=3，P=0，S=1。输出深度等于应用的滤波器的数量，即 10，输出尺寸大小为 ([32-3+0]/1)+1 = 30。因此输出尺寸是 30*30*10。</p><p><br></p><h2><b>输出层</b></h2><p>在多层卷积和填充后，我们需要以类的形式输出。卷积和池化层只会提取特征，并减少原始图像带来的参数。然而，为了生成最终的输出，我们需要应用全连接层来生成一个等于我们需要的类的数量的输出。仅仅依靠卷积层是难以达到这个要求的。卷积层可以生成 3D 激活图，而我们只需要图像是否属于一个特定的类这样的内容。输出层具有类似分类交叉熵的损失函数，用于计算预测误差。一旦前向传播完成，反向传播就会开始更新权重与偏差，以减少误差和损失。</p><p><br></p><h2><b>4. 小结</b></h2><p>正如你所看到的，CNN 由不同的卷积层和池化层组成。让我们看看整个网络是什么样子：</p><p><br></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-a182b75112c9c6d60892a95f47359d24_hd.jpg" data-rawwidth="640" data-rawheight="188" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-a182b75112c9c6d60892a95f47359d24_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-a182b75112c9c6d60892a95f47359d24_hd.jpg" data-rawwidth="640" data-rawheight="188" class="origin_image zh-lightbox-thumb lazy" width="640" data-original="https://pic3.zhimg.com/v2-a182b75112c9c6d60892a95f47359d24_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-a182b75112c9c6d60892a95f47359d24_hd.jpg"></figure><p><br></p><ul><li>我们将输入图像传递到第一个卷积层中，卷积后以激活图形式输出。图片在卷积层中过滤后的特征会被输出，并传递下去。<br></li><li>每个过滤器都会给出不同的特征，以帮助进行正确的类预测。因为我们需要保证图像大小的一致，所以我们使用同样的填充（零填充），否则填充会被使用，因为它可以帮助减少特征的数量。<br></li><li>随后加入池化层进一步减少参数的数量。<br></li><li>在预测最终提出前，数据会经过多个卷积和池化层的处理。卷积层会帮助提取特征，越深的卷积神经网络会提取越具体的特征，越浅的网络提取越浅显的特征。<br></li><li>如前所述，CNN 中的输出层是全连接层，其中来自其他层的输入在这里被平化和发送，以便将输出转换为网络所需的参数。<br></li><li>随后输出层会产生输出，这些信息会互相比较排除错误。损失函数是全连接输出层计算的均方根损失。随后我们会计算梯度错误。<br></li><li>错误会进行反向传播，以不断改进过滤器（权重）和偏差值。<br></li><li>一个训练周期由单次正向和反向传递完成。<br></li></ul><p><b>5. 在 KERAS 中使用 CNN 对图像进行分类</b></p><p>让我们尝试一下，输入猫和狗的图片，让计算机识别它们。这是图像识别和分类的经典问题，机器在这里需要做的是看到图像，并理解猫与狗的不同外形特征。这些特征可以是外形轮廓，也可以是猫的胡须之类，卷积层会攫取这些特征。让我们把数据集拿来试验一下吧。</p><p><br></p><p>以下这些图片均来自数据集。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-298e3e184c94749666abf4e7102afbd2_hd.jpg" data-rawwidth="640" data-rawheight="254" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-298e3e184c94749666abf4e7102afbd2_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-298e3e184c94749666abf4e7102afbd2_hd.jpg" data-rawwidth="640" data-rawheight="254" class="origin_image zh-lightbox-thumb lazy" width="640" data-original="https://pic2.zhimg.com/v2-298e3e184c94749666abf4e7102afbd2_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-298e3e184c94749666abf4e7102afbd2_hd.jpg"></figure><p><br></p><p>我们首先需要调整这些图像的大小，让它们形状相同。这是处理图像之前通常需要做的，因为在拍照时，让照下的图像都大小相同几乎不可能。</p><p><br></p><p>为了简化理解，我们在这里只用一个卷积层和一个池化层。注意：在 CNN 的应用阶段，这种简单的情况是不会发生的。</p><div class="highlight"><pre><code class="language-text"><span></span>#import various packagesimport osimport numpy as npimport pandas as pdimport scipyimport sklearnimport kerasfrom keras.models import Sequentialimport cv2from skimage import io
%matplotlib inline

#Defining the File Path

cat=os.listdir("/mnt/hdd/datasets/dogs_cats/train/cat")
dog=os.listdir("/mnt/hdd/datasets/dogs_cats/train/dog")
filepath="/mnt/hdd/datasets/dogs_cats/train/cat/"filepath2="/mnt/hdd/datasets/dogs_cats/train/dog/"#Loading the Images

images=[]
label = []for i in cat:
image = scipy.misc.imread(filepath+i)
images.append(image)
label.append(0) #for cat imagesfor i in dog:
image = scipy.misc.imread(filepath2+i)
images.append(image)
label.append(1) #for dog images

#resizing all the imagesfor i in range(0,23000):
images[i]=cv2.resize(images[i],(300,300))

#converting images to arrays

images=np.array(images)
label=np.array(label)

# Defining the hyperparameters

filters=10filtersize=(5,5)

epochs =5batchsize=128input_shape=(300,300,3)

#Converting the target variable to the required sizefrom keras.utils.np_utils import to_categorical
label = to_categorical(label)

#Defining the model

model = Sequential()

model.add(keras.layers.InputLayer(input_shape=input_shape))

model.add(keras.layers.convolutional.Conv2D(filters, filtersize, strides=(1, 1), padding='valid', data_format="channels_last", activation='relu'))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Flatten())

model.add(keras.layers.Dense(units=2, input_dim=50,activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(images, label, epochs=epochs, batch_size=batchsize,validation_split=0.3)

model.summary()
</code></pre></div><p><br></p><p>在这一模型中，我只使用了单一卷积和池化层，可训练参数是 219,801。很好奇如果我在这种情况使用了 MLP 会有多少参数。通过增加更多的卷积和池化层，你可以进一步降低参数的数量。我们添加的卷积层越多，被提取的特征就会更具体和复杂。</p><p><br></p><p>在该模型中，我只使用了一个卷积层和池化层，可训练参数量为 219,801。如果想知道使用 MLP 在这种情况下会得到多少，你可以通过加入更多卷积和池化层来减少参数的数量。越多的卷积层意味着提取出来的特征更加具体，更加复杂。</p><p><br></p><h2><b>结语</b></h2><p>希望本文能够让你认识卷积神经网络，这篇文章没有深入 CNN 的复杂数学原理。如果希望增进了解，你可以尝试构建自己的卷积神经网络，借此来了解它运行和预测的原理。</p><p><br></p><p>本文首发于微信公众号：机器之心（almosthuman2014），如需转载，请私信联系，感谢。</p></span><!-- react-empty: 753 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/203073911"><span data-tooltip="发布于 2017-07-24"><!-- react-text: 758 -->编辑于 <!-- /react-text --><!-- react-text: 759 -->2017-07-25<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 3724 -->550<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3731 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3734 -->38 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-64353-80518-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-64353-80518-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3739 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3742 -->分享<!-- /react-text --></button></div><!-- react-empty: 3743 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3746 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3749 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3752 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3755 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 803 --><!-- react-empty: 2582 --><!-- react-empty: 805 --><!-- react-empty: 806 --><!-- react-empty: 2583 --><!-- react-empty: 808 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="4" data-zop="{&quot;authorName&quot;:&quot;张旭&quot;,&quot;itemId&quot;:120211609,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="120211609" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;120211609&quot;,&quot;upvote_num&quot;:271,&quot;comment_num&quot;:35,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;acaf610bd2901defb60d945109f87ca2&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="张旭"><meta itemprop="image" content="https://pic4.zhimg.com/7cd593e7b0004a7a14302e5beff623a2_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/zhang-xu-99-99"><meta itemprop="zhihu:followerCount" content="428"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85722-12-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85722-12-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/zhang-xu-99-99"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/7cd593e7b0004a7a14302e5beff623a2_xs.jpg" srcset="https://pic4.zhimg.com/7cd593e7b0004a7a14302e5beff623a2_l.jpg 2x" alt="张旭"></a></div><!-- react-empty: 822 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85722-49628-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85722-49628-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/zhang-xu-99-99">张旭</a></div><!-- react-empty: 829 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">薛定谔的猫</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 836 -->271 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 837 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="271"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/120211609"><meta itemprop="dateCreated" content="2016-09-02T11:08:56.000Z"><meta itemprop="dateModified" content="2017-11-13T06:27:08.000Z"><meta itemprop="commentCount" content="35"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>谈点个人理解，占个坑。<br>按照我的理解，CNN的核心其实就是卷积核的作用，只要明白了这个问题，其余的就都是数学坑了（当然，相比较而言之后的数学坑更难）。</p><p>如果学过数字图像处理，对于卷积核的作用应该不陌生，比如你做一个最简单的方向滤波器，那就是一个二维卷积核，这个核其实就是一个模板，利用这个模板再通过卷积计算的定义就可以计算出一幅新的图像，新的图像会把这个卷积核所体现的特征突出显示出来。比如这个卷积核可以侦测水平纹理，那卷积出来的图就是原图水平纹理的图像。</p><p>现在假设要做一个图像的分类问题，比如辨别一个图像里是否有一只猫，我们可以先判断是否有猫的头，猫的尾巴，猫的身子等等，如果这些特征都具备，那么我就判定这应该是一只猫（如果用心的话你就会发现这就是CNN最后的分类层，这一部分是我们传统的神经网络的范畴）。关键在于这些特征是高级的语义特征，这种特征怎么用卷积核提取呢？</p><p>原来的卷积核都是人工事先定义好的，是经过算法设计人员精心设计的，他们发现这样或那样的设计卷积核通过卷积运算可以突出一个什么样的特征，于是就高高兴兴的拿去卷积了。但是现在我们所需要的这种特征太高级了，而且随任务的不同而不同，人工设计这样的卷积核非常困难。</p><p>于是，利用机器学习的思想，我们可以让他自己去学习出卷积核来！也就是学习出特征！</p><p>如前所述，判断是否是一只猫，只有一个特征不够，比如仅仅有猫头是不足的，因此需要多个高级语义特征的组合，所以应该需要多个卷积核，这就是为什么需要学习多个卷积核的原因。</p><p>还有一个问题，那就是为什么CNN要设计这么多层呢？首先，应该要明白，猫的头是一个特征，但是对于充斥着像素点的图像来说，用几个卷积核直接判断存在一个猫头的还是太困难，怎么办？简单，把猫头也作为一个识别目标，比如猫头应该具有更底层的一些语义特征，比如应该有猫的眼睛、猫的耳朵、猫的鼻子等等。这些特征有的还是太高级了，没关系，继续向下寻找低级特征，一直到最低级的像素点，这样就构成了多层的神经网络。</p><p>最好，CNN最不好理解的就要放大招了。虽然我们之前一直用一些我们人常见的语义特征做例子，但是实际上CNN会学习出猫头、猫尾巴、猫身然后经判定这是猫吗？显然我们的CNN完全不知道什么叫猫头、猫尾巴，也就是说，CNN不知道什么是猫头猫尾巴，它学习到的只是一种抽象特征，甚至可能有些特征在现实世界并没有对应的名词，但是这些特征组合在一起计算机就会判定这是一只猫！关于这一点，确实有些难以理解，比如一个人判断猫是看看有没有猫头、猫身子、猫尾巴，但是另一个选取的特征就是有没有猫的毛，猫的爪子，还有的人更加奇怪，他会去通过这张图像里是不是有老鼠去判断，而我们的CNN，则是用它自己学习到的特征去判断。</p><p>———————————分割线——————————————————————</p><p>最近又看了一些资料，在此纠正和阐明一些问题。</p><p>目前CNN的可视化是一个很火的方向了，有些论文中已经提到了中间的卷积层特征其实也是具有现实的语义意义的，但是只是不那么清晰。</p><p>CNN称之为深度学习，要义就在这个深字上，对于CNN而言，这个深其实就是意味着层层的特征表示。比如浅层的特征，例如点、线、面之类的简单几何形状，都是在底层训练出来的，对于这些底层的特征继续进行组合表示，就是后面的若干层的任务。最后把从低级特征组合而来的高级特征在进一步变成语义特征，就可以使用全连接层进行分类了。也就是说，最后一次分类并不一定要用神经网络，如果已经拿到了足够好的特征信息，使用其余的分类器也未尝不可。</p><p>这就是为什么CNN可以fine-tune的原因，例如你要完成一个分类猫和狗的任务，你需要从头训练一个CNN网络吗？假设你的猫狗图片样本量并不是很大，这并不是一个好主意。好的办法是，拿一个经过大型图像数据集，你如ImageNet，训练过的大规模CNN（比如VGG NET）直接载入训练，这个过程称之为fine-tuning。因为这个CNN底层已经训练到了丰富的细节信息，你所需要训练的其实是上层对这些特征的组合信息，以及最后全连接层的分类信息，所以完全不需要从头再来。这也证明了CNN确实可以有迁移学习的能力。</p><p>———————————分割线——————————————————————</p><p>之前有一点没有说，今天没啥事补充一下，也算是做个记录。</p><p>CNN的部件其实大致分为三个，卷积层、池化层、全连接层，这也是LeNet-5的经典结构，之后大部分CNN网络其实都是在这三个基本部件上做各种组合和改进。卷积层之前已经介绍过了，全连接层就是连在最后的分类器，是一个普通的bp网络，实际上如果训练得到的特征足够好，这里也可以选择其他的分类器，比如SVM等。</p><p>那么池化层是干什么的呢？池化，英文是pooling，字面上看挺难懂，但其实这可能是CNN里最简单的一步了。我们可以不按字面理解，把它理解成下采样（subsampling）。池化分为最大值池化和平均值池化，和卷积差不多，也是取一小块区域，比如一个5*5的方块，如果是最大值池化，那就选这25个像素点最大的那个输出，如果是平均值池化，就把25个像素点取平均输出。</p><p>这样做有什么好处呢？1、应该很明显可以看出，图像经过了下采样尺寸缩小了，按上面的例子，原来5*5的一个区域，现在只要一个值就表示出来了！2、增强了旋转不变性，池化操作可以看做是一种强制性的模糊策略，举个不恰当的例子，假设一个猫的图片，猫的耳朵应该处于左上5*5的一个小区域内（实际上猫的耳朵不可能这么小），无论这个耳朵如何旋转，经过池化之后结果都是近似的，因为就是这一块取平均和取最大对旋转并不care。</p><p>当然和之前的卷积一样，池化也是层层递进的，底层的池化是在模糊底层特征，如线条等，高层的池化模糊了高级语义特征，如猫耳朵。所以，一般的CNN架构都是三明治一样，卷积池化交替出现，保证提取特征的同时也强制模糊增加特征的旋转不变性。</p><hr><p>更新一下比较advantage的东西。</p><p>现代CNN相比于之前的远古CNN发生了很大变化，虽然这里的远古CNN大约在2014年论文中出现，距今也只有不到4年时间。这里也可以看出深度学习的发展日新月异，一日千里的可怕速度。</p><p>理解了本身CNN的基础含义，再来看看这些先进的CNN，是很有必要的，不要指望只靠卷积层池化层就可以得到好的效果，后来加入CNN的trick不计其数，而且也都是里程碑式的成果。下面主要以图像分类的CNN来阐述。</p><p>暴力加深流派：以AlexNet和VGGNet为首的模型，这一派观点很直接，就是不断交替使用卷积层池化层，暴力增加网络层数，最后接一下全连接层分类。这类模型在CNN早期是主流，特点是参数量大，尤其是后面的全连接层，几乎占了一般参数量。而且相比于后续的模型，效果也较差。因此这类模型后续慢慢销声匿迹了。</p><p>Inception流派：谷歌流派，这一派最早起源于NIN，称之为网络中的网络，后被谷歌发展成Inception模型（这个单词真的不好翻译。。。）。这个模型的特点是增加模型的宽度，使得模型不仅仅越长越高，还越长越胖。也就是说每一层不再用单一的卷积核卷积，而是用多个尺度的卷积核试试。显然，如果你熟悉CNN，就不难发现，这样做会使每一层的feature map数量猛增，因为一种尺寸的卷积核就能卷出一系列的feature map，何况多个！这里google使用了1*1的卷积核专门用来降channel。谷歌的特点是一个模型不玩到烂绝不算完，所以又发展出了Inception v2、Inception v3、Inception v4等等。</p><p>残差流派：2015年ResNet横空出世，开创了残差网络。使用残差直连边跨层连接，居然得到了意想不到的好效果。最重要的是，这一改进几乎彻底突破了层数的瓶颈，1000层的resnet不是梦！之后，最新的DenseNet丧心病狂地在各个层中间都引入了残差连接。目前大部分模型都在尝试引入残差连接。</p><p>注意，到此为止，大部分模型已经丢弃了全连接层，改为全局平均池化。大大降低了参数量。</p><p>混合流派：这一派不说了，就是看到哪几类模型效果好，就把这类技术混杂起来。典型的就是Xception和ResIception，将Inception和残差网络结合起来了。</p><p>BatchNromalization：不得不提这个批量标准化技术，在此技术出现之前，CNN收敛很慢，此技术出现后，大大加快了模型收敛速度，还兼具一定的防过拟合效果。当然，这个技术不仅仅限于CNN。</p></span><!-- react-empty: 847 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/120211609"><span data-tooltip="发布于 2016-09-02"><!-- react-text: 852 -->编辑于 <!-- /react-text --><!-- react-text: 853 -->2017-11-13<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 3812 -->271<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3819 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3822 -->35 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-66019-11358-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-66019-11358-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3827 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3830 -->分享<!-- /react-text --></button></div><!-- react-empty: 3831 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3834 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3837 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3840 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3843 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 897 --><!-- react-empty: 2584 --><!-- react-empty: 899 --><!-- react-empty: 900 --><!-- react-empty: 2585 --><!-- react-empty: 902 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="5" data-zop="{&quot;authorName&quot;:&quot;司徒功源&quot;,&quot;itemId&quot;:81864382,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="81864382" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;81864382&quot;,&quot;upvote_num&quot;:104,&quot;comment_num&quot;:15,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;fa110e669b2083b7dec0fd49856686ff&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="司徒功源"><meta itemprop="image" content="https://pic7.zhimg.com/v2-6f970f2d8c122fe8a70c926e72e7da94_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/si-tu-gong-yuan"><meta itemprop="zhihu:followerCount" content="480"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85722-17058-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85722-17058-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/si-tu-gong-yuan"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-6f970f2d8c122fe8a70c926e72e7da94_xs.jpg" srcset="https://pic7.zhimg.com/v2-6f970f2d8c122fe8a70c926e72e7da94_l.jpg 2x" alt="司徒功源"></a></div><!-- react-empty: 916 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85722-40525-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85722-40525-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/si-tu-gong-yuan">司徒功源</a></div><!-- react-empty: 923 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">开挖掘机的</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 930 -->104 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 931 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="104"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/81864382"><meta itemprop="dateCreated" content="2016-01-16T10:38:21.000Z"><meta itemprop="dateModified" content="2016-01-16T10:38:21.000Z"><meta itemprop="commentCount" content="15"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text">cnn的核心在于卷积核，其实关于卷积核还有另一个名字叫做滤波器，从信号处理的角度而言，滤波器是对信号做频率筛选，这里主要是空间-频率的转换，cnn的训练就是找到最好的滤波器使得滤波后的信号更容易分类，还可以从模版匹配的角度看卷积，每个卷积核都可以看成一个特征模版，训练就是为了找到最适合分类的特征模版，一点浅见。</span><!-- react-empty: 2256 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/81864382">发布于 2016-01-16</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2263 -->104<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2270 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2273 -->15 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85900-73112-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85900-73112-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2278 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2281 -->分享<!-- /react-text --></button></div><!-- react-empty: 2282 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2285 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2288 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2291 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2294 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 988 --><!-- react-empty: 2586 --><!-- react-empty: 990 --><!-- react-empty: 991 --><!-- react-empty: 2587 --><!-- react-empty: 993 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="6" data-zop="{&quot;authorName&quot;:&quot;YJango&quot;,&quot;itemId&quot;:194996805,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="194996805" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;194996805&quot;,&quot;upvote_num&quot;:872,&quot;comment_num&quot;:47,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;4eedf55cf1de9f94173b352f9c5a8d2b&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="YJango"><meta itemprop="image" content="https://pic1.zhimg.com/v2-ff764e30ee9e36f4b357eea4b925bb22_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/YJango"><meta itemprop="zhihu:followerCount" content="17576"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85723-21064-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85723-21064-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/YJango"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ff764e30ee9e36f4b357eea4b925bb22_xs.jpg" srcset="https://pic1.zhimg.com/v2-ff764e30ee9e36f4b357eea4b925bb22_l.jpg 2x" alt="YJango"></a></div><!-- react-empty: 1007 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85723-4908-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85723-4908-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/YJango">YJango</a></div><!-- react-empty: 1014 --></div><a class="UserLink-badge" href="https://www.zhihu.com/question/48510028" target="_blank" data-tooltip="已认证的个人"><svg viewBox="0 0 20 20" class="Icon Icon--badgeCert" width="16" height="16" aria-hidden="true" style="height: 16px; width: 16px;"><title>用户标识</title><g><g fill="none" fill-rule="evenodd">     <path d="M.64 11.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.12 3.21.656 4.083 1.735l-.125-.154c.876 1.085 2.304 1.093 3.195.028l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.28-2.238l.016.197c-.12-1.388.656-3.212 1.735-4.084l-.154.125c1.084-.876 1.093-2.304.028-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.28l.198-.016c-1.387.122-3.21-.655-4.083-1.734l.125.153C10.802-.265 9.374-.274 8.483.79L8.61.64c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.28 2.24L2.4 4.363c.12 1.387-.656 3.21-1.735 4.084l.154-.126C-.265 9.2-.274 10.626.79 11.517L.64 11.39z" fill="#0F88EB"></path>     <path d="M7.78 13.728l-2.633-3s-.458-.704.242-1.36c.7-.658 1.327-.22 1.327-.22L8.67 11.28l4.696-4.93s.663-.35 1.3.197c.635.545.27 1.382.27 1.382s-3.467 3.857-5.377 5.78c-.98.93-1.78.018-1.78.018z" fill="#FFF"></path>1   </g></g></svg></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">日本会津大学 人机界面实验室博士在读</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1025 -->872 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1026 --></span></div></div><meta itemprop="image" content="https://pic1.zhimg.com/v2-55069445ed54ce163b76c611ba26b639_200x112.jpg"><meta itemprop="upvoteCount" content="872"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/194996805"><meta itemprop="dateCreated" content="2017-07-07T10:15:19.000Z"><meta itemprop="dateModified" content="2017-10-19T11:27:21.000Z"><meta itemprop="commentCount" content="47"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>该文是<b><a href="https://zhuanlan.zhihu.com/p/27642620" class="internal">卷积神经网络--介绍</a></b>，并假设你理解前馈神经网络。</p><p>如果不是，强烈建议你读完<b><a href="https://www.zhihu.com/question/22553761/answer/126474394" class="internal">如何简单形象又有趣地讲解神经网络是什么？</a></b> 后再来读该篇。</p><h2>目录</h2><ul><li>视觉感知</li><ul><li>画面识别是什么</li><li>识别结果取决于什么</li></ul><li>图像表达</li><ul><li>画面识别的输入</li><li>画面不变形</li></ul><li>前馈神经网络做画面识别的不足</li><li>卷积神经网络做画面识别</li><ul><li>局部连接</li><li>空间共享</li><li>输出空间表达</li><li>Depth维的处理</li><li>Zero padding</li><li>形状、概念抓取</li><li>多filters</li><li>非线性</li><li>输出尺寸控制</li><li>矩阵乘法执行卷积</li><li>Max pooling</li><li>全连接层</li><li>结构发展</li></ul><li>画面不变性的满足</li><ul><li>平移不变性</li><li>旋转和视角不变性</li><li>尺寸不变性</li><li>Inception的理解</li><li>1x1卷积核理解</li><li>跳层连接ResNet</li></ul></ul><h2>视觉感知</h2><p><i>一、画面识别是什么任务？</i></p><p>学习知识的第一步就是<b>明确任务</b>，清楚该知识的输入输出。卷积神经网络最初是服务于画面识别的，所以我们先来看看画面识别的实质是什么。</p><p>先观看几组动物与人类视觉的差异对比图。</p><p>1. 苍蝇的视觉和人的视觉的差异</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-55069445ed54ce163b76c611ba26b639_hd.jpg" data-caption="" data-rawwidth="384" data-rawheight="216" class="content_image" width="384"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-55069445ed54ce163b76c611ba26b639_hd.jpg" data-caption="" data-rawwidth="384" data-rawheight="216" class="content_image lazy" width="384" data-actualsrc="https://pic1.zhimg.com/50/v2-55069445ed54ce163b76c611ba26b639_hd.jpg"></figure><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-4a0ea7ba42166b62bc4f42e8b150815d_hd.jpg" data-caption="" data-rawwidth="1280" data-rawheight="479" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic2.zhimg.com/v2-4a0ea7ba42166b62bc4f42e8b150815d_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-4a0ea7ba42166b62bc4f42e8b150815d_hd.jpg" data-caption="" data-rawwidth="1280" data-rawheight="479" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic2.zhimg.com/v2-4a0ea7ba42166b62bc4f42e8b150815d_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-4a0ea7ba42166b62bc4f42e8b150815d_hd.jpg"></figure><p><br></p><p>2. 蛇的视觉和人的视觉的差异</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-a4d35c245931f264ed9a0716fdf20685_hd.jpg" data-caption="" data-rawwidth="384" data-rawheight="216" class="content_image" width="384"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-a4d35c245931f264ed9a0716fdf20685_hd.jpg" data-caption="" data-rawwidth="384" data-rawheight="216" class="content_image lazy" width="384" data-actualsrc="https://pic1.zhimg.com/50/v2-a4d35c245931f264ed9a0716fdf20685_hd.jpg"></figure><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-3da84b5b80ba7a0d779284566f80be93_hd.jpg" data-caption="" data-rawwidth="1280" data-rawheight="479" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic2.zhimg.com/v2-3da84b5b80ba7a0d779284566f80be93_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-3da84b5b80ba7a0d779284566f80be93_hd.jpg" data-caption="" data-rawwidth="1280" data-rawheight="479" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic2.zhimg.com/v2-3da84b5b80ba7a0d779284566f80be93_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-3da84b5b80ba7a0d779284566f80be93_hd.jpg"></figure><p><br></p><p>（更多对比图请参考<a href="https://link.zhihu.com/?target=http%3A//chuansong.me/n/2656056" class=" wrap external" target="_blank" rel="nofollow noreferrer">链接</a>）</p><p>通过上面的两组对比图可以知道，即便是相同的图片经过不同的视觉系统，也会得到不同的感知。</p><p>这里引出一条知识：生物所看到的景象并非世界的原貌，而是长期进化出来的<b>适合自己生存环境的一种感知方式</b>。 蛇的猎物一般是夜间行动，所以它就进化出了一种可以在夜间也能很好观察的感知系统，感热。</p><p>任何视觉系统都是将图像反光与脑中所看到的概念进行关联。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-2c82abd20c4e7c40f7f13f035b924b0b_hd.jpg" data-caption="" data-rawwidth="223" data-rawheight="97" class="content_image" width="223"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-2c82abd20c4e7c40f7f13f035b924b0b_hd.jpg" data-caption="" data-rawwidth="223" data-rawheight="97" class="content_image lazy" width="223" data-actualsrc="https://pic4.zhimg.com/50/v2-2c82abd20c4e7c40f7f13f035b924b0b_hd.jpg"></figure><p><br></p><p>所以画面识别实际上并非识别这个东西客观上是什么，而是寻找人类的视觉关联方式，并再次应用。 如果我们不是人类，而是蛇类，那么画面识别所寻找的 就和现在的不一样。</p><blockquote><b>画面识别实际上是寻找（学习）人类的视觉关联方式 ，并再次应用</b>。</blockquote><p><i>二、图片被识别成什么取决于哪些因素？</i></p><p>下面用两张图片来体会识别结果取决于哪些因素。</p><p>1. 老妇与少女</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-c902a9e33b0322051a5f9165e9439247_hd.jpg" data-caption="" data-rawwidth="198" data-rawheight="255" class="content_image" width="198"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c902a9e33b0322051a5f9165e9439247_hd.jpg" data-caption="" data-rawwidth="198" data-rawheight="255" class="content_image lazy" width="198" data-actualsrc="https://pic2.zhimg.com/50/v2-c902a9e33b0322051a5f9165e9439247_hd.jpg"></figure><p><br></p><p>请观察上面这张图片，你看到的是老妇还是少女？ 以不同的方式去观察这张图片会得出不同的答案。 图片可以观察成有大鼻子、大眼睛的老妇。也可以被观察成少女，但这时老妇的嘴会被识别成少女脖子上的项链，而老妇的眼睛则被识别为少女的耳朵。</p><p>2. 海豚与男女</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-7e5bc60a9e9bd0b597e4b650fecc439e_hd.jpg" data-caption="" data-rawwidth="394" data-rawheight="509" class="content_image" width="394"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-7e5bc60a9e9bd0b597e4b650fecc439e_hd.jpg" data-caption="" data-rawwidth="394" data-rawheight="509" class="content_image lazy" width="394" data-actualsrc="https://pic2.zhimg.com/50/v2-7e5bc60a9e9bd0b597e4b650fecc439e_hd.jpg"></figure><p><br></p><p>上面这张图片如果是成人观察，多半看到的会是一对亲热的男女。倘若儿童看到这张图片，看到的则会是一群海豚（男女的轮廓是由海豚构造出的）。所以，识别结果受年龄，文化等因素的影响，换句话说：</p><blockquote><b>图片被识别成什么不仅仅取决于图片本身，还取决于图片是如何被观察的。</b></blockquote><h2>图像表达</h2><p>我们知道了“画面识别是从大量的<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation" alt="(x,y)" eeimg="1">数据中寻找人类的视觉关联方式 ，并再次应用。 其<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(1)" alt="x" eeimg="1">-是输入，表示所看到的东西<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(2)" alt="y" eeimg="1">-输出，表示该东西是什么。</p><p>在自然界中，<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(1)" alt="x" eeimg="1">是物体的反光，那么在计算机中，图像又是如何被表达和存储的呢？</p><p><br></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-d2859e5c486ed704492ab80079e99535_hd.gif" data-caption="" data-rawwidth="291" data-rawheight="290" data-thumbnail="https://pic3.zhimg.com/50/v2-d2859e5c486ed704492ab80079e99535_hd.jpg" class="content_image" width="291"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-d2859e5c486ed704492ab80079e99535_hd.jpg" data-thumbnail="https://pic3.zhimg.com/50/v2-d2859e5c486ed704492ab80079e99535_hd.jpg"></div></div></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=https%3A//medium.com/%40ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><p>图像在计算机中是一堆按顺序排列的数字，数值为0到255。0表示最暗，255表示最亮。 你可以把这堆数字用一个长长的向量来表示，也就是<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/get_started/mnist/beginners" class=" wrap external" target="_blank" rel="nofollow noreferrer">tensorflow的mnist教程</a>中784维向量的表示方式。 然而这样会失去平面结构的信息，为保留该结构信息，通常选择矩阵的表示方式：28x28的矩阵。</p><p>上图是只有黑白颜色的灰度图，而更普遍的图片表达方式是RGB颜色模型，即红（Red）、绿（Green）、蓝（Blue）三原色的色光以不同的比例相加，以产生多种多样的色光。</p><p>这样，RGB颜色模型中，单个矩阵就扩展成了有序排列的三个矩阵，也可以用三维张量去理解，其中的每一个矩阵又叫这个图片的一个channel。</p><p>在电脑中，一张图片是数字构成的“长方体”。可用 宽width, 高height, 深depth 来描述，如图。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg" data-caption="" data-rawwidth="305" data-rawheight="252" class="content_image" width="305"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg" data-caption="" data-rawwidth="305" data-rawheight="252" class="content_image lazy" width="305" data-actualsrc="https://pic3.zhimg.com/50/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg"></figure><p><br></p><blockquote>画面识别的输入<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(1)" alt="x" eeimg="1">是shape为(width, height, depth)的三维张量。</blockquote><p>接下来要考虑的就是该如何处理这样的“数字长方体”。</p><h2>画面不变性</h2><p>在决定如何处理“数字长方体”之前，需要清楚所建立的网络拥有什么样的特点。 我们知道一个物体不管在画面左侧还是右侧，都会被识别为同一物体，这一特点就是不变性（invariance），如下图所示。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-b9aed3dd68b9818561faa7d8ed24ea5a_hd.jpg" data-caption="" data-rawwidth="567" data-rawheight="769" class="origin_image zh-lightbox-thumb" width="567" data-original="https://pic4.zhimg.com/v2-b9aed3dd68b9818561faa7d8ed24ea5a_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-b9aed3dd68b9818561faa7d8ed24ea5a_hd.jpg" data-caption="" data-rawwidth="567" data-rawheight="769" class="origin_image zh-lightbox-thumb lazy" width="567" data-original="https://pic4.zhimg.com/v2-b9aed3dd68b9818561faa7d8ed24ea5a_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-b9aed3dd68b9818561faa7d8ed24ea5a_hd.jpg"></figure><p><br></p><p>我们希望所建立的网络可以尽可能的满足这些不变性特点。</p><p>为了理解卷积神经网络对这些不变性特点的贡献，我们将用不具备这些不变性特点的前馈神经网络来进行比较。</p><h2>图片识别--前馈神经网络</h2><p>方便起见，我们用depth只有1的灰度图来举例。 想要完成的任务是：在宽长为4x4的图片中识别是否有下图所示的“横折”。 图中，黄色圆点表示值为0的像素，深色圆点表示值为1的像素。 我们知道不管这个横折在图片中的什么位置，都会被认为是相同的横折。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-18c11c6f485e9f1bbc9a50eb3d248439_hd.jpg" data-caption="" data-rawwidth="411" data-rawheight="374" class="content_image" width="411"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-18c11c6f485e9f1bbc9a50eb3d248439_hd.jpg" data-caption="" data-rawwidth="411" data-rawheight="374" class="content_image lazy" width="411" data-actualsrc="https://pic1.zhimg.com/50/v2-18c11c6f485e9f1bbc9a50eb3d248439_hd.jpg"></figure><p><br></p><p>若训练前馈神经网络来完成该任务，那么表达图像的三维张量将会被摊平成一个向量，作为网络的输入，即(width, height, depth)为(4, 4, 1)的图片会被展成维度为16的向量作为网络的输入层。再经过几层不同节点个数的隐藏层，最终输出两个节点，分别表示“有横折的概率”和“没有横折的概率”，如下图所示。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-2b411af47b1cad7b727bb676c847ce59_hd.jpg" data-caption="" data-rawwidth="499" data-rawheight="505" class="origin_image zh-lightbox-thumb" width="499" data-original="https://pic2.zhimg.com/v2-2b411af47b1cad7b727bb676c847ce59_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-2b411af47b1cad7b727bb676c847ce59_hd.jpg" data-caption="" data-rawwidth="499" data-rawheight="505" class="origin_image zh-lightbox-thumb lazy" width="499" data-original="https://pic2.zhimg.com/v2-2b411af47b1cad7b727bb676c847ce59_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-2b411af47b1cad7b727bb676c847ce59_hd.jpg"></figure><p><br></p><p>下面我们用数字（16进制）对图片中的每一个像素点（pixel）进行编号。 当使用右侧那种物体位于中间的训练数据来训练网络时，网络就只会对编号为5,6,9,a的节点的权重进行调节。 若让该网络识别位于右下角的“横折”时，则无法识别。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-ce9919e4930c1f29241afec0538b2605_hd.jpg" data-caption="" data-rawwidth="816" data-rawheight="505" class="origin_image zh-lightbox-thumb" width="816" data-original="https://pic1.zhimg.com/v2-ce9919e4930c1f29241afec0538b2605_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-ce9919e4930c1f29241afec0538b2605_hd.jpg" data-caption="" data-rawwidth="816" data-rawheight="505" class="origin_image zh-lightbox-thumb lazy" width="816" data-original="https://pic1.zhimg.com/v2-ce9919e4930c1f29241afec0538b2605_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-ce9919e4930c1f29241afec0538b2605_hd.jpg"></figure><p><br></p><p>解决办法是用大量物体位于不同位置的数据训练，同时增加网络的隐藏层个数从而扩大网络学习这些变体的能力。</p><p>然而这样做十分不效率，因为我们知道在左侧的“横折”也好，还是在右侧的“横折”也罢，大家都是“横折”。 为什么相同的东西在位置变了之后要重新学习？有没有什么方法可以将中间所学到的规律也运用在其他的位置？ 换句话说，也就是<b>让不同位置用相同的权重</b>。</p><h2>图片识别--卷积神经网络</h2><p>卷积神经网络就是让权重在不同位置共享的神经网络。</p><h2>局部连接</h2><p>在卷积神经网络中，我们先选择一个局部区域，用这个局部区域去扫描整张图片。 局部区域所圈起来的所有节点会被连接到下一层的一个节点上。</p><p>为了更好的和前馈神经网络做比较，我将这些以矩阵排列的节点展成了向量。 下图展示了被红色方框所圈中编号为0,1,4,5的节点是如何通过<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(3)" alt="w_1,w_2,w_3,w_4" eeimg="1">连接到下一层的节点0上的。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-e877b9099b1139c1a34b0bf66bf92aa4_hd.jpg" data-caption="" data-rawwidth="513" data-rawheight="514" class="origin_image zh-lightbox-thumb" width="513" data-original="https://pic3.zhimg.com/v2-e877b9099b1139c1a34b0bf66bf92aa4_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-e877b9099b1139c1a34b0bf66bf92aa4_hd.jpg" data-caption="" data-rawwidth="513" data-rawheight="514" class="origin_image zh-lightbox-thumb lazy" width="513" data-original="https://pic3.zhimg.com/v2-e877b9099b1139c1a34b0bf66bf92aa4_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-e877b9099b1139c1a34b0bf66bf92aa4_hd.jpg"></figure><p><br></p><p>这个带有连接强弱的红色方框就叫做 <b>filter</b> 或 <b>kernel</b> 或 <b>feature detector</b>。 而filter的范围叫做<b>filter size</b>，这里所展示的是2x2的filter size。</p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(4)" alt="\left[ \begin{matrix} w_1&amp;w_2\\ w_3&amp;w_4\\ \end{matrix} \right]" eeimg="1"> (1)</p><p>第二层的节点0的数值就是局部区域的线性组合，即被圈中节点的数值乘以对应的权重后相加。 用<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(1)" alt="x" eeimg="1">表示输入值，<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(2)" alt="y" eeimg="1">表示输出值，用图中标注数字表示角标，则下面列出了两种计算编号为0的输出值<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(5)" alt="y_0" eeimg="1">的表达式。</p><p>注：在局部区域的线性组合后，也会和前馈神经网络一样，加上一个偏移量<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(6)" alt="b_0 " eeimg="1">。</p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(7)" alt="\begin{split} y_0 &amp;= x_0*w_1 + x_1*w_2+ x_4*w_3+ x_5*w_4+b_0\\y_0 &amp;= \left[ \begin{matrix} w_1&amp;w_2&amp; w_3&amp;w_4 \end{matrix} \right] \cdot \left[ \begin{matrix} x_0\\ x_1\\ x_4\\ x_5\\ \end{matrix} \right]+b_0 \end{split}" eeimg="1"> (2)<br></p><h2>空间共享</h2><p>当filter扫到其他位置计算输出节点<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(8)" alt="y_i " eeimg="1">时，<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(3)" alt="w_1,w_2,w_3,w_4" eeimg="1">，<b>包括</b><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(9)" alt="b_0" eeimg="1">是共用的。</p><p>下面这张动态图展示了当filter扫过不同区域时，节点的链接方式。 动态图的最后一帧则显示了所有连接。 可以注意到，每个输出节点并非像前馈神经网络中那样与全部的输入节点连接，而是部分连接。 这也就是为什么大家也叫前馈神经网络（feedforward neural network）为fully-connected neural network。 图中显示的是一步一步的移动filter来扫描全图，一次移动多少叫做stride。</p><p><br></p><figure><noscript>&lt;img src="https://pic7.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif" data-caption="" data-rawwidth="513" data-rawheight="514" data-thumbnail="https://pic7.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.jpg" class="origin_image zh-lightbox-thumb" width="513" data-original="https://pic7.zhimg.com/v2-4fd0400ccebc8adb2dffe24aac163e70_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.jpg" data-thumbnail="https://pic7.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.jpg"></div></div></figure><p><br></p><blockquote><b>空间共享也就是卷积神经网络所引入的先验知识。</b></blockquote><h2>输出表达</h2><p>如先前在图像表达中提到的，图片不用向量去表示是为了保留图片平面结构的信息。 同样的，卷积后的输出若用上图的排列方式则丢失了平面结构信息。 所以我们依然用矩阵的方式排列它们，就得到了下图所展示的连接。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-e1691956fd1beb5d7a637924a1a73d91_hd.jpg" data-caption="" data-rawwidth="513" data-rawheight="514" class="origin_image zh-lightbox-thumb" width="513" data-original="https://pic3.zhimg.com/v2-e1691956fd1beb5d7a637924a1a73d91_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-e1691956fd1beb5d7a637924a1a73d91_hd.jpg" data-caption="" data-rawwidth="513" data-rawheight="514" class="origin_image zh-lightbox-thumb lazy" width="513" data-original="https://pic3.zhimg.com/v2-e1691956fd1beb5d7a637924a1a73d91_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-e1691956fd1beb5d7a637924a1a73d91_hd.jpg"></figure><p><br></p><p>这也就是你们在网上所看到的下面这张图。在看这张图的时候请结合上图的连接一起理解，即输入（绿色）的每九个节点连接到输出（粉红色）的一个节点上的。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif" data-caption="" data-rawwidth="526" data-rawheight="384" data-thumbnail="https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.jpg" class="origin_image zh-lightbox-thumb" width="526" data-original="https://pic4.zhimg.com/v2-7fce29335f9b43bce1b373daa40cccba_r.gif"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-7fce29335f9b43bce1b373daa40cccba_hd.jpg" data-thumbnail="https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.jpg"></div></div></figure><p><br></p><p>经过一个feature detector计算后得到的粉红色区域也叫做一个“<b>Convolved Feature</b>” 或 “<b>Activation Map</b>” 或 “<b>Feature Map</b>”。</p><h2>Depth维的处理</h2><p>现在我们已经知道了depth维度只有1的灰度图是如何处理的。 但前文提过，图片的普遍表达方式是下图这样有3个channels的RGB颜色模型。 当depth为复数的时候，每个feature detector是如何卷积的？</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg" data-caption="" data-rawwidth="305" data-rawheight="252" class="content_image" width="305"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg" data-caption="" data-rawwidth="305" data-rawheight="252" class="content_image lazy" width="305" data-actualsrc="https://pic3.zhimg.com/50/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg"></figure><p><br></p><p><b>现象</b>：2x2所表达的filter size中，一个2表示width维上的局部连接数，另一个2表示height维上的局部连接数，并却没有depth维上的局部连接数，是因为depth维上并非局部，而是全部连接的。</p><p>在2D卷积中，filter在张量的width维, height维上是局部连接，在depth维上是贯串全部channels的。</p><p><b>类比</b>：想象在切蛋糕的时候，不管这个蛋糕有多少层，通常大家都会一刀切到底，但是在长和宽这两个维上是局部切割。</p><p>下面这张图展示了，在depth为复数时，filter是如何连接输入节点到输出节点的。 图中红、绿、蓝颜色的节点表示3个channels。 黄色节点表示一个feature detector卷积后得到的Feature Map。 其中被透明黑框圈中的12个节点会被连接到黄黑色的节点上。</p><ul><li>在输入depth为1时：被filter size为2x2所圈中的4个输入节点连接到1个输出节点上。</li><li>在输入depth为3时：被filter size为2x2，但是贯串3个channels后，所圈中的12个输入节点连接到1个输出节点上。</li><li>在输入depth为<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(10)" alt="n" eeimg="1">时：2x2x<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(10)" alt="n" eeimg="1">个输入节点连接到1个输出节点上。</li></ul><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-23db15ec3f783bbb5cf811711e46dbba_hd.jpg" data-caption="" data-rawwidth="302" data-rawheight="382" class="content_image" width="302"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-23db15ec3f783bbb5cf811711e46dbba_hd.jpg" data-caption="" data-rawwidth="302" data-rawheight="382" class="content_image lazy" width="302" data-actualsrc="https://pic3.zhimg.com/50/v2-23db15ec3f783bbb5cf811711e46dbba_hd.jpg"></figure><p><br></p><p>(可从<a href="https://link.zhihu.com/?target=https%3A//www.vectary.com/u/yjango/cnn" class=" wrap external" target="_blank" rel="nofollow noreferrer">vectary</a>在3D编辑下查看)</p><p><b>注意</b>：三个channels的权重并不共享。 即当深度变为3后，权重也跟着扩增到了三组，如式子(3)所示，不同channels用的是自己的权重。 式子中增加的角标r,g,b分别表示red channel, green channel, blue channel的权重。</p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(11)" alt="\left[ \begin{matrix} w_{r1}&amp;w_{r2}\\ w_{r3}&amp;w_{r4}\\ \end{matrix} \right], \left[ \begin{matrix} w_{g1}&amp;w_{g2}\\ w_{g3}&amp;w_{g4}\\ \end{matrix} \right], \left[ \begin{matrix} w_{b1}&amp;w_{b2}\\ w_{b3}&amp;w_{b4}\\ \end{matrix} \right]" eeimg="1"> (3)</p><p>计算例子：用<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(12)" alt="x_{r0}" eeimg="1">表示red channel的编号为0的输入节点，<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(13)" alt="x_{g5}" eeimg="1">表示green channel编号为5个输入节点。<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(14)" alt="x_{b1}" eeimg="1">表示blue channel。如式子(4)所表达，这时的一个输出节点实际上是12个输入节点的线性组合。</p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(15)" alt="\begin{split} y_0 &amp;= x_{r0}*w_{r1} + x_{r1}*w_{r2}+ x_{r4}*w_{r3}+ x_{r5}*w_{r4}+ x_{g0}*w_{g1} + x_{g1}*w_{g2}+ x_{g4}*w_{g3}+ x_{g5}*w_{g4}+ x_{b0}*w_{b1} + x_{b1}*w_{b2}+ x_{b4}*w_{b3}+ x_{b5}*w_{b4}+b_0\\y_0 &amp;= \left[ \begin{matrix} w_{r1}&amp;w_{r2}&amp; w_{r3}&amp;w_{r4} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{r0}\\ x_{r1}\\ x_{r4}\\ x_{r5}\\ \end{matrix} \right] +\left[ \begin{matrix} w_{g1}&amp;w_{g2}&amp; w_{g3}&amp;w_{g4} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{g0}\\ x_{g1}\\ x_{g4}\\ x_{g5}\\ \end{matrix} \right]+\left[ \begin{matrix} w_{b1}&amp;w_{b2}&amp; w_{b3}&amp;w_{b4} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{b0}\\ x_{b1}\\ x_{b4}\\ x_{b5}\\ \end{matrix} \right]+b_0\end{split}" eeimg="1">(4)</p><p>当filter扫到其他位置计算输出节点<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(16)" alt="y_i" eeimg="1">时，那12个权重在不同位置是共用的，如下面的动态图所展示。 透明黑框圈中的12个节点会连接到被白色边框选中的黄色节点上。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-0bc83b72ef50099b70a10cc3ab528f62_hd.gif" data-caption="" data-rawwidth="326" data-rawheight="455" data-thumbnail="https://pic2.zhimg.com/50/v2-0bc83b72ef50099b70a10cc3ab528f62_hd.jpg" class="content_image" width="326"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-0bc83b72ef50099b70a10cc3ab528f62_hd.jpg" data-thumbnail="https://pic2.zhimg.com/50/v2-0bc83b72ef50099b70a10cc3ab528f62_hd.jpg"></div></div></figure><p><br></p><blockquote><b>每个filter会在width维, height维上，以局部连接和空间共享，并贯串整个depth维的方式得到一个Feature Map。</b></blockquote><h2>Zero padding</h2><p>细心的读者应该早就注意到了，4x4的图片被2x2的filter卷积后变成了3x3的图片，每次卷积后都会小一圈的话，经过若干层后岂不是变的越来越小？ Zero padding就可以在这时帮助控制Feature Map的输出尺寸，同时避免了边缘信息被一步步舍弃的问题。</p><p>例如：下面4x4的图片在边缘Zero padding一圈后，再用3x3的filter卷积后，得到的Feature Map尺寸依然是4x4不变。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-c1010eb5dcf032ea95eab495a45f9b31_hd.jpg" data-caption="" data-rawwidth="173" data-rawheight="173" class="content_image" width="173"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c1010eb5dcf032ea95eab495a45f9b31_hd.jpg" data-caption="" data-rawwidth="173" data-rawheight="173" class="content_image lazy" width="173" data-actualsrc="https://pic1.zhimg.com/50/v2-c1010eb5dcf032ea95eab495a45f9b31_hd.jpg"></figure><p><br></p><p>通常大家都想要在卷积时保持图片的原始尺寸。 选择3x3的filter和1的zero padding，或5x5的filter和2的zero padding可以保持图片的原始尺寸。 这也是为什么大家多选择3x3和5x5的filter的原因。 另一个原因是3x3的filter考虑到了像素与其距离为1以内的所有其他像素的关系，而5x5则是考虑像素与其距离为2以内的所有其他像素的关系。</p><p><b>尺寸</b>：Feature Map的尺寸等于(input_size + 2 * padding_size − filter_size)/stride+1。</p><p><b>注意</b>：上面的式子是计算width或height一维的。padding_size也表示的是单边补零的个数。例如(4+2-3)/1+1 = 4，保持原尺寸。</p><p>不用去背这个式子。其中(input_size + 2 * padding_size)是经过Zero padding扩充后真正要卷积的尺寸。 减去 filter_size后表示可以滑动的范围。 再除以可以一次滑动（stride）多少后得到滑动了多少次，也就意味着得到了多少个输出节点。 再加上第一个不需要滑动也存在的输出节点后就是最后的尺寸。</p><h2>形状、概念抓取</h2><p>知道了每个filter在做什么之后，我们再来思考这样的一个filter会抓取到什么样的信息。</p><p>我们知道不同的形状都可由细小的“零件”组合而成的。比如下图中，用2x2的范围所形成的16种形状可以组合成格式各样的“更大”形状。</p><p>卷积的每个filter可以探测特定的形状。又由于Feature Map保持了抓取后的空间结构。若将探测到细小图形的Feature Map作为新的输入再次卷积后，则可以由此探测到“更大”的形状概念。 比如下图的第一个“大”形状可由2,3,4,5基础形状拼成。第二个可由2,4,5,6组成。第三个可由6,1组成。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-f53f6ac43abd2555cfbbba6ea7fdc0e4_hd.jpg" data-caption="" data-rawwidth="484" data-rawheight="291" class="origin_image zh-lightbox-thumb" width="484" data-original="https://pic1.zhimg.com/v2-f53f6ac43abd2555cfbbba6ea7fdc0e4_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-f53f6ac43abd2555cfbbba6ea7fdc0e4_hd.jpg" data-caption="" data-rawwidth="484" data-rawheight="291" class="origin_image zh-lightbox-thumb lazy" width="484" data-original="https://pic1.zhimg.com/v2-f53f6ac43abd2555cfbbba6ea7fdc0e4_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-f53f6ac43abd2555cfbbba6ea7fdc0e4_hd.jpg"></figure><p><br></p><p>除了基础形状之外，颜色、对比度等概念对画面的识别结果也有影响。卷积层也会根据需要去探测特定的概念。</p><p>可以从下面这张图中感受到不同数值的filters所卷积过后的Feature Map可以探测边缘，棱角，模糊，突出等概念。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-644d108587a6ce7fa471ede5d2e11e98_hd.jpg" data-caption="" data-rawwidth="342" data-rawheight="562" class="content_image" width="342"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-644d108587a6ce7fa471ede5d2e11e98_hd.jpg" data-caption="" data-rawwidth="342" data-rawheight="562" class="content_image lazy" width="342" data-actualsrc="https://pic3.zhimg.com/50/v2-644d108587a6ce7fa471ede5d2e11e98_hd.jpg"></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><p>如我们先前所提，图片被识别成什么不仅仅取决于图片本身，还取决于图片是如何被观察的。</p><p>而filter内的权重矩阵W是网络根据数据学习得到的，也就是说，我们让神经网络自己学习以什么样的方式去观察图片。</p><p>拿老妇与少女的那幅图片举例，当标签是少女时，卷积网络就会学习抓取可以成少女的形状、概念。 当标签是老妇时，卷积网络就会学习抓取可以成老妇的形状、概念。</p><p>下图展现了在人脸识别中经过层层的卷积后，所能够探测的形状、概念也变得越来越抽象和复杂。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-c78b8d059715bb5f42c93716a98d5a69_hd.jpg" data-caption="" data-rawwidth="366" data-rawheight="546" class="content_image" width="366"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c78b8d059715bb5f42c93716a98d5a69_hd.jpg" data-caption="" data-rawwidth="366" data-rawheight="546" class="content_image lazy" width="366" data-actualsrc="https://pic4.zhimg.com/50/v2-c78b8d059715bb5f42c93716a98d5a69_hd.jpg"></figure><p><br></p><blockquote><b>卷积神经网络会尽可能寻找最能解释训练数据的抓取方式。</b></blockquote><h2>多filters</h2><p>每个filter可以抓取探测特定的形状的存在。 假如我们要探测下图的长方框形状时，可以用4个filters去探测4个基础“零件”。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-6df64fccc9a8e2f696626f85233acb3c_hd.jpg" data-caption="" data-rawwidth="123" data-rawheight="127" class="content_image" width="123"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-6df64fccc9a8e2f696626f85233acb3c_hd.jpg" data-caption="" data-rawwidth="123" data-rawheight="127" class="content_image lazy" width="123" data-actualsrc="https://pic4.zhimg.com/50/v2-6df64fccc9a8e2f696626f85233acb3c_hd.jpg"></figure><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-65461a21a909eca2e190c54db59a2c8f_hd.jpg" data-caption="" data-rawwidth="315" data-rawheight="82" class="content_image" width="315"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-65461a21a909eca2e190c54db59a2c8f_hd.jpg" data-caption="" data-rawwidth="315" data-rawheight="82" class="content_image lazy" width="315" data-actualsrc="https://pic2.zhimg.com/50/v2-65461a21a909eca2e190c54db59a2c8f_hd.jpg"></figure><p><br></p><p>因此我们自然而然的会选择用多个不同的filters对同一个图片进行多次抓取。 如下图（动态图过大，如果显示不出，请看到该<a href="https://link.zhihu.com/?target=https%3A//ujwlkarn.files.wordpress.com/2016/08/giphy.gif" class=" wrap external" target="_blank" rel="nofollow noreferrer">链接</a>观看），同一个图片，经过两个（红色、绿色）不同的filters扫描过后可得到不同特点的Feature Maps。 每增加一个filter，就意味着你想让网络多抓取一个特征。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-c7f1ea1d42820b4de30bd548c3986ecd_hd.gif" data-caption="" data-rawwidth="212" data-rawheight="120" data-thumbnail="https://pic2.zhimg.com/50/v2-c7f1ea1d42820b4de30bd548c3986ecd_hd.jpg" class="content_image" width="212"&gt;</noscript><div class="RichText-gifPlaceholder"><div data-reactroot="" class="GifPlayer" data-za-detail-view-path-module="GifItem"><img class="column-gif" role="presentation" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-c7f1ea1d42820b4de30bd548c3986ecd_hd.jpg" data-thumbnail="https://pic2.zhimg.com/50/v2-c7f1ea1d42820b4de30bd548c3986ecd_hd.jpg"></div></div></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><p>这样卷积层的输出也不再是depth为1的一个平面，而是和输入一样是depth为复数的长方体。</p><p>如下图所示，当我们增加一个filter（紫色表示）后，就又可以得到一个Feature Map。 将不同filters所卷积得到的Feature Maps按顺序堆叠后，就得到了一个卷积层的最终输出。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-d11e1d2f2c41b6df713573f8155bc324_hd.jpg" data-caption="" data-rawwidth="1271" data-rawheight="629" class="origin_image zh-lightbox-thumb" width="1271" data-original="https://pic3.zhimg.com/v2-d11e1d2f2c41b6df713573f8155bc324_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-d11e1d2f2c41b6df713573f8155bc324_hd.jpg" data-caption="" data-rawwidth="1271" data-rawheight="629" class="origin_image zh-lightbox-thumb lazy" width="1271" data-original="https://pic3.zhimg.com/v2-d11e1d2f2c41b6df713573f8155bc324_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-d11e1d2f2c41b6df713573f8155bc324_hd.jpg"></figure><p><br></p><blockquote><b>卷积层的输入是长方体，输出也是长方体。</b></blockquote><p>这样卷积后输出的长方体可以作为新的输入送入另一个卷积层中处理。<br></p><h2>加入非线性</h2><p>和前馈神经网络一样，经过线性组合和偏移后，会加入非线性增强模型的拟合能力。</p><p>将卷积所得的Feature Map经过ReLU变换（elementwise）后所得到的output就如下图所展示。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-54a469b2873542e75abf2bc5d8fcaa1a_hd.jpg" data-caption="" data-rawwidth="748" data-rawheight="280" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic1.zhimg.com/v2-54a469b2873542e75abf2bc5d8fcaa1a_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-54a469b2873542e75abf2bc5d8fcaa1a_hd.jpg" data-caption="" data-rawwidth="748" data-rawheight="280" class="origin_image zh-lightbox-thumb lazy" width="748" data-original="https://pic1.zhimg.com/v2-54a469b2873542e75abf2bc5d8fcaa1a_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-54a469b2873542e75abf2bc5d8fcaa1a_hd.jpg"></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=http%3A//mlss.tuebingen.mpg.de/2015/slides/fergus/Fergus_1.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><h2>输出长方体</h2><p>现在我们知道了一个卷积层的输出也是一个长方体。 那么这个输出长方体的(width, height, depth)由哪些因素决定和控制。</p><p>这里直接用<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">CS231n</a>的Summary：</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-a9983c3cee935b68c73965bc1abe268c_hd.jpg" data-caption="" data-rawwidth="383" data-rawheight="376" class="content_image" width="383"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-a9983c3cee935b68c73965bc1abe268c_hd.jpg" data-caption="" data-rawwidth="383" data-rawheight="376" class="content_image lazy" width="383" data-actualsrc="https://pic4.zhimg.com/50/v2-a9983c3cee935b68c73965bc1abe268c_hd.jpg"></figure><p><br></p><p>计算例子：请体会<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">CS231n</a>的Convolution Demo部分的演示。</p><h2>矩阵乘法执行卷积</h2><p>如果按常规以扫描的方式一步步计算局部节点和filter的权重的点乘，则不能高效的利用GPU的并行能力。 所以更普遍的方法是用两个大矩阵的乘法来一次性囊括所有计算。</p><p>因为卷积层的每个输出节点都是由若干个输入节点的线性组合所计算。 因为输出的节点个数是<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(17)" alt="W_2 \times H_2\times D_2" eeimg="1">，所以就有<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(17)" alt="W_2 \times H_2\times D_2" eeimg="1">个线性组合。</p><p>读过我写的<a href="https://link.zhihu.com/?target=https%3A//yjango.gitbooks.io/superorganism/content/xian_xing_dai_shu.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">线性代数教程</a>的读者请回忆，矩阵乘矩阵的意义可以理解为批量的线性组合按顺序排列。 其中一个矩阵所表示的信息是多组权重，另一个矩阵所表示的信息是需要进行组合的向量。 大家习惯性的把组成成分放在矩阵乘法的右边，而把权重放在矩阵乘法的左边。 所以这个大型矩阵乘法可以用<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(18)" alt="W_{row}\cdot X_{col}" eeimg="1">表示，其中<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(19)" alt="W_{row}" eeimg="1">和<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(20)" alt="X_{col}" eeimg="1">都是矩阵。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-11a4d56793af815eb2b4585d64aec178_hd.jpg" data-caption="" data-rawwidth="551" data-rawheight="136" class="origin_image zh-lightbox-thumb" width="551" data-original="https://pic3.zhimg.com/v2-11a4d56793af815eb2b4585d64aec178_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-11a4d56793af815eb2b4585d64aec178_hd.jpg" data-caption="" data-rawwidth="551" data-rawheight="136" class="origin_image zh-lightbox-thumb lazy" width="551" data-original="https://pic3.zhimg.com/v2-11a4d56793af815eb2b4585d64aec178_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-11a4d56793af815eb2b4585d64aec178_hd.jpg"></figure><p><br></p><p>卷积的每个输出是由局部的输入节点和对应的filter权重展成向量后所计算的，如式子(2)。 那么<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(19)" alt="W_{row}" eeimg="1">中的每一行则是每个filter的权重，有<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(21)" alt="F\cdot F \cdot D_1" eeimg="1">个； 而<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(20)" alt="X_{col}" eeimg="1">的每一列是所有需要进行组合的节点（上面的动态图中被黑色透明框圈中的节点），也有<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(21)" alt="F\cdot F \cdot D_1" eeimg="1">个。 <img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(20)" alt="X_{col}" eeimg="1">的列的个数则表示每个filter要滑动多少次才可以把整个图片扫描完，有<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(22)" alt="W_2\cdot H_2" eeimg="1">次。 因为我们有多个filters，<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(19)" alt="W_{row}" eeimg="1">的行的个数则是filter的个数<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(23)" alt="K" eeimg="1">。</p><p>最后我们得到：</p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(24)" alt="W_{row} \in R^{K \times F\cdot F \cdot D_1}" eeimg="1"></p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(25)" alt="X_{col} \in R^{F\cdot F \cdot D_1 \times W_2\cdot H_2}" eeimg="1"></p><p><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(26)" alt="W_{row}\cdot X_{col} \in R^{K \times W_2\cdot H_2}" eeimg="1"></p><p>当然矩阵乘法后需要将<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(18)" alt="W_{row}\cdot X_{col}" eeimg="1">整理成形状为<img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(17)" alt="W_2 \times H_2\times D_2" eeimg="1">的三维张量以供后续处理（如再送入另一个卷积层）。 <img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/equation(20)" alt="X_{col}" eeimg="1">则也需要逐步的局部滑动图片，最后堆叠构成用于计算矩阵乘法的形式。</p><h2>Max pooling</h2><p>在卷积后还会有一个pooling的操作，尽管有其他的比如average pooling等，这里只提max pooling。</p><p>max pooling的操作如下图所示：整个图片被不重叠的分割成若干个同样大小的小块（pooling size）。每个小块内只取最大的数字，再舍弃其他节点后，保持原有的平面结构得出output。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-1a4b2a3795d8f073e921d766e70ce6ec_hd.jpg" data-caption="" data-rawwidth="787" data-rawheight="368" class="origin_image zh-lightbox-thumb" width="787" data-original="https://pic4.zhimg.com/v2-1a4b2a3795d8f073e921d766e70ce6ec_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-1a4b2a3795d8f073e921d766e70ce6ec_hd.jpg" data-caption="" data-rawwidth="787" data-rawheight="368" class="origin_image zh-lightbox-thumb lazy" width="787" data-original="https://pic4.zhimg.com/v2-1a4b2a3795d8f073e921d766e70ce6ec_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-1a4b2a3795d8f073e921d766e70ce6ec_hd.jpg"></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><p>max pooling在不同的depth上是分开执行的，且不需要参数控制。 那么问题就max pooling有什么作用？部分信息被舍弃后难道没有影响吗？</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-cd717414dcf32dac4df73c00f1e7c6c3_hd.jpg" data-caption="" data-rawwidth="514" data-rawheight="406" class="origin_image zh-lightbox-thumb" width="514" data-original="https://pic1.zhimg.com/v2-cd717414dcf32dac4df73c00f1e7c6c3_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-cd717414dcf32dac4df73c00f1e7c6c3_hd.jpg" data-caption="" data-rawwidth="514" data-rawheight="406" class="origin_image zh-lightbox-thumb lazy" width="514" data-original="https://pic1.zhimg.com/v2-cd717414dcf32dac4df73c00f1e7c6c3_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-cd717414dcf32dac4df73c00f1e7c6c3_hd.jpg"></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><p>Max pooling的主要功能是downsampling，却不会损坏识别结果。 这意味着卷积后的Feature Map中有对于识别物体不必要的冗余信息。 那么我们就反过来思考，这些“冗余”信息是如何产生的。</p><p>直觉上，我们为了探测到某个特定形状的存在，用一个filter对整个图片进行逐步扫描。但只有出现了该特定形状的区域所卷积获得的输出才是真正有用的，用该filter卷积其他区域得出的数值就可能对该形状是否存在的判定影响较小。 比如下图中，我们还是考虑探测“横折”这个形状。 卷积后得到3x3的Feature Map中，真正有用的就是数字为3的那个节点，其余数值对于这个任务而言都是无关的。 所以用3x3的Max pooling后，并没有对“横折”的探测产生影响。 试想在这里例子中如果不使用Max pooling，而让网络自己去学习。 网络也会去学习与Max pooling近似效果的权重。因为是近似效果，增加了更多的parameters的代价，却还不如直接进行Max pooling。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-8e9d7ec0662e903e475bd93a64067554_hd.jpg" data-caption="" data-rawwidth="545" data-rawheight="345" class="origin_image zh-lightbox-thumb" width="545" data-original="https://pic1.zhimg.com/v2-8e9d7ec0662e903e475bd93a64067554_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-8e9d7ec0662e903e475bd93a64067554_hd.jpg" data-caption="" data-rawwidth="545" data-rawheight="345" class="origin_image zh-lightbox-thumb lazy" width="545" data-original="https://pic1.zhimg.com/v2-8e9d7ec0662e903e475bd93a64067554_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-8e9d7ec0662e903e475bd93a64067554_hd.jpg"></figure><p><br></p><p>Max pooling还有类似“选择句”的功能。假如有两个节点，其中第一个节点会在某些输入情况下最大，那么网络就只在这个节点上流通信息；而另一些输入又会让第二个节点的值最大，那么网络就转而走这个节点的分支。</p><p>但是Max pooling也有不好的地方。因为并非所有的抓取都像上图的极端例子。有些周边信息对某个概念是否存在的判定也有影响。 并且Max pooling是对所有的Feature Maps进行等价的操作。就好比用相同网孔的渔网打鱼，一定会有漏网之鱼。</p><h2>全连接层</h2><p>当抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 全连接层（也叫前馈层）就可以用来将最后的输出映射到<a href="https://link.zhihu.com/?target=https%3A//yjango.gitbooks.io/superorganism/content/ren_gong_shen_jing_wang_luo.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">线性可分的空间</a>。 通常卷积网络的最后会将末端得到的长方体平摊(flatten)成一个长长的向量，并送入全连接层配合输出层进行分类。</p><p>卷积神经网络大致就是covolutional layer, pooling layer, ReLu layer, fully-connected layer的组合，例如下图所示的结构。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-cf87890eb8f2358f23a1ac78eb764257_hd.jpg" data-caption="" data-rawwidth="748" data-rawheight="263" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic2.zhimg.com/v2-cf87890eb8f2358f23a1ac78eb764257_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-cf87890eb8f2358f23a1ac78eb764257_hd.jpg" data-caption="" data-rawwidth="748" data-rawheight="263" class="origin_image zh-lightbox-thumb lazy" width="748" data-original="https://pic2.zhimg.com/v2-cf87890eb8f2358f23a1ac78eb764257_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-cf87890eb8f2358f23a1ac78eb764257_hd.jpg"></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><p>这里也体现了深层神经网络或deep learning之所以称deep的一个原因：模型将特征抓取层和分类层合在了一起。 负责特征抓取的卷积层主要是用来学习“如何观察”。</p><p>下图简述了机器学习的发展，从最初的人工定义特征再放入分类器的方法，到让机器自己学习特征，再到如今尽量减少人为干涉的deep learning。</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-60e7c1e89c5aed5b828cbb24fc1e5a80_hd.jpg" data-caption="" data-rawwidth="566" data-rawheight="789" class="origin_image zh-lightbox-thumb" width="566" data-original="https://pic4.zhimg.com/v2-60e7c1e89c5aed5b828cbb24fc1e5a80_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-60e7c1e89c5aed5b828cbb24fc1e5a80_hd.jpg" data-caption="" data-rawwidth="566" data-rawheight="789" class="origin_image zh-lightbox-thumb lazy" width="566" data-original="https://pic4.zhimg.com/v2-60e7c1e89c5aed5b828cbb24fc1e5a80_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-60e7c1e89c5aed5b828cbb24fc1e5a80_hd.jpg"></figure><p><br></p><p>[<a href="https://link.zhihu.com/?target=http%3A//www.deeplearningbook.org/contents/intro.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">from</a>]</p><h2>结构发展</h2><p>以上介绍了卷积神经网络的基本概念。 以下是几个比较有名的卷积神经网络结构，详细的请看<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">CS231n</a>。</p><ul><li><b>LeNet</b>：第一个成功的卷积神经网络应用</li><li><b>AlexNet</b>：类似LeNet，但更深更大。使用了层叠的卷积层来抓取特征（通常是一个卷积层马上一个max pooling层）</li><li><b>ZF Net</b>：增加了中间卷积层的尺寸，让第一层的stride和filter size更小。</li><li><b>GoogLeNet</b>：减少parameters数量，最后一层用max pooling层代替了全连接层，更重要的是<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.07261" class=" wrap external" target="_blank" rel="nofollow noreferrer">Inception-v4</a>模块的使用。</li><li><b>VGGNet</b>：只使用3x3 卷积层和2x2 pooling层从头到尾堆叠。</li><li><b>ResNet</b>：引入了跨层连接和batch normalization。</li><li><b>DenseNet</b>：将跨层连接从头进行到尾。</li></ul><p>总结一下：这些结构的发展趋势有：</p><ul><li>使用small filter size的卷积层和pooling</li><li>去掉parameters过多的全连接层</li><li>Inception（稍后会对其中的细节进行说明）</li><li>跳层连接</li></ul><h2>不变性的满足</h2><p>接下来会谈谈我个人的，对于画面不变性是如何被卷积神经网络满足的想法。 同时结合不变性，对上面提到的结构发展的重要变动进行直觉上的解读。</p><p>需要明白的是为什么加入不变性可以提高网络表现。 并不是因为我们用了更炫酷的处理方式，而是加入了先验知识，无需从零开始用数据学习，节省了训练所需数据量。 思考表现提高的原因一定要从训练所需要的数据量切入。 提出满足新的不变性特点的神经网络是计算机视觉的一个主要研究方向。</p><h2>平移不变性</h2><p>可以说卷积神经网络最初引入局部连接和空间共享，就是为了满足平移不变性。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-1aac56212d5d143a006d569318e3ee8b_hd.jpg" data-caption="" data-rawwidth="550" data-rawheight="136" class="origin_image zh-lightbox-thumb" width="550" data-original="https://pic2.zhimg.com/v2-1aac56212d5d143a006d569318e3ee8b_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-1aac56212d5d143a006d569318e3ee8b_hd.jpg" data-caption="" data-rawwidth="550" data-rawheight="136" class="origin_image zh-lightbox-thumb lazy" width="550" data-original="https://pic2.zhimg.com/v2-1aac56212d5d143a006d569318e3ee8b_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-1aac56212d5d143a006d569318e3ee8b_hd.jpg"></figure><p><br></p><p>因为空间共享，在不同位置的同一形状就可以被等价识别，所以不需要对每个位置都进行学习。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-18c11c6f485e9f1bbc9a50eb3d248439_hd.jpg" data-caption="" data-rawwidth="411" data-rawheight="374" class="content_image" width="411"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-18c11c6f485e9f1bbc9a50eb3d248439_hd.jpg" data-caption="" data-rawwidth="411" data-rawheight="374" class="content_image lazy" width="411" data-actualsrc="https://pic1.zhimg.com/50/v2-18c11c6f485e9f1bbc9a50eb3d248439_hd.jpg"></figure><p><br></p><h2>旋转和视角不变性</h2><p>个人觉得卷积神经网络克服这一不变性的主要手段还是靠大量的数据。 并没有明确加入“旋转和视角不变性”的先验特性。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-0ce892f8b247f2b48a76cc57cbcba41d_hd.jpg" data-caption="" data-rawwidth="552" data-rawheight="250" class="origin_image zh-lightbox-thumb" width="552" data-original="https://pic1.zhimg.com/v2-0ce892f8b247f2b48a76cc57cbcba41d_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-0ce892f8b247f2b48a76cc57cbcba41d_hd.jpg" data-caption="" data-rawwidth="552" data-rawheight="250" class="origin_image zh-lightbox-thumb lazy" width="552" data-original="https://pic1.zhimg.com/v2-0ce892f8b247f2b48a76cc57cbcba41d_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-0ce892f8b247f2b48a76cc57cbcba41d_hd.jpg"></figure><p><br></p><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06211" class=" wrap external" target="_blank" rel="nofollow noreferrer">Deformable Convolutional Networks</a>似乎是对此变性进行了进行增强。</p><h2>尺寸不变性</h2><p>与平移不变性不同，最初的卷积网络并没有明确照顾尺寸不变性这一特点。</p><p>我们知道filter的size是事先选择的，而不同的尺寸所寻找的形状（概念）范围不同。</p><p>从直观上思考，如果选择小范围，再一步步通过组合，仍然是可以得到大范围的形状。 如3x3尺寸的形状都是可以由2x2形状的图形组合而成。所以形状的尺寸不变性对卷积神经网络而言并不算问题。 这恐怕ZF Net让第一层的stride和filter size更小，VGGNet将所有filter size都设置成3x3仍可以得到优秀结果的一个原因。</p><p>但是，除了形状之外，很多概念的抓取通常需要考虑一个像素与周边更多像素之间的关系后得出。 也就是说5x5的filter也是有它的优点。 同时，小尺寸的堆叠需要很多个filters来共同完成，如果需要抓取的形状恰巧在5x5的范围，那么5x5会比3x3来的更有效率。 所以一次性使用多个不同filter size来抓取多个范围不同的概念是一种顺理成章的想法，而这个也就是Inception。 可以说Inception是为了尺寸不变性而引入的一个先验知识。</p><h2>Inception</h2><p>下图是Inception的结构，尽管也有不同的版本，但是其动机都是一样的：消除尺寸对于识别结果的影响，一次性使用多个不同filter size来抓取多个范围不同的概念，并让网络自己选择需要的特征。</p><p>你也一定注意到了蓝色的1x1卷积，撇开它，先看左边的这个结构。</p><p>输入（可以是被卷积完的长方体输出作为该层的输入）进来后，通常我们可以选择直接使用像素信息(1x1卷积)传递到下一层，可以选择3x3卷积，可以选择5x5卷积，还可以选择max pooling的方式downsample刚被卷积后的feature maps。 但在实际的网络设计中，究竟该如何选择需要大量的实验和经验的。 Inception就不用我们来选择，而是将4个选项给神经网络，让网络自己去选择最合适的解决方案。</p><p>接下来我们再看右边的这个结构，多了很多蓝色的1x1卷积。 这些1x1卷积的作用是为了让网络根据需要能够更灵活的控制数据的depth的。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-9692631d087622f1b34c80055f13fac5_hd.jpg" data-caption="" data-rawwidth="983" data-rawheight="311" class="origin_image zh-lightbox-thumb" width="983" data-original="https://pic1.zhimg.com/v2-9692631d087622f1b34c80055f13fac5_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-9692631d087622f1b34c80055f13fac5_hd.jpg" data-caption="" data-rawwidth="983" data-rawheight="311" class="origin_image zh-lightbox-thumb lazy" width="983" data-original="https://pic1.zhimg.com/v2-9692631d087622f1b34c80055f13fac5_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-9692631d087622f1b34c80055f13fac5_hd.jpg"></figure><p><br></p><h2>1x1卷积核</h2><p>如果卷积的输出输入都只是一个平面，那么1x1卷积核并没有什么意义，它是完全不考虑像素与周边其他像素关系。 但卷积的输出输入是长方体，所以1x1卷积实际上是对每个像素点，在不同的channels上进行线性组合（信息整合），且保留了图片的原有平面结构，调控depth，从而完成升维或降维的功能。</p><p>如下图所示，如果选择2个filters的1x1卷积层，那么数据就从原本的depth 3 降到了2。若用4个filters，则起到了升维的作用。</p><p>这就是为什么上面Inception的4个选择中都混合一个1x1卷积，如右侧所展示的那样。 其中，绿色的1x1卷积本身就1x1卷积，所以不需要再用另一个1x1卷积。 而max pooling用来去掉卷积得到的Feature Map中的冗余信息，所以出现在1x1卷积之前，紧随刚被卷积后的feature maps。（由于没做过实验，不清楚调换顺序会有什么影响。）</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-59429b22ac90930c502736b33db0d8e0_hd.jpg" data-caption="" data-rawwidth="643" data-rawheight="517" class="origin_image zh-lightbox-thumb" width="643" data-original="https://pic3.zhimg.com/v2-59429b22ac90930c502736b33db0d8e0_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-59429b22ac90930c502736b33db0d8e0_hd.jpg" data-caption="" data-rawwidth="643" data-rawheight="517" class="origin_image zh-lightbox-thumb lazy" width="643" data-original="https://pic3.zhimg.com/v2-59429b22ac90930c502736b33db0d8e0_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-59429b22ac90930c502736b33db0d8e0_hd.jpg"></figure><p><br></p><h2>跳层连接</h2><p>前馈神经网络也好，卷积神经网络也好，都是一层一层逐步变换的，不允许跳层组合。 但现实中是否有跳层组合的现象？</p><p>比如说我们在判断一个人的时候，很多时候我们并不是观察它的全部，或者给你的图片本身就是残缺的。 这时我们会靠单个五官，外加这个人的着装，再加他的身形来综合判断这个人，如下图所示。 这样，即便图片本身是残缺的也可以很好的判断它是什么。 这和前馈神经网络的先验知识不同，它允许不同层级之间的因素进行信息交互、综合判断。</p><p>残差网络就是拥有这种特点的神经网络。大家喜欢用identity mappings去解释为什么残差网络更优秀。 这里我只是提供了一个以先验知识的角度去理解的方式。 需要注意的是每一层并不会像我这里所展示的那样，会形成明确的五官层。 只是有这样的组合趋势，实际无法保证神经网络到底学到了什么内容。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-40fb6ab7bf89ce43af1c52e673da65eb_hd.jpg" data-caption="" data-rawwidth="509" data-rawheight="507" class="origin_image zh-lightbox-thumb" width="509" data-original="https://pic3.zhimg.com/v2-40fb6ab7bf89ce43af1c52e673da65eb_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-40fb6ab7bf89ce43af1c52e673da65eb_hd.jpg" data-caption="" data-rawwidth="509" data-rawheight="507" class="origin_image zh-lightbox-thumb lazy" width="509" data-original="https://pic3.zhimg.com/v2-40fb6ab7bf89ce43af1c52e673da65eb_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-40fb6ab7bf89ce43af1c52e673da65eb_hd.jpg"></figure><p><br></p><p>用下图举一个更易思考的例子。 图形1,2,3,4,5,6是第一层卷积层抓取到的概念。 图形7,8,9是第二层卷积层抓取到的概念。 图形7,8,9是由1,2,3,4,5,6的基础上组合而成的。</p><p>但当我们想要探测的图形10并不是单纯的靠图形7,8,9组成，而是第一个卷积层的图形6和第二个卷积层的8,9组成的话，不允许跨层连接的卷积网络不得不用更多的filter来保持第一层已经抓取到的图形信息。并且每次传递到下一层都需要学习那个用于保留前一层图形概念的filter的权重。 当层数变深后，会越来越难以保持，还需要max pooling将冗余信息去掉。</p><p>一个合理的做法就是直接将上一层所抓取的概念也跳层传递给下下一层，不用让其每次都重新学习。 就好比在编程时构建了不同规模的functions。 每个function我们都是保留，而不是重新再写一遍。提高了重用性。</p><p>同时，因为ResNet使用了跳层连接的方式。也不需要max pooling对保留低层信息时所产生的冗余信息进行去除。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-87fc4b7449d751c59977c3a368ae6f7e_hd.jpg" data-caption="" data-rawwidth="484" data-rawheight="436" class="origin_image zh-lightbox-thumb" width="484" data-original="https://pic1.zhimg.com/v2-87fc4b7449d751c59977c3a368ae6f7e_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-87fc4b7449d751c59977c3a368ae6f7e_hd.jpg" data-caption="" data-rawwidth="484" data-rawheight="436" class="origin_image zh-lightbox-thumb lazy" width="484" data-original="https://pic1.zhimg.com/v2-87fc4b7449d751c59977c3a368ae6f7e_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-87fc4b7449d751c59977c3a368ae6f7e_hd.jpg"></figure><p><br></p><p>Inception中的第一个1x1的卷积通道也有类似的作用，但是1x1的卷积仍有权重需要学习。 并且Inception所使用的结合方式是concatenate的合并成一个更大的向量的方式，而ResNet的结合方式是sum。 两个结合方式各有优点。 concatenate当需要用不同的维度去组合成新观念的时候更有益。 而sum则更适用于并存的判断。比如既有油头发，又有胖身躯，同时穿着常年不洗的牛仔裤，三个不同层面的概念并存时，该人会被判定为程序员的情况。 又比如双向LSTM中正向和逆向序列抓取的结合常用相加的方式结合。在语音识别中，这表示既可以正向抓取某种特征，又可以反向抓取另一种特征。当两种特征同时存在时才会被识别成某个特定声音。</p><p>在下图的ResNet中，前一层的输入会跳过部分卷积层，将底层信息传递到高层。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-d3fd09f011583932b832ea64f78233af_hd.jpg" data-caption="" data-rawwidth="317" data-rawheight="615" class="content_image" width="317"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-d3fd09f011583932b832ea64f78233af_hd.jpg" data-caption="" data-rawwidth="317" data-rawheight="615" class="content_image lazy" width="317" data-actualsrc="https://pic2.zhimg.com/50/v2-d3fd09f011583932b832ea64f78233af_hd.jpg"></figure><p><br></p><p>在下图的DenseNet中，底层信息会被传递到所有的后续高层。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-0bebba2947e5e968a93e6def0ae5d00c_hd.jpg" data-caption="" data-rawwidth="602" data-rawheight="389" class="origin_image zh-lightbox-thumb" width="602" data-original="https://pic2.zhimg.com/v2-0bebba2947e5e968a93e6def0ae5d00c_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-0bebba2947e5e968a93e6def0ae5d00c_hd.jpg" data-caption="" data-rawwidth="602" data-rawheight="389" class="origin_image zh-lightbox-thumb lazy" width="602" data-original="https://pic2.zhimg.com/v2-0bebba2947e5e968a93e6def0ae5d00c_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-0bebba2947e5e968a93e6def0ae5d00c_hd.jpg"></figure><p><br></p><h2>后续</h2><p>随着时间推移，各个ResNet,GoogLeNet等框架也都在原有的基础上进行了发展和改进。 但基本都是上文描述的概念的组合使用加上其他的tricks。</p><p>如下图所展示的，加入跳层连接的Inception-ResNet。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-389496d1436895dfe43199a0f54c35ca_hd.jpg" data-caption="" data-rawwidth="1820" data-rawheight="1086" class="origin_image zh-lightbox-thumb" width="1820" data-original="https://pic2.zhimg.com/v2-389496d1436895dfe43199a0f54c35ca_r.jpg"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-389496d1436895dfe43199a0f54c35ca_hd.jpg" data-caption="" data-rawwidth="1820" data-rawheight="1086" class="origin_image zh-lightbox-thumb lazy" width="1820" data-original="https://pic2.zhimg.com/v2-389496d1436895dfe43199a0f54c35ca_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-389496d1436895dfe43199a0f54c35ca_hd.jpg"></figure><p><br></p><p>但对我而言，</p><blockquote><b>真正重要的是这些技巧对于各种不变性的满足。</b></blockquote><p><br></p><p><b>深度学习通俗易懂教程专栏<a href="https://zhuanlan.zhihu.com/YJango" class="internal">超智能体 - 知乎专栏</a></b></p><p><b>阅读列表：</b></p><ol><li><b>深层神经网络：</b><a href="https://zhuanlan.zhihu.com/p/22888385" class="internal">深层学习为何要“Deep”（上）</a>（由于下篇写的并不通俗，不推荐阅读，用公开课代替）</li><li><b>反向传播算法实例：</b>未编写</li><li><b>深度学习总览：</b>公开课：<a href="https://zhuanlan.zhihu.com/p/26647094" class="internal">深层神经网络设计理念</a></li><li><b>深度学习入门误区：</b><a href="https://zhuanlan.zhihu.com/p/24273922" class="internal">知乎Live</a>（公开课涵盖了Live的内容，若觉得作者辛苦也可参加。算了，还是不要参加了！）</li><li><b>Tensorflow ：</b><a href="https://zhuanlan.zhihu.com/p/23932714" class="internal">TensorFlow整体把握</a></li><li><b>前馈神经网络（1）：</b><a href="https://zhuanlan.zhihu.com/p/27853521" class="internal">前馈神经网络--代码LV1</a></li><li><b>前馈神经网络（2）：</b><a href="https://zhuanlan.zhihu.com/p/27853766" class="internal">前馈神经网络--代码LV2</a></li><li><b>前馈神经网络（3）：</b><a href="https://zhuanlan.zhihu.com/p/27854076" class="internal">前馈神经网络--代码LV3</a></li><li><b>循环神经网络（1）</b>：<a href="https://zhuanlan.zhihu.com/p/24720659" class="internal">循环神经网络--介绍</a></li><li><b>循环神经网络（2）</b>：<a href="https://zhuanlan.zhihu.com/p/25518711" class="internal">循环神经网络--实现LSTM</a></li><li><b>循环神经网络（3）</b>：<a href="https://zhuanlan.zhihu.com/p/25821063" class="internal">循环神经网络--scan实现LSTM</a></li><li><b>循环神经网络（4）</b>：<a href="https://zhuanlan.zhihu.com/p/25858226" class="internal">循环神经网络--双向GRU</a></li><li><b>卷积神经网络（1）</b>：<a href="https://zhuanlan.zhihu.com/p/27642620" class="internal">卷积神经网络--介绍</a></li></ol></span><!-- react-empty: 1036 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/194996805"><span data-tooltip="发布于 2017-07-07"><!-- react-text: 1041 -->编辑于 <!-- /react-text --><!-- react-text: 1042 -->2017-10-19<!-- /react-text --></span></a></div></div><div><div class="ContentItem-actions Sticky RichContent-actions is-bottom"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 3989 -->872<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 3996 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 3999 -->47 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-97481-35456-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-97481-35456-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4004 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4007 -->分享<!-- /react-text --></button></div><!-- react-empty: 4008 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4011 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4014 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4017 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4020 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div></div><!-- react-empty: 1086 --><!-- react-empty: 2588 --><!-- react-empty: 1088 --><!-- react-empty: 1089 --><!-- react-empty: 2589 --><!-- react-empty: 1091 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="7" data-zop="{&quot;authorName&quot;:&quot;许铁-巡洋舰科技&quot;,&quot;itemId&quot;:282874143,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="282874143" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;282874143&quot;,&quot;upvote_num&quot;:61,&quot;comment_num&quot;:4,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;901ad779582a43a79f86a55004f0410f&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="许铁-巡洋舰科技"><meta itemprop="image" content="https://pic4.zhimg.com/v2-eafd9ea1543db5e1963bf5ec1665f810_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/hun-dun-xun-yang-jian"><meta itemprop="zhihu:followerCount" content="9983"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85792-78367-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85792-78367-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/hun-dun-xun-yang-jian"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-eafd9ea1543db5e1963bf5ec1665f810_xs.jpg" srcset="https://pic4.zhimg.com/v2-eafd9ea1543db5e1963bf5ec1665f810_l.jpg 2x" alt="许铁-巡洋舰科技"></a></div><!-- react-empty: 1105 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85792-84048-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85792-84048-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/hun-dun-xun-yang-jian">许铁-巡洋舰科技</a></div><!-- react-empty: 1112 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">微信公众号请关注chaoscruiser ,铁哥个人微信号ironcruiser</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1119 -->61 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1120 --></span></div></div><meta itemprop="image" content="https://pic1.zhimg.com/v2-8336e5d89cbef19182f70d84b650e0af_200x112.jpg"><meta itemprop="upvoteCount" content="61"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/282874143"><meta itemprop="dateCreated" content="2017-12-25T12:13:56.000Z"><meta itemprop="dateModified" content="2017-12-25T12:13:56.000Z"><meta itemprop="commentCount" content="4"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>说起CNN，最初人们想到的都是某电视台，但等过几年，人们想起的多半是深度学习了。</p><p><br></p><p>应该说， CNN是这两年深度学习风暴的罪魁祸首， 自2012年， 正是它让打入冷宫的神经网络重见天日并且建立起自己在人工智能王国的霸主地位。</p><p><br></p><p>如过你认为深度学习是只能用来理解图像的，你就大错特错了， 因为它的用途太广了，上至文字，中有图像， 下至音频， 从手写数字识别到大名鼎鼎的GAN对抗学习， 都离不开它。</p><p><br></p><p>不过要了解CNN，还是拿图像做例子比较恰当。一句话来说CNN图像处理的本质，就是信息抽取， 巨大的网络可以抽取一步步得到最关键的图像特征， 我们有时也叫自动的特征工程。</p><p><br></p><p>CNN的建造灵感来自于人类对视觉信息的识别过程。 人脑对物体的识别的第一个问题是： 对应某一类对象的图像千千万， 比如一个苹果， 就有各种状态的成千上万状态， 我们识别物体的类别，事实上是给这成千上万不同的图片都打上同一个标签。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-8336e5d89cbef19182f70d84b650e0af_hd.jpg" data-size="normal" data-rawwidth="271" data-rawheight="186" class="content_image" width="271"&gt;</noscript><img src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-8336e5d89cbef19182f70d84b650e0af_hd.jpg" data-size="normal" data-rawwidth="271" data-rawheight="186" class="content_image lazy" width="271" data-actualsrc="https://pic1.zhimg.com/50/v2-8336e5d89cbef19182f70d84b650e0af_hd.jpg"><figcaption>CNN的灵感来自人大脑</figcaption></figure><p>物理里管这种一个事物的结果与一些列的变化都没有关的特性，叫不变性， 比如如果你转动一个苹果任何一个角度它都是苹果， 这就是苹果有旋转不变性，但是数字6就不行， 如果你给它旋转特定角度它就变成9了， 它就没有旋转不变性。</p><p><br></p><p>我们人通常可以无视这些变化认出事物来，也就是把和这种变化有关的信息忽略。如果我们对图像进行识别， 事实上我们的算法就要有人的这种本领， 首先让它学会什么东西与真实的物体信息是无关的。</p><p><br></p><p>就拿数字识别举个例子吧， 一个数字是什么，虽然与旋转的角度有关系， 但与它在图片中的上下左右没关系， 我们管这种不变性叫平移不变性。</p><p><br></p><p>解决这个问题，最粗暴的一个方法是制造很多的样本，比如把“1” 放在很多不同的位置，然后让机器在错误中学习。 然后穷尽所有的位置， 不过我相信没有人是这么完成对物体的识别的。</p><p><br></p><p>那怎么办？CNN中的卷积正是这一问题的答案，因为卷积操作本身具有平移不变性（我知道听起来不明觉厉 ，请看下文）。</p><p><br></p><p>卷积，顾名思义， “卷”有席卷的意思，“积“ 有乘积的意思。 卷积实质上是用一个叫kernel的矩阵，从图像的小块上一一贴过去，一次和图像块的每一个像素乘积得到一个output值， 扫过之后就得到了一个新的图像。我们用一个3*3的卷积卷过一个4*4的图像， 看看取得的效果。</p><p><br></p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-17dea398ba752e3c78523b4c9a194b99_hd.jpg" data-size="normal" data-rawwidth="800" data-rawheight="548" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic2.zhimg.com/v2-17dea398ba752e3c78523b4c9a194b99_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-17dea398ba752e3c78523b4c9a194b99_hd.jpg" style="width: 654px; height: 447.99px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>卷积的数学过程</figcaption></figure><p>一个卷积核就像一个小小的探测器， 它的DNA是被刻录在卷积核的数字里的， 告诉我们它要干什么， 而卷积核扫过图片，只要它的DNA是不变的，那么它在图片上下左右的哪个位置看到的结果都相同， 这变是卷积本身具有平移不变性的原理。 由于这种不变性， 一个能够识别1的卷积在图片的哪个位置都可以识别1，一次训练成本，即可以对任何图片进行操作。</p><p><br></p><p>图像处理领域，卷积早已有另一个名字 ， 叫做滤镜，滤波器， 我们把图像放进去，它就出来一个新图像，可以是图像的边缘，可以是锐化过的图像，也可以是模糊过的图像。</p><p><br></p><p>如果大家玩过photoshop， 大家都会发现里面有一些滤镜，比如说锐化，模糊， 高反差识别这一类，都是用着类似的技术，这样的技术所作的事情是图像的每个小片用一个矩阵进行处理，得到一个画面的转换 。 我们有时候会说低通和高通滤镜 ，低通滤镜通常可以用来降噪， 而高通则可以得到图像的细微纹理。 你玩photoshop，玩的就是卷积，卷积核里面的数字定了， 它的功能也就定了。</p><p><br></p><p>为什么这样做有效果了？因为图像的特征往往存在于相邻像素之间， kernel就是通过计算小区域内像素的关系来提取局部特征，可以理解为一个局部信息的传感器， 或物理里的算子。</p><p><br></p><p>比如提到的边缘提取滤镜， 它所做的物理操作又称为拉普拉斯， 只有像素在由明亮到变暗的过程里它才得1， 其他均得0，因此它所提取的图像特征就是边缘。 事实上我们知道图像中的信息往往包含在其边缘，你给以一个人画素描， 一定能够完全识别这个人 。 我们通过寻找到信息的关键载体-边缘， 而把其他多余的信息过滤掉，得到了比第一层更好处理的图像， 大大减少了需要搜索图像的可能性。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-e2ac64aa84ee40f6015e5eb01f741a9b_hd.jpg" data-size="normal" data-rawwidth="638" data-rawheight="479" class="origin_image zh-lightbox-thumb" width="638" data-original="https://pic1.zhimg.com/v2-e2ac64aa84ee40f6015e5eb01f741a9b_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic1.zhimg.com/50/v2-e2ac64aa84ee40f6015e5eb01f741a9b_hd.jpg" style="width: 638px; height: 479px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>卷积的边缘抽取过程</figcaption></figure><p>常用于卷积的Kernel本质是两个： 第一， kernel具有局域性， 即只对图像中的局部区域敏感， 第二， 权重共享。 也就是说我们是用一个kernel来扫描整个图像， 其中过程kernel的值是不变的。这点就可以保证刚刚说的平移不变形。 比如说你识别一个物体， 显然你的识别不应该依赖物体的位置。 和位置无关， 及平移不变。</p><p><br></p><p>那卷积如何帮你从不同的图形中识别数字1了？数字的尖锐的线条会让卷积的值很高（响起警报）。无论你1出现在图像中的哪一个位置， 我的局部扫描+统一权重算法都给你搞出来， 你用同一个识别1的卷积核来扫过图片，voila，任何一个位置我都给你找出来。</p><p><br></p><p>那卷积和神经网络有什么关系了？答案是卷积扫过图像，每一个卷积核与图像块相乘的过程，都可以看作是一个独立的神经元用它的神经突触去探测图像的一个小局部，然后再做一个决策，就是我看到了什么或没看到什么。整个卷积过程， 不就对应一层神经网络吗？啊哈， 整个卷积过程相当于一层神经网络！</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-633abe607bdd38717a67046c21c67c94_hd.jpg" data-size="normal" data-rawwidth="878" data-rawheight="589" class="origin_image zh-lightbox-thumb" width="878" data-original="https://pic4.zhimg.com/v2-633abe607bdd38717a67046c21c67c94_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/50/v2-633abe607bdd38717a67046c21c67c94_hd.jpg" style="width: 654px; height: 438.731px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>一个个小探测器一般的神经元</figcaption></figure><p>刚刚说了卷积是一个能够对图片中任何位置的同一类信息进行抽取的工具， 那么我们还讲到我们除了抽取， 还要做的一个工作是，取出重要信息，扔掉不重要的，实现这一个的操作，叫做pooling</p><p><br></p><p>但是大家注意，这个时候如果原图像是28*28， 那么从kernel里出来的图形依然是28*28， 而事实上， 事实是上， 大部分时候一个图像的局部特征的变化都不会是像素级。我们可以把局部特征不变形看做一个假设， 把这个假设作为一个数学公式加入到卷积层里帮我们过滤冗余信息， 这就是pooling所做的事情 -也就是扔掉你周边得和你长得差不多得那些像素。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-79f0799430eac4ae7c76274259e17c11_hd.jpg" data-size="normal" data-rawwidth="600" data-rawheight="313" class="origin_image zh-lightbox-thumb" width="600" data-original="https://pic2.zhimg.com/v2-79f0799430eac4ae7c76274259e17c11_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-79f0799430eac4ae7c76274259e17c11_hd.jpg" style="width: 600px; height: 313px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>Max Pooling的数学过程</figcaption></figure><p><br></p><p>Pooling的本质即降采样，以提升统计效率，用一个比较冠冕的话说是利用局部特征不变性降维 ，pooling的方法很多，常见的叫做max pooling，就是找到相邻几个像素里值最大的那个作为代表其它扔掉。</p><p><br></p><p>这样经过从卷积到pooling的过程， 在识别1的任务里，我们可以验明在每个小区域里有没有存在边缘， 从而找到可能存在1的区域。 在pooling的终结点， 我们得到的是一个降低维度了的图像，这个图像的含义是告诉你在原有的图像的每个区域里是含有1还是不含有1， 又叫做特征图。</p><p>好了，我们可以从一堆图片中识别出1了， 那么我们怎么搞定2呢？ 我们把2写成一个Z型， 你有没有思路我们如何做到这点？ 我们不能只识别竖着的线条，还需要识别横向的线条，记住，一个卷积层只搞定一个特征，如果你既要找竖线也要找横线， 我们需要两个不同的卷积层，并且把他们并联在一起，</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-bc7c9f6f0c7e5f367ae0de83eaa7668d_hd.jpg" data-size="normal" data-rawwidth="1414" data-rawheight="1442" class="origin_image zh-lightbox-thumb" width="1414" data-original="https://pic1.zhimg.com/v2-bc7c9f6f0c7e5f367ae0de83eaa7668d_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic1.zhimg.com/50/v2-bc7c9f6f0c7e5f367ae0de83eaa7668d_hd.jpg" style="width: 654px; height: 666.95px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>手写数字识别</figcaption></figure><p>然后呢？ 横线对应一张特征图， 竖线对应另一个张特征图， 如果要识别2， 你无非需要比较这两张特征图，看是否有哪个位置两个特征图同时发生了警报（既有横线又有竖线）。</p><p>这个比较的过程，我们还是可以用一个卷积搞定（理由依然是平移不变性）！</p><p>这个时候， 新的卷积层对之前并连的两个卷积的结果做了一个综合， 或者说形成了一个特征之特征， 即横向和竖线交叉的特征。</p><p><br></p><p>这里把我们的理论可以更上一层路。 深度意味着什么？ 我们想一下， 要正确的识别一个图像，你不可能只看变，也不可能只看边角， 你要对图像的整体有认识才知道张三李四。 也就是说我们要从局部关联进化到全局关联， 真实的图像一定是有一个全局的，比如手我的脸， 只有我的眼镜，鼻子耳朵都被一起观察时候才称得上我的脸，一个只要局部，就什么都不是了。如何提取全局特征？</p><p><br></p><p>从一个层次到另一个层次的递进， 通常是对上一层次做横向以及纵向的整合（图层间的组合或图层之内的组合或两者），我们的特征组合是基于之前已经通过pooling降低维度的图层，因此事实上每一个神经元决策的信息相对上一层都更多，我们用一个学术名词 – 感受野来表述一个神经元决策所涵盖的像素多少， 上一层次看到更多的输入神经元， 因此感受野看更多了 。 越靠近顶层的神经元， 所要做的事情就越接近全局关联。</p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-3dfcd951a40c672f48048d9aeaac5999_hd.jpg" data-caption="" data-size="normal" data-rawwidth="331" data-rawheight="152" class="content_image" width="331"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-3dfcd951a40c672f48048d9aeaac5999_hd.jpg" style="width: 331px; height: 152px;"><div class="VagueImage-mask is-active"></div></div></span></figure><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-4d5bb9872cb04eb52c08091f25b54743_hd.jpg" data-size="normal" data-rawwidth="602" data-rawheight="339" class="origin_image zh-lightbox-thumb" width="602" data-original="https://pic2.zhimg.com/v2-4d5bb9872cb04eb52c08091f25b54743_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-4d5bb9872cb04eb52c08091f25b54743_hd.jpg" style="width: 602px; height: 339px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>越深，感受野越大， 表示越抽象</figcaption></figure><p>这和物理学的一个基本方法--尺度变换有着异曲同工之妙（我们后面讲）， 也是提取全局信息的一个非常核心的办法，我管它叫级级递进法。 你一级一级的进行对画面进行降采样， 把图像里的四个小格子合成一个， 再把新的图像里四个小格子合成一个， 直到一个很大的图像被缩小成一个小样。每一层的卷积，都不是一个卷积，而是一组执行不同特征提取的卷积网络，比如我刚刚说的 不同方向的边缘沟成的一组卷积， 你可以想象后面有不同大小的角度组成的一组网络， 他体现了在一个空间尺度上我们所能够达到的特征工程。</p><p><br></p><p>如此级级互联， 越靠上层感受野就越大。 整个CNN网络如同一封建等级社会，最上层的，就是君王，它是整个集团唯一具有全局视野的人，下一级别， 是各大领主，然后是领主上的风尘，骑士，知道农民（底层神经元）。</p><p><br></p><p>我们把刚刚的全局换一个词叫抽象。深度卷积赋予了神经网络以抽象能力。 这样的一级级向上卷积做基变换的过程，有人说叫搞基（深度学习就是搞基），深一点想叫表征， 和人的思维做个比喻就是抽象。 抽象是我在很深的层次把不同的东西联系起来，CNN教会了我们事先抽象的一种物理方法。</p><p><br></p><p>到目前为止， 我所描述的是都是一些人工的特征工程，即使网络在深，顶多说的上是深度网络，而与学习无关。我们说这样一个系统（mxnxpxz）， 我们要人工设计，几乎穷经皓首也可能做的都是错的。我们说， 这样的一个结构， 只能靠机器自己学，这就是深度学习的本质了， 我们通过几条basic假设（正则）和一个优化函数，让优化（进化）来寻找这样一个结构。 Basic假设无非图像的几个基本结构， 体现在几个不变形上，物理真是好伟大啊。</p><p><br></p><p>深度学习的训练，就是计算机帮助人完成了机器学习最难的一步特征工程（特征工程本质就是基变换啊）。以前人类穷尽脑汁思考如何做图像识别， 是寻找人是如何识别图像的， 希望把人能用来识别物体的特征输入给计算机， 但是现在通过深度卷积，计算机自己完成了这个过程。</p><p><br></p><p>卷积网络在2012 年的发展趋势， 大家可以关注几个方向：</p><p><br></p><p>1， 更深的模型 ： 从AlexNet到VCG19 ，High way network 再到残差网络， 一个主要的发展趋势是更深的模型。 当你采用更深的模型，经常你会发现一些神奇的事情发生了。 当然网络的宽度（通道数量）也在增加。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-c129032a1aa776ffbe24e5be9157c156_hd.jpg" data-size="normal" data-rawwidth="638" data-rawheight="479" class="origin_image zh-lightbox-thumb" width="638" data-original="https://pic3.zhimg.com/v2-c129032a1aa776ffbe24e5be9157c156_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-c129032a1aa776ffbe24e5be9157c156_hd.jpg" style="width: 638px; height: 479px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>这只是最初级的CNN</figcaption></figure><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-871aeced723dfa6eb1413499034e4054_hd.jpg" data-size="normal" data-rawwidth="638" data-rawheight="359" class="origin_image zh-lightbox-thumb" width="638" data-original="https://pic3.zhimg.com/v2-871aeced723dfa6eb1413499034e4054_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-871aeced723dfa6eb1413499034e4054_hd.jpg" style="width: 638px; height: 359px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>这也只是小菜一碟</figcaption></figure><p>2， 更通畅的信息交换 : 深，带来的第一个问题是训练困难， 反向传播难以传递。 从残差网络， 到目前开始流行的Dense Network， 一个主要的发展趋势是不同层级间的信息的交换越来越通畅。 我们逐步在不同层之间加入信息的直连通道。</p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-e6610f9332d22740128c0b8cd741e716_hd.jpg" data-size="normal" data-rawwidth="1504" data-rawheight="1000" class="origin_image zh-lightbox-thumb" width="1504" data-original="https://pic2.zhimg.com/v2-e6610f9332d22740128c0b8cd741e716_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-e6610f9332d22740128c0b8cd741e716_hd.jpg" style="width: 654px; height: 434.84px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>Dense Network</figcaption></figure><p>3， 与监督学习之外的学习方法的结合， 如迁移学习， 半监督学习， 对抗学习， 和强化学习。 后两者的有趣程度远超监督学习。</p><p><br></p><p>4， 轻量化， CNN网络越来越深， 使得网络的文件动辄装不下， 这点使得CNN网络的轻量化部署成为重点， 我们希望在性能和能耗中取中。 一个很好的办法是对网络权重进行减枝，去掉不重要的权重， 另外一个是把每个权重的数据位数本身缩减，甚至是使用0和1表示， 虽然看上去我们丢失了很多信息， 但是由于巨大网络中的信息是统计表达的，我们到底损失多大还真不一定。</p><p><br></p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-880028d8425196c58c6c034d0a8829ec_hd.jpg" data-size="normal" data-rawwidth="658" data-rawheight="350" class="origin_image zh-lightbox-thumb" width="658" data-original="https://pic3.zhimg.com/v2-880028d8425196c58c6c034d0a8829ec_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-880028d8425196c58c6c034d0a8829ec_hd.jpg" style="width: 654px; height: 347.872px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>酷似生物过程的剪枝处理</figcaption></figure><p><br></p><p>以上是CNN的小结， 不要以为图像处理与你无关，我刚刚说的其实一篇文章如果你把它转化为一个矩阵无非一个图像， 一段音频你给它转换成一个矩阵无非一个图像， 你看， 都可以和CNN挂钩。</p><p><br></p><p>我想说，无论你是做什么的， 无论是苦逼的计算机工程师， 游戏设计师，还是外表高大上的金融分析师，甚至作为一个普通消费者， 你的生活以后都和CNN脱不开干系了 ， 预知更多情报还请关注：</p><p><br></p><p>巡洋舰的<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzQwNzI3OA%3D%3D%26mid%3D2651383029%26idx%3D1%26sn%3Da792653f736540e06d96e6f3970264e0%26chksm%3D84f3cab4b38443a2754072d8b3d3fdade7ea503b15ecea46595bb149edcc139000b7f315e624%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">深度学习实战课程</a>， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛。刚刚讲的方法都将在课程里详细展开。</p><p><br></p><p>目前课程线下版本已经基本报名完毕（特殊申请可加一到两个名额）， 为了缓解众多异地学员的需求， 我们提出一个线上加线下的课程简版， 课程包括全部课程视频， notebook作业， 和一个课程模块的来京线下实践机会， 名额限5名，预报从速，详情请联系陈欣（cx13951038115）。</p><p><br></p><p>「真诚赞赏，手留余香」</p></span><!-- react-empty: 1130 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/282874143">发布于 2017-12-25</a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 4032 -->61<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4039 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4042 -->4 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-97918-47727-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-97918-47727-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4047 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4050 -->分享<!-- /react-text --></button></div><!-- react-empty: 4051 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4054 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4057 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 4060 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 4063 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 1177 --><!-- react-empty: 2590 --><!-- react-empty: 1179 --><!-- react-empty: 1180 --><!-- react-empty: 2591 --><!-- react-empty: 1182 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="8" data-zop="{&quot;authorName&quot;:&quot;成勇敢&quot;,&quot;itemId&quot;:81312717,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="81312717" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;81312717&quot;,&quot;upvote_num&quot;:108,&quot;comment_num&quot;:12,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;8500ada6077a3ec98bdfb488cfe0a659&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="成勇敢"><meta itemprop="image" content="https://pic2.zhimg.com/cda51b6e598339716adddf4653075e37_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/sea-sea-41"><meta itemprop="zhihu:followerCount" content="112"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85805-57053-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85805-57053-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/sea-sea-41"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/cda51b6e598339716adddf4653075e37_xs.jpg" srcset="https://pic2.zhimg.com/cda51b6e598339716adddf4653075e37_l.jpg 2x" alt="成勇敢"></a></div><!-- react-empty: 1196 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85805-5362-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85805-5362-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/sea-sea-41">成勇敢</a></div><!-- react-empty: 1203 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">好读书，无甚解</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1210 -->108 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1211 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="108"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/81312717"><meta itemprop="dateCreated" content="2016-01-13T02:21:54.000Z"><meta itemprop="dateModified" content="2018-01-05T05:16:23.000Z"><meta itemprop="commentCount" content="12"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>更新2016、4、29<br>内容大概就这样了，需要补充什么欢迎评论。<br>……………………………………………<br>…………转载请注明出处…………<br>………………………………………………</p><p>说一下我的个人理解，<br>错误与不足之处请各位指正。</p><p><b>一 卷积</b></p><p><i><b>1 什么是图像卷积？</b></i></p><p>图像是一个二维的离散信号，对图像卷积，就是求卷积核作用在图像后得到的图像对于该卷积核的累加响应。<br>比如，Sobel算子可以得到图像的边缘。</p><p><i><b>2 为什么进行图像卷积？</b></i></p><p>我们人看到一幅图像，眨眼之间就知道图像中有什么，图像中的主体在干什么。但计算机不同，计算机看到的每一副图像都是一个数字矩阵。那我们怎么让计算机从一个个数字矩阵中得到有用的信息呢，比如边缘，角点？更甚一点，怎么让计算机理解图像呢？</p><p>对图像进行卷积，就是接近目标的第一步。</p><p><b>二 一个简单的分类处理流程</b></p><p>在计算机真正理解图像之前，我们先让计算机知道一幅图像中是猫 ，另一副是狗 。我们以分类为例进行后续介绍。</p><p>分类问题，在机器学习中就是典型的监督学习问题。</p><p>这里，我按照cs231n上举的例子介绍一个简单的图像分类流程。<br>我们用曼哈顿或者欧氏距离加上最或者K近邻分类器分类器分类。</p><p><b><i>训练阶段</i></b><br>用最近邻分类器做分类时，训练阶段非常简单。我们只是读取图片，并记住每张图片的类标。<br><b><i>测试阶段</i></b><br>我们对每一张测试图像遍历整个样本集，找到和它最或者前K个相似的图像。</p><p>相似怎么度量？就是简单的用L1或L2距离。而整个过程中到底使用L1还是L2，是最近邻还是K近邻，K到底取多少？这些都是参数，也就是调参需要调的东西。</p><p>显然，用K近邻分类器的优点是训练阶段毫不费事，但是测试阶段随着样本集的增大可是要了命了。然而，CNN与之相反，它的训练过程较为漫长，而测试阶段时耗很少。</p><p><b>三 传统方法</b></p><p>这里，我介绍一个简单的猫狗分类任务。<br>假设，我们拿到了许多张猫和狗的图像。<br><b><i>训练阶段</i></b><br>1 首先进行一些简单的图像预处理，诸如平滑，去噪，光照归一化等等。<br>2 提取一些诸如SIFT, HOG, LBP特征。<br>2.5 这一步可有可无，就是对特征进行编码，比如常用BoW，FisherVector等。<br>3 将特征放到一个分类器，比如SVM，进行2分类，训练出最优分类面。<br><b><i>测试阶段</i></b><br>1,2 和训练阶段相同。<br>3 用刚才训练好的分类器做出决策。</p><p>但是，关键问题是：这些手工设计的特征，是不是真的对分类有利呢？可不可以让机器自己学习出对分类有利的特征呢？<br>我们引入卷积神经网络。</p><p><b>四 基于CNN的图像分类</b></p><p>我们第三部分介绍了传统方法下的图像分类流程：特征提取+特征表达+分类。而CNN将这些步骤合并到一个统一整体中（当然你也可以用CNN来提取特征用其他分类器训练分类）。<br>CNN自YannLecun的手写字符识别及AlexNet在ImageNet夺冠后声名大噪，广泛运用到很多图像分类的场景中。<br><b>4.1 卷积层</b><br>下面，我简单卷积层的相关知识。<br><i><u>4.1.1 局部连接和权值共享</u></i><br>CNN的局部连接和权值共享通过卷积核来实现，卷积核也就是“感受野”，感受野使得训练参数，训练复杂度大为减少。（具体怎么算可以参考LeNet5）<br><i><u>4.1.2 降采样</u></i><br>降采样主要体现在池化层，或称为降采样层中。使得特征映射的resolution再次减少。而不论是Pooling层还是卷积层，步长stride也是降采样的有力手段。<br><b>4.2 激活函数层</b><br>为什么要激活函数？为什么要非线性的激活函数？<br>因为没有激活函数，或者激活函数是线性的，就算你<b><u>亿万层的神经网络相当于没有！！！</u></b><br><b>4.3 分类层</b><br>CNN分类中，常见的分类函数就是多项逻辑斯蒂回归模型，Softmax回归模型。它是基于概率的分类模型，利用最小化负对数似然函数来优化。<br><b>5 直观的理解</b><br>排名第一的答案里面的图片是一个简单的说明，简单的概括就是<b>CNN是一个层级递增的结构，像人理解文章一样，先逐字逐句，再段落大意，再全文的理解，CNN也是从像素，边缘，局部形状一直到整体形状的感知</b>。<br>还有很多很有意思的工作，比如某些神经元只对某些图像特征感兴趣啊。。。。。。<br>有需要的同学可以<b><u><i>自己explore more！</i></u></b></p>就这样，有帮助请点赞。</span><!-- react-empty: 1221 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/81312717"><span data-tooltip="发布于 2016-01-13"><!-- react-text: 1226 -->编辑于 <!-- /react-text --><!-- react-text: 1227 -->昨天 13:16<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 1234 -->108<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1241 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1244 -->12 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85806-48912-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85806-48912-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1249 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1252 -->分享<!-- /react-text --></button></div><!-- react-empty: 1253 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1256 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1259 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1262 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1265 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 1271 --><!-- react-empty: 2592 --><!-- react-empty: 1273 --><!-- react-empty: 1274 --><!-- react-empty: 2593 --><!-- react-empty: 1276 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="9" data-zop="{&quot;authorName&quot;:&quot;[已重置]&quot;,&quot;itemId&quot;:120076653,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="120076653" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;120076653&quot;,&quot;upvote_num&quot;:83,&quot;comment_num&quot;:4,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;408320dbe0b107d72c895522e38b4104&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="[已重置]"><meta itemprop="image" content="https://pic1.zhimg.com/263714a0e2218dbc52d047126e2a2c30_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/ijointoo"><meta itemprop="zhihu:followerCount" content="39"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85806-37176-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85806-37176-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/ijointoo"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/263714a0e2218dbc52d047126e2a2c30_xs.jpg" srcset="https://pic1.zhimg.com/263714a0e2218dbc52d047126e2a2c30_l.jpg 2x" alt="[已重置]"></a></div><!-- react-empty: 1290 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85806-14824-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85806-14824-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/ijointoo">[已重置]</a></div><!-- react-empty: 1297 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">互联网非公开股权融资平台</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1304 -->83 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1305 --></span></div></div><meta itemprop="image" content="https://pic3.zhimg.com/548c7958e9b20196c34cc5efe2d38991_200x112.jpg"><meta itemprop="upvoteCount" content="83"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/120076653"><meta itemprop="dateCreated" content="2016-09-01T13:53:40.000Z"><meta itemprop="dateModified" content="2016-09-01T13:53:40.000Z"><meta itemprop="commentCount" content="4"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>通过一个图像分类问题介绍卷积神经网络是如何工作的。下面是卷积神经网络判断一个图片是否包含“儿童”的过程，包括四个步骤：图像输入（Input Image）→卷积（Convolution）→最大池化（Max Pooling）→全连接神经网络（Fully-Connected Neural Network）计算。</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/548c7958e9b20196c34cc5efe2d38991_hd.jpg" data-rawwidth="884" data-rawheight="706" class="origin_image zh-lightbox-thumb" width="884" data-original="https://pic3.zhimg.com/548c7958e9b20196c34cc5efe2d38991_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/548c7958e9b20196c34cc5efe2d38991_hd.jpg" style="width: 654px; height: 522.312px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>首先将图片分割成如图19的重叠的独立小块；图19中，这张照片被分割成了77张大小相同的小图片。</p><p></p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/b563e0cbe4fc62215b8b7af6205242a4_hd.jpg" data-rawwidth="1024" data-rawheight="649" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic4.zhimg.com/b563e0cbe4fc62215b8b7af6205242a4_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/50/b563e0cbe4fc62215b8b7af6205242a4_hd.jpg" style="width: 654px; height: 414.498px;"><div class="VagueImage-mask is-active"></div></div></span></figure>接下来将每一个独立小块输入小的神经网络；这个小的神经网络已经被训练用来判断一个图片是否属于“儿童”类别，它输出的是一个特征数组。<br><p></p><p></p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/0d4eeabf87fb7c13d440d5e8eab11aa0_hd.jpg" data-rawwidth="759" data-rawheight="267" class="origin_image zh-lightbox-thumb" width="759" data-original="https://pic2.zhimg.com/0d4eeabf87fb7c13d440d5e8eab11aa0_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/0d4eeabf87fb7c13d440d5e8eab11aa0_hd.jpg" style="width: 654px; height: 230.063px;"><div class="VagueImage-mask is-active"></div></div></span></figure>标准的数码相机有红、绿、蓝三个通道（Channels），每一种颜色的像素值在0-255之间，构成三个堆叠的二维矩阵；灰度图像则只有一个通道，可以用一个二维矩阵来表示。<p></p><p>将所有的独立小块输入小的神经网络后，再将每一个输出的特征数组按照第一步时77个独立小块的相对位置做排布，得到一个新数组。<br></p><p></p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/a18471cc3ad16f989b15bdc8280613da_hd.jpg" data-rawwidth="780" data-rawheight="635" class="origin_image zh-lightbox-thumb" width="780" data-original="https://pic1.zhimg.com/a18471cc3ad16f989b15bdc8280613da_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic1.zhimg.com/50/a18471cc3ad16f989b15bdc8280613da_hd.jpg" style="width: 654px; height: 532.423px;"><div class="VagueImage-mask is-active"></div></div></span></figure>第二步中，这个小的神经网络对这77张大小相同的小图片都进行同样的计算，也称权重共享（Shared Weights）。这样做是因为，第一，对图像等数组数据来说，局部数组的值经常是高度相关的，可以形成容易被探测到的独特的局部特征；第二，图像和其它信号的局部统计特征与其位置是不太相关的，如果特征图能在图片的一个部分出现，也能出现在任何地方。所以不同位置的单元共享同样的权重，并在数组的不同部分探测相同的模式。数学上，这种由一个特征图执行的过滤操作是一个离散的卷积，卷积神经网络由此得名。<br><p></p><p>卷积步骤完成后，再使用Max Pooling算法来缩减像素采样数组，按照2×2来分割特征矩阵，分出的每一个网格中只保留最大值数组，丢弃其它数组，得到最大池化数组（Max-Pooled Array）。</p><p></p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/2bfb124a0ecc6bd76fb0e420285e69b9_hd.jpg" data-rawwidth="883" data-rawheight="363" class="origin_image zh-lightbox-thumb" width="883" data-original="https://pic2.zhimg.com/2bfb124a0ecc6bd76fb0e420285e69b9_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/2bfb124a0ecc6bd76fb0e420285e69b9_hd.jpg" style="width: 654px; height: 268.858px;"><div class="VagueImage-mask is-active"></div></div></span></figure>接下来将最大池化数组作为另一个神经网络的输入，这个全连接神经网络会最终计算出此图是否符合预期的判断。<br><p></p><p></p><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/7c32be1b79f3ccc6d1b7d95f1a0a6dfa_hd.jpg" data-rawwidth="505" data-rawheight="461" class="origin_image zh-lightbox-thumb" width="505" data-original="https://pic2.zhimg.com/7c32be1b79f3ccc6d1b7d95f1a0a6dfa_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/7c32be1b79f3ccc6d1b7d95f1a0a6dfa_hd.jpg" style="width: 505px; height: 461px;"><div class="VagueImage-mask is-active"></div></div></span></figure>在实际应用时，卷积、最大池化和全连接神经网络计算，这几步中的每一步都可以多次重复进行，总思路是将大图片不断压缩，直到输出单一的值。使用更多卷积步骤，神经网络就可以处理和学习更多的特征。<br><p></p><br><p>原文链接：<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMjM5NDYxNjc5Ng%3D%3D%26mid%3D2649704078%26idx%3D1%26sn%3D3309b203f4c9839981265def2cc724cc%26scene%3D1%26srcid%3D0901isLiWiElskKPdubcq7yF%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">人工智能综述及技术实现 | 爱就投研究</a></p></span><!-- react-empty: 1315 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/120076653">发布于 2016-09-01</a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 1325 -->83<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1332 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1335 -->4 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85811-628-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85811-628-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1340 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1343 -->分享<!-- /react-text --></button></div><!-- react-empty: 1344 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1347 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1350 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1353 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1356 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 1362 --><!-- react-empty: 2594 --><!-- react-empty: 1364 --><!-- react-empty: 1365 --><!-- react-empty: 2595 --><!-- react-empty: 1367 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="10" data-zop="{&quot;authorName&quot;:&quot;张觉非&quot;,&quot;itemId&quot;:144777834,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="144777834" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;144777834&quot;,&quot;upvote_num&quot;:28,&quot;comment_num&quot;:3,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;3101c771f129990f920c5dba3fb6637d&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="张觉非"><meta itemprop="image" content="https://pic2.zhimg.com/v2-5bd50bae21d0280a419a18e0e992ad9d_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/zhang-jue-fei"><meta itemprop="zhihu:followerCount" content="925"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85812-57566-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85812-57566-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/zhang-jue-fei"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-5bd50bae21d0280a419a18e0e992ad9d_xs.jpg" srcset="https://pic2.zhimg.com/v2-5bd50bae21d0280a419a18e0e992ad9d_l.jpg 2x" alt="张觉非"></a></div><!-- react-empty: 1381 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85812-3202-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85812-3202-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/zhang-jue-fei">张觉非</a></div><!-- react-empty: 1388 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><!-- react-empty: 1391 --></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1395 -->28 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1396 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="28"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/144777834"><meta itemprop="dateCreated" content="2017-02-06T08:49:43.000Z"><meta itemprop="dateModified" content="2017-02-06T08:49:43.000Z"><meta itemprop="commentCount" content="3"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>我感觉 CNN 的卷积层就是一个基于滤波器的图像处理器。图像处理领域使用一些人们预先定义好的滤波器起到模糊图像、识别边缘等操作。CNN 就是把滤波器也纳入学习过程，把它们当作可训练的参数。这样在 CNN 的靠近输入图像的那部分（卷积层）就可以认为训练出来了一些滤波器，这些滤波器滤出来的图像再送往网络后端——一个类似传统神经网络的分类器。</p><br><p>至于那些训练出来的滤波器是起什么作用的，那就有点黑盒的意思了。常见的滤波器：高通、低通、高斯模糊、SOBEL 查找边缘 ... 这些，是白盒。是人有目的设计出来的。CNN 卷积层的那些滤波器则是根据训练样本拟合出来的。用这些训练得到的滤波器去滤一滤图像看看，也许能看出其中一些滤波器的“目的”。</p></span><!-- react-empty: 2297 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/144777834">发布于 2017-02-06</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2304 -->28<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2311 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2314 -->3 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85900-96777-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85900-96777-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2319 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2322 -->分享<!-- /react-text --></button></div><!-- react-empty: 2323 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2326 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2329 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2332 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2335 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 1453 --><!-- react-empty: 2596 --><!-- react-empty: 1455 --><!-- react-empty: 1456 --><!-- react-empty: 2597 --><!-- react-empty: 1458 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="11" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:157448234,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="157448234" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;157448234&quot;,&quot;upvote_num&quot;:35,&quot;comment_num&quot;:7,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;edcb3f84977d280287bc025bdbe86465&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="4443"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><!-- react-text: 1472 -->知乎用户<!-- /react-text --></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">Quant Trader</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1479 -->35 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1480 --></span></div></div><meta itemprop="image" content="https://pic3.zhimg.com/v2-4c43b251f1e07aeeac4aa9f2223888e8_200x112.jpg"><meta itemprop="upvoteCount" content="35"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/157448234"><meta itemprop="dateCreated" content="2017-04-15T21:40:03.000Z"><meta itemprop="dateModified" content="2017-04-15T21:40:03.000Z"><meta itemprop="commentCount" content="7"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>以下我从The Data Science Blog转载一篇解释卷积神经网络的好文章，图文并茂，通俗易懂。作者：Ujjwal Karn ，简介：<a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/" class=" wrap external" target="_blank" rel="nofollow noreferrer">the data science blog</a> 。文章原链接：<a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" class=" wrap external" target="_blank" rel="nofollow noreferrer">An Intuitive Explanation of Convolutional Neural Networks</a></p><br><p>An Intuitive Explanation of Convolutional Neural Networks</p><br><p>Posted on <a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" class=" wrap external" target="_blank" rel="nofollow noreferrer">August 11, 2016</a> by <a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/author/ujwlkarn/" class=" wrap external" target="_blank" rel="nofollow noreferrer">ujjwalkarn</a></p><br><p>What are Convolutional Neural Networks and why are they important?</p><p>Convolutional Neural Networks (<b>ConvNets</b> or <b>CNNs</b>) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-4c43b251f1e07aeeac4aa9f2223888e8_hd.jpg" data-rawwidth="751" data-rawheight="284" class="origin_image zh-lightbox-thumb" width="751" data-original="https://pic3.zhimg.com/v2-4c43b251f1e07aeeac4aa9f2223888e8_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-4c43b251f1e07aeeac4aa9f2223888e8_hd.jpg" style="width: 654px; height: 247.318px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 1: Source [1]</p><br><p>In <b>Figure 1</b> above, a ConvNet is able to recognize scenes and the system is able to suggest relevant tags such as ‘bridge’, ‘railway’ and ‘tennis’ while <b>Figure 2</b> shows an example of ConvNets being used for recognizing everyday objects, humans and animals. Lately, ConvNets have been effective in several Natural Language Processing tasks (such as sentence classification) as well.</p><br><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-7c1ea3c08456fbbe9f18bb4cf76f88c0_hd.jpg" data-rawwidth="748" data-rawheight="279" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic1.zhimg.com/v2-7c1ea3c08456fbbe9f18bb4cf76f88c0_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic1.zhimg.com/50/v2-7c1ea3c08456fbbe9f18bb4cf76f88c0_hd.jpg" style="width: 654px; height: 243.939px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 2: Source [2]</p><br><p>ConvNets, therefore, are an important tool for most machine learning practitioners today. However, understanding ConvNets and learning to use them for the first time can sometimes be an intimidating experience. The primary purpose of this blog post is to develop an understanding of how Convolutional Neural Networks work on images.</p><p>If you are new to neural networks in general, I would recommend reading <a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">this short tutorial on Multi Layer Perceptrons</a> to get an idea about how they work, before proceeding. Multi Layer Perceptrons are referred to as “Fully Connected Layers” in this post.</p><br><p>The LeNet Architecture (1990s)</p><p>LeNet was one of the very first convolutional neural networks which helped propel the field of Deep Learning. This pioneering work by Yann LeCun was named <a href="https://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">LeNet5</a> after many previous successful iterations since the year 1988 [3]. At that time the LeNet architecture was used mainly for character recognition tasks such as reading zip codes, digits, etc.</p><p>Below, we will develop an intuition of how the LeNet architecture learns to recognize images. There have been several new architectures proposed in the recent years which are improvements over the LeNet, but they all use the main concepts from the LeNet and are relatively easier to understand if you have a clear understanding of the former.</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-9b70d0829d8357d0d16f9db279d50012_hd.jpg" data-rawwidth="748" data-rawheight="178" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic3.zhimg.com/v2-9b70d0829d8357d0d16f9db279d50012_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-9b70d0829d8357d0d16f9db279d50012_hd.jpg" style="width: 654px; height: 155.631px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 3: A simple ConvNet. Source [5]</p><br><p>The Convolutional Neural Network in <b>Figure 3 </b>is similar in architecture to the original LeNet and classifies an input image into four categories: dog, cat, boat or bird (the original LeNet was used mainly for character recognition tasks). As evident from the figure above, on receiving a boat image as input, the network correctly assigns the highest probability for boat (0.94) among all four categories. The sum of all probabilities in the output layer should be one (explained later in this post).</p><p>There are four main operations in the ConvNet shown in <b>Figure 3</b> above:</p><ol><li>Convolution</li><li>Non Linearity (ReLU)</li><li>Pooling or Sub Sampling</li><li>Classification (Fully Connected Layer)</li></ol><p>These operations are the basic building blocks of <i>every</i> Convolutional Neural Network, so understanding how these work is an important step to developing a sound understanding of ConvNets. We will try to understand the intuition behind each of these operations below.</p><p>Images are a matrix of pixel values</p><p>Essentially, every image can be represented as a matrix of pixel values.</p><br><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-177283e18db018ac8321e158190dafdc_hd.jpg" data-rawwidth="192" data-rawheight="192" class="content_image" width="192"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic4.zhimg.com/50/v2-177283e18db018ac8321e158190dafdc_hd.jpg" style="width: 192px; height: 192px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 4: Every image is a matrix of pixel values. Source [6]</p><br><p><a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Channel_%28digital_image%29" class=" wrap external" target="_blank" rel="nofollow noreferrer">Channel</a> is a conventional term used to refer to a certain component of an image. An image from a standard digital camera will have three channels – red, green and blue – you can imagine those as three 2d-matrices stacked over each other (one for each color), each having pixel values in the range 0 to 255.</p><p>A <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Grayscale" class=" wrap external" target="_blank" rel="nofollow noreferrer">grayscale</a> image, on the other hand, has just one channel. For the purpose of this post, we will only consider grayscale images, so we will have a single 2d matrix representing an image. The value of each pixel in the matrix will range from 0 to 255 – zero indicating black and 255 indicating white.</p><br><p>The Convolution Step</p><p>ConvNets derive their name from the <a href="https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Convolution" class=" wrap external" target="_blank" rel="nofollow noreferrer">“convolution” operator</a>. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. We will not go into the mathematical details of Convolution here, but will try to understand how it works over images.</p><p>As we discussed above, every image can be considered as a matrix of pixel values. Consider a 5 x 5 image whose pixel values are only 0 and 1 (note that for a grayscale image, pixel values range from 0 to 255, the green matrix below is a special case where pixel values are only 0 and 1):</p><br><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-a67dbfb0e31fc4b32aeb6debba882197_hd.jpg" data-rawwidth="127" data-rawheight="115" class="content_image" width="127"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-a67dbfb0e31fc4b32aeb6debba882197_hd.jpg" style="width: 127px; height: 115px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><p>Also, consider another 3 x 3 matrix as shown below:</p><br><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-4afb1f5a464fd499dd1b60e8a30f219c_hd.jpg" data-rawwidth="74" data-rawheight="63" class="content_image" width="74"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-4afb1f5a464fd499dd1b60e8a30f219c_hd.jpg" style="width: 74px; height: 63px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><p>Then, the Convolution of the 5 x 5 image and the 3 x 3 matrix can be computed as shown in the animation in <b>Figure 5</b> below:</p><br><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-32b19250b52d9177343aed3b428b95f0_hd.jpg" data-rawwidth="268" data-rawheight="196" class="content_image" width="268"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-32b19250b52d9177343aed3b428b95f0_hd.jpg" style="width: 268px; height: 196px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 5: The Convolution operation. The output matrix is called Convolved Feature or <b>Feature Map. Source [7]</b></p><br><p>Take a moment to understand how the computation above is being done. We slide the orange matrix over our original image (green) by 1 pixel (also called ‘stride’) and for every position, we compute element wise multiplication (between the two matrices) and add the multiplication outputs to get the final integer which forms a single element of the output matrix (pink). Note that the 3×3 matrix “sees” only a part of the input image in each stride.</p><p>In CNN terminology, the 3×3 matrix is called a ‘<b>filter</b>‘ or ‘kernel’ or ‘feature detector’ and the matrix formed by sliding the filter over the image and computing the dot product is called the ‘Convolved Feature’ or ‘Activation Map’ or the ‘<b>Feature Map</b>‘. It is important to note that filters acts as feature detectors from the original input image.</p><p>It is evident from the animation above that different values of the filter matrix will produce different Feature Maps for the same input image. As an example, consider the following input image:</p><br><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-9db46f880cb650fd5e6dbafdabc4c3f1_hd.jpg" data-rawwidth="67" data-rawheight="66" class="content_image" width="67"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-9db46f880cb650fd5e6dbafdabc4c3f1_hd.jpg" style="width: 67px; height: 66px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><p>In the table below, we can see the effects of convolution of the above image with different filters. As shown, we can perform operations such as Edge Detection, Sharpen and Blur just by changing the numeric values of our filter matrix before the convolution operation [8] – this means that different filters can detect different features from an image, for example edges, curves etc. More such examples are available in Section 8.2.4 <a href="https://link.zhihu.com/?target=http%3A//docs.gimp.org/en/plug-in-convmatrix.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">here</a>.</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-644d108587a6ce7fa471ede5d2e11e98_hd.jpg" data-rawwidth="342" data-rawheight="562" class="content_image" width="342"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-644d108587a6ce7fa471ede5d2e11e98_hd.jpg" style="width: 342px; height: 562px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><p>Another good way to understand the Convolution operation is by looking at the animation in <b>Figure 6</b> below:</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-2481d2d208bfea92623999fc4ed1e0a5_hd.jpg" data-rawwidth="502" data-rawheight="293" class="origin_image zh-lightbox-thumb" width="502" data-original="https://pic3.zhimg.com/v2-2481d2d208bfea92623999fc4ed1e0a5_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-2481d2d208bfea92623999fc4ed1e0a5_hd.jpg" style="width: 502px; height: 293px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 6: The Convolution Operation. Source [9]</p><br><p>A filter (with red outline) slides over the input image (convolution operation) to produce a feature map. The convolution of another filter (with the green outline), over the same image gives a different feature map as shown. It is important to note that the Convolution operation captures the local dependencies in the original image. Also notice how these two different filters generate different feature maps from the same original image. Remember that the image and the two filters above are just numeric matrices as we have discussed above.</p><p>In practice, a CNN <i>learns</i> the values of these filters on its own during the training process (although we still need to specify parameters such as <u>number of filters</u>, <u>filter size</u>, <u>architecture of the network</u> etc. before the training process). The more number of filters we have, the more image features get extracted and the better our network becomes at recognizing patterns in unseen images.</p><p>The size of the Feature Map (Convolved Feature) is controlled by three parameters [4] that we need to decide before the convolution step is performed:</p><ul><li><b>Depth:</b> Depth corresponds to the number of filters we use for the convolution operation. In the network shown in <b>Figure 7</b>, we are performing convolution of the original boat image using three distinct filters, thus producing three different feature maps as shown. You can think of these three feature maps as stacked 2d matrices, so, the ‘depth’ of the feature map would be three.</li></ul><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-c1ee1079e919077518c1f7d2b625d3a3_hd.jpg" data-rawwidth="401" data-rawheight="201" class="content_image" width="401"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-c1ee1079e919077518c1f7d2b625d3a3_hd.jpg" style="width: 401px; height: 201px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 7</p><br><ul><li><b>Stride:</b> Stride is the number of pixels by which we slide our filter matrix over the input matrix. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2, then the filters jump 2 pixels at a time as we slide them around. Having a larger stride will produce smaller feature maps.</li><li><b>Zero-padding:</b> Sometimes, it is convenient to pad the input matrix with zeros around the border, so that we can apply the filter to bordering elements of our input image matrix. A nice feature of zero padding is that it allows us to control the size of the feature maps. Adding zero-padding is also called <i>wide convolution</i><b>,</b> and not using zero-padding would be a <i>narrow convolution</i>. This has been explained clearly in [14].</li></ul><br><p>Introducing Non Linearity (ReLU)</p><p>An additional operation called ReLU has been used after every Convolution operation in <b>Figure 3</b> above. ReLU stands for Rectified Linear Unit and is a non-linear operation. Its output is given by:</p><br><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-f352c36d8c236a1f199cc26f088317e8_hd.jpg" data-rawwidth="537" data-rawheight="168" class="origin_image zh-lightbox-thumb" width="537" data-original="https://pic4.zhimg.com/v2-f352c36d8c236a1f199cc26f088317e8_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/50/v2-f352c36d8c236a1f199cc26f088317e8_hd.jpg" style="width: 537px; height: 168px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 8: the ReLU operation</p><br><p>ReLU is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero. The purpose of ReLU is to introduce non-linearity in our ConvNet, since most of the real-world data we would want our ConvNet to learn would be non-linear (Convolution is a linear operation – element wise matrix multiplication and addition, so we account for non-linearity by introducing a non-linear function like ReLU).</p><p>The ReLU operation can be understood clearly from <b>Figure 9</b> below. It shows the ReLU operation applied to one of the feature maps obtained in <b>Figure 6</b> above. The output feature map here is also referred to as the ‘Rectified’ feature map.</p><br><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-2b463dc45d56b569c29f6f9664458f1d_hd.jpg" data-rawwidth="748" data-rawheight="280" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic4.zhimg.com/v2-2b463dc45d56b569c29f6f9664458f1d_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/50/v2-2b463dc45d56b569c29f6f9664458f1d_hd.jpg" style="width: 654px; height: 244.813px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 9: ReLU operation. Source [10]</p><br><p>Other non linear functions such as <b>tanh</b> or <b>sigmoid</b> can also be used instead of ReLU, but ReLU has been found to perform better in most situations.</p><br><p>The Pooling Step</p><p>Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.</p><p>In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. In practice, Max Pooling has been shown to work better.</p><p><b>Figure 10</b> shows an example of Max Pooling operation on a Rectified Feature map (obtained after convolution + ReLU operation) by using a 2×2 window.</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-bdcb436f061e5b819e1fdda4f855669d_hd.jpg" data-rawwidth="494" data-rawheight="421" class="origin_image zh-lightbox-thumb" width="494" data-original="https://pic3.zhimg.com/v2-bdcb436f061e5b819e1fdda4f855669d_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-bdcb436f061e5b819e1fdda4f855669d_hd.jpg" style="width: 494px; height: 421px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 10: Max Pooling. Source [4]</p><br><p>We slide our 2 x 2 window by 2 cells (also called ‘stride’) and take the maximum value in each region. As shown in <b>Figure 10</b>, this reduces the dimensionality of our feature map.</p><p>In the network shown in <b>Figure 11,</b> pooling operation is applied separately to each feature map (notice that, due to this, we get three output maps from three input maps).</p><br><br><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-5af7e27e7c68259833feebbf7702083b_hd.jpg" data-rawwidth="401" data-rawheight="219" class="content_image" width="401"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-5af7e27e7c68259833feebbf7702083b_hd.jpg" style="width: 401px; height: 219px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><p>Figure 11: Pooling applied to Rectified Feature Maps</p><br><p><b>Figure 12</b> shows the effect of Pooling on the Rectified Feature Map we received after the ReLU operation in <b>Figure 9</b> above.</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-d72a2799609dac5f46e30b3f51d168d7_hd.jpg" data-rawwidth="748" data-rawheight="319" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic3.zhimg.com/v2-d72a2799609dac5f46e30b3f51d168d7_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-d72a2799609dac5f46e30b3f51d168d7_hd.jpg" style="width: 654px; height: 278.912px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 12: Pooling. Source [10]</p><br><p>The function of Pooling is to progressively reduce the spatial size of the input representation [4]. In particular, pooling</p><ul><li>makes the input representations (feature dimension) smaller and more manageable</li><li>reduces the number of parameters and computations in the network, therefore, controlling <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Overfitting" class=" wrap external" target="_blank" rel="nofollow noreferrer">overfitting</a> [4]</li><li>makes the network invariant to small transformations, distortions and translations in the input image (a small distortion in input will not change the output of Pooling – since we take the maximum / average value in a local neighborhood).</li><li>helps us arrive at an almost scale invariant representation of our image (the exact term is “equivariant”). This is very powerful since we can detect objects in an image no matter where they are located (read [18] and [19] for details).</li></ul><p>Story so far</p><br><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-1010d3e7a7782833d6a66c123e652e49_hd.jpg" data-rawwidth="748" data-rawheight="178" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic2.zhimg.com/v2-1010d3e7a7782833d6a66c123e652e49_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-1010d3e7a7782833d6a66c123e652e49_hd.jpg" style="width: 654px; height: 155.631px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 13</p><br><p>So far we have seen how Convolution, ReLU and Pooling work. It is important to understand that these layers are the basic building blocks of any CNN. As shown in <b>Figure 13</b>, we have two sets of Convolution, ReLU &amp; Pooling layers – the 2nd Convolution layer performs convolution on the output of the first Pooling Layer using six filters to produce a total of six feature maps. ReLU is then applied individually on all of these six feature maps. We then perform Max Pooling operation separately on each of the six rectified feature maps.</p><p>Together these layers extract the useful features from the images, introduce non-linearity in our network and reduce feature dimension while aiming to make the features somewhat equivariant to scale and translation [18].</p><p>The output of the 2nd Pooling Layer acts as an input to the Fully Connected Layer, which we will discuss in the next section.</p><br><p>Fully Connected Layer</p><p>The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer (other classifiers like SVM can also be used, but will stick to softmax in this post). The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer. I recommend <a href="https://link.zhihu.com/?target=https%3A//ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">reading this post</a> if you are unfamiliar with Multi Layer Perceptrons.</p><p>The output from the convolutional and pooling layers represent high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes based on the training dataset. For example, the image classification task we set out to perform has four possible outputs as shown in <b>Figure 14</b> below (note that Figure 14 does not show connections between the nodes in the fully connected layer)</p><br><br><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-c9b7139d8f3d8f025b57f0fbb3b41c10_hd.jpg" data-rawwidth="484" data-rawheight="152" class="origin_image zh-lightbox-thumb" width="484" data-original="https://pic2.zhimg.com/v2-c9b7139d8f3d8f025b57f0fbb3b41c10_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-c9b7139d8f3d8f025b57f0fbb3b41c10_hd.jpg" style="width: 484px; height: 152px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 14: Fully Connected Layer -each node is connected to every other node in the adjacent layer</p><br><p>Apart from classification, adding a fully-connected layer is also a (usually) cheap way of learning non-linear combinations of these features. Most of the features from convolutional and pooling layers may be good for the classification task, but combinations of those features might be even better [11].</p><p>The sum of output probabilities from the Fully Connected Layer is 1. This is ensured by using the <a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/linear-classify/%23softmax" class=" wrap external" target="_blank" rel="nofollow noreferrer">Softmax</a> as the activation function in the output layer of the Fully Connected Layer. The Softmax function takes a vector of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum to one.</p><p>Putting it all together – Training using Backpropagation</p><p>As discussed above, the Convolution + Pooling layers act as Feature Extractors from the input image while Fully Connected layer acts as a classifier.</p><p>Note that in <b>Figure 15</b> below, since the input image is a boat, the target probability is 1 for Boat class and 0 for other three classes, i.e.</p><ul><li>Input Image = Boat</li><li>Target Vector = [0, 0, 1, 0]</li></ul><br><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-cf87890eb8f2358f23a1ac78eb764257_hd.jpg" data-rawwidth="748" data-rawheight="263" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic2.zhimg.com/v2-cf87890eb8f2358f23a1ac78eb764257_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-cf87890eb8f2358f23a1ac78eb764257_hd.jpg" style="width: 654px; height: 229.949px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 15: Training the ConvNet</p><br><p>The overall training process of the Convolution Network may be summarized as below:</p><ul><li><b>Step1:</b> We initialize all filters and parameters / weights with random values</li><li><b>Step2:</b> The network takes a training image as input, goes through the forward propagation step (convolution, ReLU and pooling operations along with forward propagation in the Fully Connected layer) and finds the output probabilities for each class.</li><ul><li>Lets say the output probabilities for the boat image above are [0.2, 0.4, 0.1, 0.3]</li><li>Since weights are randomly assigned for the first training example, output probabilities are also random.</li></ul><li><b>Step3:</b> Calculate the total error at the output layer (summation over all 4 classes)</li><ul><li><b>Total Error = ∑  ½ (target probability – output probability) ²</b></li></ul><li><b>Step4:</b> Use Backpropagation to calculate the <i>gradients</i> of the error with respect to all weights in the network and use <i>gradient descent</i> to update all filter values / weights and parameter values to minimize the output error.</li><ul><li>The weights are adjusted in proportion to their contribution to the total error.</li><li>When the same image is input again, output probabilities might now be [0.1, 0.1, 0.7, 0.1], which is closer to the target vector [0, 0, 1, 0].</li><li>This means that the network has <i>learnt</i> to classify this particular image correctly by adjusting its weights / filters such that the output error is reduced.</li><li>Parameters like number of filters, filter sizes, architecture of the network etc. have all been fixed before Step 1 and do not change during training process – only the values of the filter matrix and connection weights get updated.</li></ul><li><b>Step5:</b> Repeat steps 2-4 with all images in the training set.</li></ul><p>The above steps <i>train</i> the ConvNet – this essentially means that all the weights and parameters of the ConvNet have now been optimized to correctly classify images from the training set.</p><p>When a new (unseen) image is input into the ConvNet, the network would go through the forward propagation step and output a probability for each class (for a new image, the output probabilities are calculated using the weights which have been optimized to correctly classify all the previous training examples). If our training set is large enough, the network will (hopefully) generalize well to new images and classify them into correct categories.</p><p><b>Note 1:</b> The steps above have been oversimplified and mathematical details have been avoided to provide intuition into the training process. See [4] and [12] for a mathematical formulation and thorough understanding.</p><p><b>Note 2:</b> In the example above we used two sets of alternating Convolution and Pooling layers. Please note however, that these operations can be repeated any number of times in a single ConvNet. In fact, some of the best performing ConvNets today have tens of Convolution and Pooling layers! Also, it is not necessary to have a Pooling layer after every Convolutional Layer. As can be seen in the <b>Figure 16</b> below, we can have multiple Convolution + ReLU operations in succession before having a Pooling operation. Also notice how each layer of the ConvNet is visualized in the Figure 16 below.</p><br><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-6bbe27c711c33fef7cf736c4eaac2da7_hd.jpg" data-rawwidth="606" data-rawheight="294" class="origin_image zh-lightbox-thumb" width="606" data-original="https://pic4.zhimg.com/v2-6bbe27c711c33fef7cf736c4eaac2da7_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/50/v2-6bbe27c711c33fef7cf736c4eaac2da7_hd.jpg" style="width: 606px; height: 294px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 16: Source [4]</p><br><p>Visualizing Convolutional Neural Networks</p><p>In general, the more convolution steps we have, the more complicated features our network will be able to learn to recognize. For example, in Image Classification a ConvNet may learn to detect edges from raw pixels in the first layer, then use the edges to detect simple shapes in the second layer, and then use these shapes to deter higher-level features, such as facial shapes in higher layers [14]. This is demonstrated in <b>Figure 17</b> below – these features were learnt using a <a href="https://link.zhihu.com/?target=http%3A//web.eecs.umich.edu/%7Ehonglak/icml09-ConvolutionalDeepBeliefNetworks.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Convolutional Deep Belief Network</a> and the figure is included here just for demonstrating the idea (this is only an example: real life convolution filters may detect objects that have no meaning to humans).</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-eaf93f1027f136b4d3fd9cbe7a452327_hd.jpg" data-rawwidth="242" data-rawheight="255" class="content_image" width="242"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-eaf93f1027f136b4d3fd9cbe7a452327_hd.jpg" style="width: 242px; height: 255px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 17: Learned features from a Convolutional Deep Belief Network</p><br><p>Adam Harley created amazing visualizations of a Convolutional Neural Network trained on the MNIST Database of handwritten digits [13]. I highly recommend <a href="https://link.zhihu.com/?target=http%3A//scs.ryerson.ca/%7Eaharley/vis/conv/flat.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">playing around with it</a> to understand details of how a CNN works.</p><p>We will see below how the network works for an input ‘8’. Note that the visualization in <b>Figure 18</b> does not show the ReLU operation separately.</p><br><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-4915c442c1035096c91fe5581cb17f94_hd.jpg" data-rawwidth="748" data-rawheight="424" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic3.zhimg.com/v2-4915c442c1035096c91fe5581cb17f94_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/50/v2-4915c442c1035096c91fe5581cb17f94_hd.jpg" style="width: 654px; height: 370.717px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 18: Visualizing a ConvNet trained on handwritten digits</p><br><p>The input image contains 1024 pixels (32 x 32 image) and the first Convolution layer (Convolution Layer 1) is formed by convolution of six unique 5 × 5 (stride 1) filters with the input image. As seen, using six different filters produces a feature map of depth six.</p><p>Convolutional Layer 1 is followed by Pooling Layer 1 that does 2 × 2 max pooling (with stride 2) separately over the six feature maps in Convolution Layer 1. You can move your mouse pointer over any pixel in the Pooling Layer and observe the 4 x 4 grid it forms in the previous Convolution Layer (demonstrated in <b>Figure 19</b>). You’ll notice that the pixel having the maximum value (the brightest one) in the 4 x 4 grid makes it to the Pooling layer.</p><br><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-089e247e34f4d85cac85d4e34b5b824f_hd.jpg" data-rawwidth="748" data-rawheight="179" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic4.zhimg.com/v2-089e247e34f4d85cac85d4e34b5b824f_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/50/v2-089e247e34f4d85cac85d4e34b5b824f_hd.jpg" style="width: 654px; height: 156.505px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 19: Visualizing the Pooling Operation</p><br><p>Pooling Layer 1 is followed by sixteen 5 × 5 (stride 1) convolutional filters that perform the convolution operation. This is followed by Pooling Layer 2 that does 2 × 2 max pooling (with stride 2). These two layers use the same concepts as described above.</p><p>We then have three fully-connected (FC) layers. There are:</p><ul><li>120 neurons in the first FC layer</li><li>100 neurons in the second FC layer</li><li>10 neurons in the third FC layer corresponding to the 10 digits – also called the Output layer</li></ul><p>Notice how in <b>Figure 20</b>, each of the 10 nodes in the output layer are connected to all 100 nodes in the 2nd Fully Connected layer (hence the name Fully Connected).</p><p>Also, note how the only bright node in the Output Layer corresponds to ‘8’ – this means that the network correctly classifies our handwritten digit (brighter node denotes that the output from it is higher, i.e. 8 has the highest probability among all other digits).</p><br><figure><noscript>&lt;img src="https://pic2.zhimg.com/50/v2-90fe0b779375d4610a7f7ce06ed73b8e_hd.jpg" data-rawwidth="748" data-rawheight="155" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic2.zhimg.com/v2-90fe0b779375d4610a7f7ce06ed73b8e_r.jpg"&gt;</noscript><span><div data-reactroot="" class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/50/v2-90fe0b779375d4610a7f7ce06ed73b8e_hd.jpg" style="width: 654px; height: 135.521px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>Figure 20: Visualizing the Filly Connected Layers</p><br><p>The 3d version of the same visualization is available <a href="https://link.zhihu.com/?target=http%3A//scs.ryerson.ca/%7Eaharley/vis/conv/" class=" wrap external" target="_blank" rel="nofollow noreferrer">here</a>.</p><br><p>Other ConvNet Architectures</p><p>Convolutional Neural Networks have been around since early 1990s. We discussed the LeNet above which was one of the very first convolutional neural networks. Some other influential architectures are listed below [3] [4].</p><ul><li><b>LeNet (1990s):</b> Already covered in this article.</li><li><b>1990s to 2012:</b> In the years from late 1990s to early 2010s convolutional neural network were in incubation. As more and more data and computing power became available, tasks that convolutional neural networks could tackle became more and more interesting.</li><li><b>AlexNet (2012) – </b>In 2012, Alex Krizhevsky (and others) released <a href="https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">AlexNet</a> which was a deeper and much wider version of the LeNet and won by a large margin the difficult ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It was a significant breakthrough with respect to the previous approaches and the current widespread application of CNNs can be attributed to this work.</li><li><b>ZF Net (2013) –</b> The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1311.2901" class=" wrap external" target="_blank" rel="nofollow noreferrer">ZFNet</a> (short for Zeiler &amp; Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters.</li><li><b>GoogLeNet (2014) – </b>The ILSVRC 2014 winner was a Convolutional Network from <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.4842" class=" wrap external" target="_blank" rel="nofollow noreferrer">Szegedy et al.</a> from Google. Its main contribution was the development of an <i>Inception Module</i> that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M).</li><li><b>VGGNet (2014) –</b> The runner-up in ILSVRC 2014 was the network that became known as the <a href="https://link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/%7Evgg/research/very_deep/" class=" wrap external" target="_blank" rel="nofollow noreferrer">VGGNet</a>. Its main contribution was in showing that the depth of the network (number of layers) is a critical component for good performance.</li><li><b>ResNets (2015) – </b><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1512.03385" class=" wrap external" target="_blank" rel="nofollow noreferrer">Residual Network</a> developed by Kaiming He (and others) was the winner of ILSVRC 2015. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 2016).</li><li><b>DenseNet (August 2016) –</b> Recently published by Gao Huang (and others), the <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1608.06993" class=" wrap external" target="_blank" rel="nofollow noreferrer">Densely Connected Convolutional Network</a> has each layer directly connected to every other layer in a feed-forward fashion. The DenseNet has been shown to obtain significant improvements over previous state-of-the-art architectures on five highly competitive object recognition benchmark tasks. Check out the Torch implementation <a href="https://link.zhihu.com/?target=https%3A//github.com/liuzhuang13/DenseNet" class=" wrap external" target="_blank" rel="nofollow noreferrer">here</a>.</li></ul><br><p>Conclusion</p><p>In this post, I have tried to explain the main concepts behind Convolutional Neural Networks in simple terms. There are several details I have oversimplified / skipped, but hopefully this post gave you some intuition around how they work.</p><p>This post was originally inspired from <a href="https://link.zhihu.com/?target=http%3A//www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Understanding Convolutional Neural Networks for NLP</a> by Denny Britz (which I would recommend reading) and a number of explanations here are based on that post. For a more thorough understanding of some of these concepts, I would encourage you to go through the <a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer">notes</a> from <a href="https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Stanford’s course on ConvNets</a> as well as other excellent resources mentioned under References below. If you face any issues understanding any of the above concepts or have questions / suggestions, feel free to leave a comment below.</p><p>All images and animations used in this post belong to their respective authors as listed in References section below.</p><br><p>References</p><ol><li><a href="https://link.zhihu.com/?target=https%3A//www.clarifai.com/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Clarifai Home Page</a></li><li>Shaoqing Ren, <i>et al, </i>“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, 2015, <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1506.01497v3.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">arXiv:1506.01497 </a></li><li><a href="https://link.zhihu.com/?target=http%3A//culurciello.github.io/tech/2016/06/04/nets.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Neural Network Architectures</a>, Eugenio Culurciello’s blog</li><li><a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">CS231n Convolutional Neural Networks for Visual Recognition, Stanford</a></li><li><a href="https://link.zhihu.com/?target=https%3A//www.clarifai.com/technology" class=" wrap external" target="_blank" rel="nofollow noreferrer">Clarifai / Technology</a></li><li><a href="https://link.zhihu.com/?target=https%3A//medium.com/%40ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721%23.2gfx5zcw3" class=" wrap external" target="_blank" rel="nofollow noreferrer">Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks</a></li><li><a href="https://link.zhihu.com/?target=http%3A//deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution" class=" wrap external" target="_blank" rel="nofollow noreferrer">Feature extraction using convolution, Stanford</a></li><li><a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kernel_%28image_processing%29" class=" wrap external" target="_blank" rel="nofollow noreferrer">Wikipedia article on Kernel (image processing) </a></li><li><a href="https://link.zhihu.com/?target=http%3A//cs.nyu.edu/%7Efergus/tutorials/deep_learning_cvpr12" class=" wrap external" target="_blank" rel="nofollow noreferrer">Deep Learning Methods for Vision, CVPR 2012 Tutorial </a></li><li><a href="https://link.zhihu.com/?target=http%3A//mlss.tuebingen.mpg.de/2015/slides/fergus/Fergus_1.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Neural Networks by Rob Fergus, Machine Learning Summer School 2015</a></li><li><a href="https://link.zhihu.com/?target=http%3A//stats.stackexchange.com/a/182122/53914" class=" wrap external" target="_blank" rel="nofollow noreferrer">What do the fully connected layers do in CNNs? </a></li><li><a href="https://link.zhihu.com/?target=http%3A//andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Convolutional Neural Networks, Andrew Gibiansky </a></li><li>A. W. Harley, “An Interactive Node-Link Visualization of Convolutional Neural Networks,” in ISVC, pages 867-877, 2015 (<a href="https://link.zhihu.com/?target=http%3A//scs.ryerson.ca/%7Eaharley/vis/harley_vis_isvc15.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">link</a>)</li><li><a href="https://link.zhihu.com/?target=http%3A//www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Understanding Convolutional Neural Networks for NLP</a></li><li><a href="https://link.zhihu.com/?target=http%3A//andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Backpropagation in Convolutional Neural Networks</a></li><li><a href="https://link.zhihu.com/?target=https%3A//adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/" class=" wrap external" target="_blank" rel="nofollow noreferrer">A Beginner’s Guide To Understanding Convolutional Neural Networks</a></li><li>Vincent Dumoulin, <i>et al</i>, “A guide to convolution arithmetic for deep learning”, 2015, <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.07285v1.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">arXiv:1603.07285</a></li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md" class=" wrap external" target="_blank" rel="nofollow noreferrer">What is the difference between deep learning and usual machine learning?</a></li><li><a href="https://link.zhihu.com/?target=https%3A//www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features" class=" wrap external" target="_blank" rel="nofollow noreferrer">How is a convolutional neural network able to learn invariant features?</a></li><li><a href="https://link.zhihu.com/?target=http%3A//journal.frontiersin.org/article/10.3389/frobt.2015.00036/full" class=" wrap external" target="_blank" rel="nofollow noreferrer">A Taxonomy of Deep Convolutional Neural Nets for Computer Vision</a></li></ol></span><!-- react-empty: 1490 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/157448234">发布于 2017-04-16</a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 1500 -->35<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1507 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1510 -->7 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85840-88149-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85840-88149-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1515 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1518 -->分享<!-- /react-text --></button></div><!-- react-empty: 1519 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1522 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1525 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1528 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1531 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 1537 --><!-- react-empty: 2598 --><!-- react-empty: 1539 --><!-- react-empty: 1540 --><!-- react-empty: 2599 --><!-- react-empty: 1542 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="12" data-zop="{&quot;authorName&quot;:&quot;尼箍纳斯凯奇&quot;,&quot;itemId&quot;:109346565,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="109346565" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;109346565&quot;,&quot;upvote_num&quot;:25,&quot;comment_num&quot;:10,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;38f56dadf075979dc114fb1d076945b9&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="尼箍纳斯凯奇"><meta itemprop="image" content="https://pic1.zhimg.com/v2-98bc111189146c1877192ad06ef8dc7b_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/tang-xu-60-83"><meta itemprop="zhihu:followerCount" content="1697"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85842-93630-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85842-93630-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/tang-xu-60-83"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-98bc111189146c1877192ad06ef8dc7b_xs.jpg" srcset="https://pic1.zhimg.com/v2-98bc111189146c1877192ad06ef8dc7b_l.jpg 2x" alt="尼箍纳斯凯奇"></a></div><!-- react-empty: 1556 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85842-34204-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85842-34204-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/tang-xu-60-83">尼箍纳斯凯奇</a></div><!-- react-empty: 1563 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">Deep Learning/Face Recognition/GAN</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1570 -->25 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1571 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="25"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/109346565"><meta itemprop="dateCreated" content="2016-07-04T12:15:02.000Z"><meta itemprop="dateModified" content="2016-07-04T12:15:02.000Z"><meta itemprop="commentCount" content="10"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text">你们这些回答很容易被研究者july抄到博客里面去的 (｀_´)ゞ </span><!-- react-empty: 2338 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/109346565">发布于 2016-07-04</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2345 -->25<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2352 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2355 -->10 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85900-52518-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85900-52518-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2360 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2363 -->分享<!-- /react-text --></button></div><!-- react-empty: 2364 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2367 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2370 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2373 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2376 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 1628 --><!-- react-empty: 2600 --><!-- react-empty: 1630 --><!-- react-empty: 1631 --><!-- react-empty: 2601 --><!-- react-empty: 1633 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="13" data-zop="{&quot;authorName&quot;:&quot;匿名用户&quot;,&quot;itemId&quot;:79322589,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="79322589" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;79322589&quot;,&quot;upvote_num&quot;:10,&quot;comment_num&quot;:5,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;0&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="匿名用户"><meta itemprop="image" content="https://pic3.zhimg.com/aadd7b895_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="0"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/aadd7b895_xs.jpg" srcset="https://pic3.zhimg.com/aadd7b895_l.jpg 2x" alt="匿名用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><!-- react-text: 1647 -->匿名用户<!-- /react-text --></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><!-- react-empty: 1650 --></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1654 -->10 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1655 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="10"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/79322589"><meta itemprop="dateCreated" content="2015-12-30T16:04:41.000Z"><meta itemprop="dateModified" content="2017-03-14T09:37:55.000Z"><meta itemprop="commentCount" content="5"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>没什么特别的，就是多层神经网络而已。后面的层你就想成用前面层训练的特征作为下一层的输入，所以越到后面层数的特征越具体</p><p>卷积看做特征稀疏化后压缩，不丢失信息的情况下减少复杂度</p></span><!-- react-empty: 2379 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/79322589"><span data-tooltip="发布于 2015-12-31"><!-- react-text: 1670 -->编辑于 <!-- /react-text --><!-- react-text: 1671 -->2017-03-14<!-- /react-text --></span></a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2386 -->10<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2393 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2396 -->5 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85900-33191-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85900-33191-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2401 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2404 -->分享<!-- /react-text --></button></div><!-- react-empty: 2405 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2408 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2411 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2414 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2417 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 1715 --><!-- react-empty: 2602 --><!-- react-empty: 1717 --><!-- react-empty: 1718 --><!-- react-empty: 2603 --><!-- react-empty: 1720 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="14" data-zop="{&quot;authorName&quot;:&quot;李振华&quot;,&quot;itemId&quot;:218205885,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="218205885" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;218205885&quot;,&quot;upvote_num&quot;:13,&quot;comment_num&quot;:3,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;623bd765cdd63caa17785db6e88af5cf&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="李振华"><meta itemprop="image" content="https://pic2.zhimg.com/v2-6b56feab2ee23b9858d89eec75e0a301_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/whearer"><meta itemprop="zhihu:followerCount" content="1464"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85842-27328-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85842-27328-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/whearer"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-6b56feab2ee23b9858d89eec75e0a301_xs.jpg" srcset="https://pic2.zhimg.com/v2-6b56feab2ee23b9858d89eec75e0a301_l.jpg 2x" alt="李振华"></a></div><!-- react-empty: 1734 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85842-77922-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85842-77922-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/whearer">李振华</a></div><!-- react-empty: 1741 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">CityU，计算机科学</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1748 -->13 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1749 --></span></div></div><meta itemprop="image" content="https://pic3.zhimg.com/v2-b43aa0850bffce9361284c8623df5813_200x112.jpg"><meta itemprop="upvoteCount" content="13"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/218205885"><meta itemprop="dateCreated" content="2017-08-21T15:16:07.000Z"><meta itemprop="dateModified" content="2017-09-18T14:28:09.000Z"><meta itemprop="commentCount" content="3"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>前面很多答主都说了，卷积核就是滤波器，然而很不幸的，这对于我们这些没有学过信号处理的人来说，这种说法把问题变得更复杂更难理解了。</p><p><br></p><p>从卷积运算上来说一下我的理解吧。一般的，卷积神经网络的卷积运算是使用一个k*k的矩阵，对图像对应的像素做乘法，然后加和。例如对于一张图片（下面图像来自楼上的回答，懒得在找了）</p><p><br></p><figure><noscript>&lt;img src="https://pic1.zhimg.com/50/v2-ad1c05ed02b80dbbacadd178b3b867af_hd.jpg" data-rawwidth="127" data-rawheight="115" class="content_image" width="127"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-ad1c05ed02b80dbbacadd178b3b867af_hd.jpg" style="width: 127px; height: 115px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>使用卷积核</p><figure><noscript>&lt;img src="https://pic4.zhimg.com/50/v2-e76286835bf0b3b2c6e04a015ca70665_hd.jpg" data-rawwidth="74" data-rawheight="63" class="content_image" width="74"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic4.zhimg.com/50/v2-e76286835bf0b3b2c6e04a015ca70665_hd.jpg" style="width: 74px; height: 63px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>做卷积的话，可以得到</p><figure><noscript>&lt;img src="https://pic3.zhimg.com/50/v2-b43aa0850bffce9361284c8623df5813_hd.jpg" data-rawwidth="268" data-rawheight="196" class="content_image" width="268"&gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-b43aa0850bffce9361284c8623df5813_hd.jpg" style="width: 268px; height: 196px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>这可以看出来，如果把卷积核和图像中对应区域都拉成一个向量的话，卷积运算其实就是向量的内积运算。内积是衡量两个向量之间的相似性的，所以卷积核在图像某一区域的卷积实际上就是这个卷积核与图像该区域的相似性。因此，</p><ol><li>一个卷积核探索一种相似性，多个卷积核探索多个相似性。</li><li>卷积核作用在不同的样本上，探索输入数据的局部相关性，即不同样本数据都有这样的局部特征。这是因为，如果一个kernel与A相似又与B相似，那么自然A与B相似，如果A和B是不同的样本，那么kernel 探索的是样本数据共有的局部特征。</li><li>卷积核作用在同一样本的不同位置上（即卷积核在整张图上共享(权值共享）)，如果A和B是同一张图上的不同位置，那么kernel探索的就是不同位置共有的局部特征，或者说，局部特征的平移不变性。</li></ol></span><!-- react-empty: 1759 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/218205885"><span data-tooltip="发布于 2017-08-21"><!-- react-text: 1764 -->编辑于 <!-- /react-text --><!-- react-text: 1765 -->2017-09-18<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 1772 -->13<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1779 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1782 -->3 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85845-85722-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85845-85722-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1787 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1790 -->分享<!-- /react-text --></button></div><!-- react-empty: 1791 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1794 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1797 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 1800 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 1803 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 1809 --><!-- react-empty: 2604 --><!-- react-empty: 1811 --><!-- react-empty: 1812 --><!-- react-empty: 2605 --><!-- react-empty: 1814 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="15" data-zop="{&quot;authorName&quot;:&quot;find goo&quot;,&quot;itemId&quot;:130326267,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="130326267" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;130326267&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:1,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;741ccce6b8fb3b224170a2a37f940d63&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="find goo"><meta itemprop="image" content="https://pic2.zhimg.com/v2-51695b3364c4eee98f5c456f1521f238_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/find-goo"><meta itemprop="zhihu:followerCount" content="2413"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85846-85312-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85846-85312-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/find-goo"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-51695b3364c4eee98f5c456f1521f238_xs.jpg" srcset="https://pic2.zhimg.com/v2-51695b3364c4eee98f5c456f1521f238_l.jpg 2x" alt="find goo"></a></div><!-- react-empty: 1828 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85846-56852-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85846-56852-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/find-goo">find goo</a></div><!-- react-empty: 1835 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">MOV AX, BX</div></div></div></div></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/130326267"><meta itemprop="dateCreated" content="2016-11-07T23:49:24.000Z"><meta itemprop="dateModified" content="2016-11-07T23:49:24.000Z"><meta itemprop="commentCount" content="1"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text">用神经网络就别谈什么理解原理，神经网络理论上是n维空间函数。</span><!-- react-empty: 2420 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/130326267">发布于 2016-11-08</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2427 -->0<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2434 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2437 -->1 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85901-45975-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85901-45975-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2442 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2445 -->分享<!-- /react-text --></button></div><!-- react-empty: 2446 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2449 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2452 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2455 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2458 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 1895 --><!-- react-empty: 2606 --><!-- react-empty: 1897 --><!-- react-empty: 1898 --><!-- react-empty: 2607 --><!-- react-empty: 1900 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="16" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:81181507,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="81181507" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;81181507&quot;,&quot;upvote_num&quot;:4,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;6ab1ba8d043ad96a7bd334153db159e2&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="5032"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><!-- react-text: 1914 -->知乎用户<!-- /react-text --></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">我是机器鼓励师</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 1921 -->4 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 1922 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="4"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/81181507"><meta itemprop="dateCreated" content="2016-01-12T05:49:49.000Z"><meta itemprop="dateModified" content="2016-01-12T05:49:49.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text">我答过很多这方面的问题，你搜吧</span><!-- react-empty: 2461 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/81181507">发布于 2016-01-12</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2468 -->4<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2475 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2478 -->添加评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85901-94040-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85901-94040-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2483 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2486 -->分享<!-- /react-text --></button></div><!-- react-empty: 2487 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2490 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2493 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2496 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2499 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 1979 --><!-- react-empty: 2608 --><!-- react-empty: 1981 --><!-- react-empty: 1982 --><!-- react-empty: 2609 --><!-- react-empty: 1984 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="17" data-zop="{&quot;authorName&quot;:&quot;盛世伟&quot;,&quot;itemId&quot;:93461439,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="93461439" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;93461439&quot;,&quot;upvote_num&quot;:13,&quot;comment_num&quot;:2,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;d1f52374f0b416f6a8e8ed5fdb4ce20d&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="盛世伟"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/hao-wan-75"><meta itemprop="zhihu:followerCount" content="5"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85846-70946-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85846-70946-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/hao-wan-75"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="盛世伟"></a></div><!-- react-empty: 1998 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85846-41863-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85846-41863-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/hao-wan-75">盛世伟</a></div><!-- react-empty: 2005 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">我是EE攻城狮</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 2012 -->13 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 2013 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="13"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/93461439"><meta itemprop="dateCreated" content="2016-04-03T01:39:24.000Z"><meta itemprop="dateModified" content="2016-04-03T01:39:24.000Z"><meta itemprop="commentCount" content="2"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><span><span data-reactroot="" class="UserLink"><div class="Popover"><div id="Popover-85901-68550-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85901-68550-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/fa110e669b2083b7dec0fd49856686ff"><!-- react-text: 5 -->@<!-- /react-text --><!-- react-text: 6 -->司徒功源<!-- /react-text --></a></div><!-- react-empty: 7 --></div></span></span>解答的非常正确，补充一点，每个卷积核其实都是在傅里叶变换后的频域上做乘积，也就是我们常说的虑波。在传统的cv里面，他们管这个叫garbor filter。只不过，cnn使用learning的方法学到这些filter，传统的cv是人工design filter罢了。</span><!-- react-empty: 2502 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/93461439">发布于 2016-04-03</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2509 -->13<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2516 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2519 -->2 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85901-29065-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85901-29065-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2524 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2527 -->分享<!-- /react-text --></button></div><!-- react-empty: 2528 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2531 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2534 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2537 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2540 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 2070 --><!-- react-empty: 2610 --><!-- react-empty: 2072 --><!-- react-empty: 2073 --><!-- react-empty: 2611 --><!-- react-empty: 2075 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="18" data-zop="{&quot;authorName&quot;:&quot;孙九爷&quot;,&quot;itemId&quot;:81234521,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="81234521" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;81234521&quot;,&quot;upvote_num&quot;:4,&quot;comment_num&quot;:1,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;7aa550a83281062dba9c7ce8dc6ec068&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="孙九爷"><meta itemprop="image" content="https://pic2.zhimg.com/v2-f0a43e4fac2c4035e5323b58ef2cef51_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/sun-jiu-ye"><meta itemprop="zhihu:followerCount" content="167"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-85849-97461-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85849-97461-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/sun-jiu-ye"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-f0a43e4fac2c4035e5323b58ef2cef51_xs.jpg" srcset="https://pic2.zhimg.com/v2-f0a43e4fac2c4035e5323b58ef2cef51_l.jpg 2x" alt="孙九爷"></a></div><!-- react-empty: 2089 --></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-85849-5890-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85849-5890-content"><a class="UserLink-link" data-za-detail-view-element_name="User" href="https://www.zhihu.com/people/sun-jiu-ye">孙九爷</a></div><!-- react-empty: 2096 --></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">复杂网络，健身，喜欢历史，科比球迷</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 2103 -->4 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 2104 --></span></div></div><meta itemprop="image" content=""><meta itemprop="upvoteCount" content="4"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/81234521"><meta itemprop="dateCreated" content="2016-01-12T11:35:41.000Z"><meta itemprop="dateModified" content="2016-01-12T11:35:41.000Z"><meta itemprop="commentCount" content="1"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text">试着回答。<br>卷积神经网络最关键的是卷积核，这是卷积神经网络可以从低到高组合得到抽象特征的根本。通过移动卷积核来提取上层输入的局部特征然后非线性组合这些特征得到下层的输入。这样堆叠多层就形成了对特征的高度抽象。所以卷积神经网络的关键是raw data足够多、卷积核提取局部特征、非线性特征组合。它不是真正的理解了要识别的对象，而是从大量数据的统计特征中找到了它。算是暴力学习吧。</span><!-- react-empty: 2543 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/81234521">发布于 2016-01-12</a></div></div><div class="ContentItem-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2550 -->4<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2557 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2560 -->1 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85901-26894-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85901-26894-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2565 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2568 -->分享<!-- /react-text --></button></div><!-- react-empty: 2569 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2572 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2575 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2578 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2581 -->感谢<!-- /react-text --></button></div></div><!-- react-empty: 2161 --><!-- react-empty: 2612 --><!-- react-empty: 2163 --><!-- react-empty: 2164 --><!-- react-empty: 2613 --><!-- react-empty: 2166 --></div></div><div class="List-item"><div class="ContentItem AnswerItem" data-za-index="19" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:133222673,&quot;title&quot;:&quot;卷积神经网络工作原理直观的解释？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="133222673" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;133222673&quot;,&quot;upvote_num&quot;:6,&quot;comment_num&quot;:6,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;39022858&quot;,&quot;author_member_hash_id&quot;:&quot;c42c1143229deb9edc8aa92d96498a13&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="448"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><!-- react-text: 2180 -->知乎用户<!-- /react-text --></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText AuthorInfo-badgeText">只有1枚比特币的在线教育从业者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button class="Button Button--plain" type="button"><!-- react-text: 2187 -->6 人赞同了该回答<!-- /react-text --></button><!-- react-empty: 2188 --></span></div></div><meta itemprop="image" content="https://pic3.zhimg.com/v2-b2e04fce43545e86c208d20b9d1800c9_200x112.jpg"><meta itemprop="upvoteCount" content="6"><meta itemprop="url" content="https://www.zhihu.com/question/39022858/answer/133222673"><meta itemprop="dateCreated" content="2016-11-27T07:14:57.000Z"><meta itemprop="dateModified" content="2016-12-06T02:21:43.000Z"><meta itemprop="commentCount" content="6"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText CopyrightRichText-richText" itemprop="text"><p>还不太懂原理，但是已经发现这个算法存在挺多坑的，给图吧，都是根据<a href="https://link.zhihu.com/?target=http%3A//scs.ryerson.ca/%7Eaharley/vis/conv/" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">scs.ryerson.ca/~aharley</span><span class="invisible">/vis/conv/</span><span class="ellipsis"></span></a>上面生成的。算不上回答题主，但也是给一个视角。<br>1. 当字很小的时候<br></p><figure><noscript>&amp;lt;img src="https://pic3.zhimg.com/50/v2-b2e04fce43545e86c208d20b9d1800c9_hd.jpg" data-rawwidth="388" data-rawheight="820" class="content_image" width="388"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-b2e04fce43545e86c208d20b9d1800c9_hd.jpg" style="width: 388px; height: 820px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><figure><noscript>&amp;lt;img src="https://pic4.zhimg.com/50/v2-a166249a3a8d8537d9a9aa29b93e4a45_hd.jpg" data-rawwidth="384" data-rawheight="818" class="content_image" width="384"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic4.zhimg.com/50/v2-a166249a3a8d8537d9a9aa29b93e4a45_hd.jpg" style="width: 384px; height: 818px;"><div class="VagueImage-mask is-active"></div></div></span></figure>2. 当字是斜着/倒着的时候：<br><figure><noscript>&amp;lt;img src="https://pic3.zhimg.com/50/v2-1dad4f8cb95207f0476b062aa20f0980_hd.jpg" data-rawwidth="388" data-rawheight="824" class="content_image" width="388"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-1dad4f8cb95207f0476b062aa20f0980_hd.jpg" style="width: 388px; height: 824px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><figure><noscript>&amp;lt;img src="https://pic3.zhimg.com/50/v2-b45434cb22e6523bb0491cda84541784_hd.jpg" data-rawwidth="384" data-rawheight="830" class="content_image" width="384"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-b45434cb22e6523bb0491cda84541784_hd.jpg" style="width: 384px; height: 830px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br>3. 被遮挡的时候<br><figure><noscript>&amp;lt;img src="https://pic2.zhimg.com/50/v2-b6755c1e604b90162fae0b172c4965cb_hd.jpg" data-rawwidth="384" data-rawheight="818" class="content_image" width="384"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic2.zhimg.com/50/v2-b6755c1e604b90162fae0b172c4965cb_hd.jpg" style="width: 384px; height: 818px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br><figure><noscript>&amp;lt;img src="https://pic1.zhimg.com/50/v2-0e0c8d449ed2a66235f7df119a6ed09f_hd.jpg" data-rawwidth="388" data-rawheight="814" class="content_image" width="388"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic1.zhimg.com/50/v2-0e0c8d449ed2a66235f7df119a6ed09f_hd.jpg" style="width: 388px; height: 814px;"><div class="VagueImage-mask is-active"></div></div></span></figure>4. 有遮挡物的时候：<br><figure><noscript>&amp;lt;img src="https://pic4.zhimg.com/50/v2-d478a131f337ed4396b3c3e1c59efe12_hd.jpg" data-rawwidth="386" data-rawheight="818" class="content_image" width="386"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic4.zhimg.com/50/v2-d478a131f337ed4396b3c3e1c59efe12_hd.jpg" style="width: 386px; height: 818px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br>6. 与其他数字高度重叠的时候<br><figure><noscript>&amp;lt;img src="https://pic3.zhimg.com/50/v2-1c9694ac7bc791d70a2e10aa5cf40f9c_hd.jpg" data-rawwidth="384" data-rawheight="814" class="content_image" width="384"&amp;gt;</noscript><span><div data-reactroot="" class="VagueImage content_image" data-src="https://pic3.zhimg.com/50/v2-1c9694ac7bc791d70a2e10aa5cf40f9c_hd.jpg" style="width: 384px; height: 814px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p></p>个人感觉好像CNN不能捕捉线条与线条之间的关系。</span><!-- react-empty: 2198 --></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/39022858/answer/133222673"><span data-tooltip="发布于 2016-11-27"><!-- react-text: 2203 -->编辑于 <!-- /react-text --><!-- react-text: 2204 -->2016-12-06<!-- /react-text --></span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button class="Button VoteButton VoteButton--up" aria-label="赞同" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-upIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg><!-- react-text: 2211 -->6<!-- /react-text --></button><button class="Button VoteButton VoteButton--down" aria-label="反对" type="button"><svg viewBox="0 0 20 18" class="Icon VoteButton-downIcon Icon--triangle" width="9" height="16" aria-hidden="true" style="height: 16px; width: 9px;"><title></title><g><path d="M0 15.243c0-.326.088-.533.236-.896l7.98-13.204C8.57.57 9.086 0 10 0s1.43.57 1.784 1.143l7.98 13.204c.15.363.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H1.955c-1.08 0-1.955-.517-1.955-1.9z"></path></g></svg></button></span><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2218 -->​<!-- /react-text --><svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2221 -->6 条评论<!-- /react-text --></button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-85856-19943-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-85856-19943-content"><button class="Button Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2226 -->​<!-- /react-text --><svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2229 -->分享<!-- /react-text --></button></div><!-- react-empty: 2230 --></div><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2233 -->​<!-- /react-text --><svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2236 -->收藏<!-- /react-text --></button><button class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel" type="button"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2239 -->​<!-- /react-text --><svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2242 -->感谢<!-- /react-text --></button><button class="Button ContentItem-action ContentItem-rightButton Button--plain" data-zop-retract-question="true" type="button"><span class="RichContent-collapsedText">收起</span><svg viewBox="0 0 10 6" class="Icon ContentItem-arrowIcon is-active Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div><!-- react-empty: 2248 --><!-- react-empty: 2614 --><!-- react-empty: 2250 --><!-- react-empty: 2251 --><!-- react-empty: 2615 --><!-- react-empty: 2253 --></div></div><div data-reactid="383"></div></div></div></div></div></div></div></div><div class="Question-sideColumn Question-sideColumn--sticky" data-za-detail-view-path-module="RightSideBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div><div class="Sticky is-fixed" style="width: 296px; top: -362px; left: 963.6px;"><div class="Card AppBanner"><a class="AppBanner-link" href="http://zhi.hu/BDXoI"><div class="AppBanner-layout"><img class="AppBanner-qrcode" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAYAAABw4pVUAAAEDElEQVR4Ae3cQW5cRwwE0FGgAzhA7porxGe1gRxBWev9RaHRPc0syrsCySKnCmRjJMgfP/759fW6+O/fv//61u3Pn7+/YePfgq/Xy3zj1ptv3Hqx9cZP4z9OE5ZvT4Easqff8eoaclzSPcJPy1dvrPXi1Ruc8p3PfLHz7GL77/I5bzdkV9HD9TXksKC7dDVkV8HD9Y83RH5vnHHx6o1N+fYX218sf6o3Xz5x4jM/8XdDVGwY15BhA2xfQ1RkGMc3ZHi+R/t0gy1YvfHW38bdkNuKh341JAh0O1xDbise+o2/Ias33vzVN0U9duvl28XdkF0FD9fXkMOC7tLVkF0FD9fHN+T2jU39fEOSHokv1af4af5uSFL8cryGXBY8tashSaHL8ccbsnqj3z2v83iz3x3389nP+C7uhuwqeLi+hhwWdJeuhuwqeLj+05t8mP843btvuAPf1qcbogPDuIYMG2D7GqIiw/hj9e9D0k1NN976lK8+1htf5bN+F+/O1w3ZdeBwfQ05LOguXQ3ZVfBw/eN7iDc43UTn2c23v/yrOM1jP/ON2z/lG7de3A1RkWFcQ4YNsH0NUZFh/PH19fXt79RXb6bzr9abn27uav7qfOY7j/3NTzjxdUOSgpfjNeSy4KldDUkKXY7Hn2V583bn8wbLn+Kpf6rfjdtfPuN+PuPiboiKDOMaMmyA7WuIigzj+LMs50s303yxN3WVbzXf/uLEl+J+HvkTlr8bkhS7HK8hlwVP7WpIUuhy/PGzrNX+3kDrV2+sfKl+Nd/5Ur1x69N85ie+boiKDeMaMmyA7WuIigzjz3TTVm+k+fKn+K4e9kt8zmO+8cSf4omvG6IDw7iGDBtg+xqiIsN4+XuIN9KbuPp5Ep/xxO88p+sTv/E0r/FuiIoM4xoybIDta4iKDOP4PcT50o1MNzvV20+c6lf7my+2f4qbL7bez9MNUbFhXEOGDbB9DVGRYfz4nfruPN7E/xtfmmd1ft8E+Y3Lb7wbooLDuIYMG2D7GqIiw3j5e8juvN5QceL35lovTnwpnvpZv5pvfTdERYZxDRk2wPY1REWG8eP/XHz3DfbzenONi9N88qV8+cWr9Snf+ezXDVGRYVxDhg2wfQ1RkWH8eEOcJ90889MNXc23v1i+hFN9iq/yr+rRDUkKX47XkMuCp3Y1JCl0OR7fkMvzvNIN9yabbzzNb37iM574U779uyFJ0cvxGnJZ8NSuhiSFLsfH35B0Y1f1SHze7JRv/1Rv3Hr7ibshKjaMa8iwAbavISoyjOMbkm7i6fnt540V2//d9fZbxc5nfTdERYZxDRk2wPY1REWG8eMNSTf69rzp5jpPml8+88XyW2/c+tX8boiKDuMaMmyA7WuIigzj/wC7sm2izpKNiwAAAABJRU5ErkJggg==" alt="QR Code of Downloading Zhihu App"><div class="AppBanner-content"><div class="AppBanner-title">下载知乎客户端</div><div class="AppBanner-description">与世界分享知识、经验和见解</div></div></div></a></div><!-- react-empty: 549 --><!-- react-empty: 550 --><!-- react-empty: 551 --><div class="Card" data-za-detail-view-path-module="RelatedQuestions" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header SimilarQuestions-title"><div class="Card-headerText">相关问题</div></div><div class="Card-section SimilarQuestions-list"><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;29366638&quot;,&quot;author_member_hash_id&quot;:&quot;c975db45cc4d20017eeb9e404a5c3251&quot;}}}"><meta itemprop="name" content="卷积神经网络和深度神经网络的区别是什么？"><meta itemprop="url" content="https://www.zhihu.com/question/29366638"><meta itemprop="answerCount" content="8"><meta itemprop="zhihu:followerCount" content="154"><a class="Button Button--plain" target="_blank" type="button" href="https://www.zhihu.com/question/29366638"><!-- react-text: 646 -->卷积神经网络和深度神经网络的区别是什么？<!-- /react-text --></a><!-- react-text: 647 --> <!-- /react-text --><!-- react-text: 648 -->8<!-- /react-text --><!-- react-text: 649 --> 个回答<!-- /react-text --></div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;52668301&quot;,&quot;author_member_hash_id&quot;:&quot;9017f9877139989ab7295033a34b8d9c&quot;}}}"><meta itemprop="name" content="CNN(卷积神经网络)是什么？有入门简介或文章吗？"><meta itemprop="url" content="https://www.zhihu.com/question/52668301"><meta itemprop="answerCount" content="24"><meta itemprop="zhihu:followerCount" content="2252"><a class="Button Button--plain" target="_blank" type="button" href="https://www.zhihu.com/question/52668301"><!-- react-text: 656 -->CNN(卷积神经网络)是什么？有入门简介或文章吗？<!-- /react-text --></a><!-- react-text: 657 --> <!-- /react-text --><!-- react-text: 658 -->24<!-- /react-text --><!-- react-text: 659 --> 个回答<!-- /react-text --></div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;28832761&quot;,&quot;author_member_hash_id&quot;:&quot;e43b5f415b91dfcab00bd0d4ab902cc5&quot;}}}"><meta itemprop="name" content="卷积神经网络如何应用在彩色图像上？"><meta itemprop="url" content="https://www.zhihu.com/question/28832761"><meta itemprop="answerCount" content="5"><meta itemprop="zhihu:followerCount" content="54"><a class="Button Button--plain" target="_blank" type="button" href="https://www.zhihu.com/question/28832761"><!-- react-text: 666 -->卷积神经网络如何应用在彩色图像上？<!-- /react-text --></a><!-- react-text: 667 --> <!-- /react-text --><!-- react-text: 668 -->5<!-- /react-text --><!-- react-text: 669 --> 个回答<!-- /react-text --></div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;31148344&quot;,&quot;author_member_hash_id&quot;:&quot;e7ebdcb572648e8f7f1af95fc0c1c938&quot;}}}"><meta itemprop="name" content="如何用c++在mnist上实现一个简单的卷积神经网络，有哪些参考资料？"><meta itemprop="url" content="https://www.zhihu.com/question/31148344"><meta itemprop="answerCount" content="8"><meta itemprop="zhihu:followerCount" content="156"><a class="Button Button--plain" target="_blank" type="button" href="https://www.zhihu.com/question/31148344"><!-- react-text: 676 -->如何用c++在mnist上实现一个简单的卷积神经网络，有哪些参考资料？<!-- /react-text --></a><!-- react-text: 677 --> <!-- /react-text --><!-- react-text: 678 -->8<!-- /react-text --><!-- react-text: 679 --> 个回答<!-- /react-text --></div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;60493121&quot;,&quot;author_member_hash_id&quot;:&quot;746049a8b754272a2c989c1c16ce0f35&quot;}}}"><meta itemprop="name" content="神经网络中隐层有确切的含义吗？"><meta itemprop="url" content="https://www.zhihu.com/question/60493121"><meta itemprop="answerCount" content="7"><meta itemprop="zhihu:followerCount" content="283"><a class="Button Button--plain" target="_blank" type="button" href="https://www.zhihu.com/question/60493121"><!-- react-text: 686 -->神经网络中隐层有确切的含义吗？<!-- /react-text --></a><!-- react-text: 687 --> <!-- /react-text --><!-- react-text: 688 -->7<!-- /react-text --><!-- react-text: 689 --> 个回答<!-- /react-text --></div></div></div><div class="Card" data-za-detail-view-path-module="ContentList" data-za-detail-view-path-module_name="相关推荐" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null},&quot;attached_info_bytes&quot;:&quot;MiA5MDNhYjU4ODU5MzA0ZGEzYWQxYjNmNWU2MDg3MDVkOQ==&quot;}"><div class="Card-header RelatedCommodities-title"><div class="Card-headerText">相关推荐</div></div><div class="Card-section RelatedCommodities-list"><a class="Button RelatedCommodities-item Button--plain" target="_blank" href="https://www.zhihu.com/remix/albums/924666770973941760" type="button" data-za-detail-view-path-module="RemixAlbumItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;RemixAlbum&quot;,&quot;id&quot;:&quot;924666770973941760&quot;}}}"><img class="RelatedCommodities-image" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-eac4ab2beb9f030daf24a70a51b7ea64_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two">8.8折 | 给年轻人的人文修养课</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-remixMeta"><div><!-- react-text: 2629 -->共 <!-- /react-text --><!-- react-text: 2630 -->13<!-- /react-text --><!-- react-text: 2631 --> 节课<!-- /react-text --></div><button class="Button RelatedCommodities-remixListen Button--plain" type="button"><!-- react-text: 2633 -->试听<!-- /react-text --></button></div></div></div></a><a class="Button RelatedCommodities-item Button--plain" target="_blank" href="https://www.zhihu.com/lives/818124675715600384" type="button" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;818124675715600384&quot;,&quot;author_member_hash_id&quot;:&quot;710eba6a35a79b11101c571177962ffd&quot;}}}"><img class="RelatedCommodities-image" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-969b00dda95cbadbaa93d6bf44c80715_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two">机器学习入门需要哪些数学基础？</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg width="15" height="15" viewBox="0 0 15 15" class="Icon Icon--ratingHalf" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg></div><!-- react-text: 2657 -->3402 人参与<!-- /react-text --></div></div></div></a><a class="Button RelatedCommodities-item Button--plain" target="_blank" href="https://www.zhihu.com/publications/book/119558918" type="button" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119558918&quot;}}}"><img class="RelatedCommodities-image" src="./卷积神经网络工作原理直观的解释？ - 知乎_files/v2-f3ab1e5a93100324a80abc2dc90bd9ae_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two">Python 机器学习实践：测试驱动的开发方法</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta"><!-- react-text: 2665 -->445 人读过<!-- /react-text --><span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;"><!-- react-text: 2668 -->​<!-- /react-text --><svg class="Zi Zi--Ebook" width="13" height="14" fill="currentColor" viewBox="0 0 24 24"><path d="M16 17.649V2.931a.921.921 0 0 0-.045-.283.943.943 0 0 0-1.182-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.934.934 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886z" fill-rule="evenodd"></path></svg></span><!-- react-text: 2671 -->阅读<!-- /react-text --></span></div></div></div></a></div></div><!-- react-empty: 554 --><footer class="Footer"><a class="Footer-item" target="_blank" href="https://liukanshan.zhihu.com/">刘看山</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" href="https://www.zhihu.com/question/19581624">知乎指南</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" href="https://www.zhihu.com/terms">知乎协议</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" href="https://www.zhihu.com/app">应用</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" href="https://www.zhihu.com/careers">工作</a><br><a class="Footer-item" target="_blank" href="https://zhuanlan.zhihu.com/p/28561671">侵权举报</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" href="http://www.12377.cn/">网上有害信息举报专区</a><br><span class="Footer-item">违法和不良信息举报：010-82716601</span><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/jubao">儿童色情信息举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/contact">联系我们</a><span> © 2018 知乎</span></footer></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 886px;"></div></div></div></div><!-- react-empty: 385 --><span></span><div class="KanshanDiversion" data-reactid="387"><div class="Kanshan-container"><div class="Kanshan--dynamic"></div></div><!-- react-empty: 388 --></div></div></main><!-- react-empty: 389 --><!-- react-empty: 390 --><!-- react-empty: 391 --><div class="CornerButtons"><div class="CornerAnimayedFlex"><button class="Button CornerButton Button--plain" data-tooltip="建议反馈" data-tooltip-position="left" aria-label="建议反馈" type="button"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex"><button class="Button CornerButton Button--plain" data-tooltip="回到顶部" data-tooltip-position="left" aria-label="回到顶部" type="button"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script src="./卷积神经网络工作原理直观的解释？ - 知乎_files/vendor.72d10fe31301a0517a94.js.下载"></script><script src="./卷积神经网络工作原理直观的解释？ - 知乎_files/main.raven.209e4d0594c7b9f65720.js.下载" async=""></script><script src="./卷积神经网络工作原理直观的解释？ - 知乎_files/main.app.e3f380c3f73c2238723c.js.下载"></script><script></script><div><div data-reactroot="" style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><span><div></div></span></body></html>