<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/35083779 -->
<html lang="zh" data-hairline="true" data-theme="light" data-focus-method="pointer"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>分布式TensorFlow入门教程</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" property="description" content="码字不易，欢迎给个赞！欢迎交流与转载，文章会同步发布在公众号：机器学习算法全栈工程师(Jeemy110)前言深度学习在各个领域实现突破的一部分原因是我们使用了更多的数据（大数据）来训练更复杂的模型（深度神经网…"><meta data-react-helmet="true" property="og:title" content="分布式TensorFlow入门教程"><meta data-react-helmet="true" property="og:url" content="http://zhuanlan.zhihu.com/p/35083779"><meta data-react-helmet="true" property="og:description" content="码字不易，欢迎给个赞！欢迎交流与转载，文章会同步发布在公众号：机器学习算法全栈工程师(Jeemy110)前言深度学习在各个领域实现突破的一部分原因是我们使用了更多的数据（大数据）来训练更复杂的模型（深度神经网…"><meta data-react-helmet="true" property="og:image" content="https://pic4.zhimg.com/v2-b51e38ce3bb1b4517c25bd2bc397d398_r.jpg"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link href="./分布式TensorFlow入门教程_files/column.app.b135e894c9bee1f30d77.css" rel="stylesheet"><script type="text/javascript" charset="utf-8" async="" src="./分布式TensorFlow入门教程_files/column.modals.e29ea3aab47b431063fe.js.下载"></script><script type="text/javascript" charset="utf-8" async="" src="./分布式TensorFlow入门教程_files/column.richinput.d6473ef647574d98c584.js.下载"></script></head><body class=""><div id="root"><div class="App" data-reactroot=""><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;yue-liang-he-xiao&quot;}" data-zop="{&quot;authorName&quot;:&quot;小白将&quot;,&quot;itemId&quot;:35083779,&quot;title&quot;:&quot;分布式TensorFlow入门教程&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;35083779&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1519.2px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo Icon--logo" style="height:30px;width:64px" width="64" height="30" aria-hidden="true"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="https://zhuanlan.zhihu.com/JeemyJohn"><img class="Avatar Avatar--round" width="30" height="30" src="./分布式TensorFlow入门教程_files/v2-4f43d00a3ae064f695fb1314ca3b7631_is.jpg" srcset="https://pic4.zhimg.com/v2-4f43d00a3ae064f695fb1314ca3b7631_im.jpg 2x" alt="机器学习算法工程师"></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://zhuanlan.zhihu.com/JeemyJohn">机器学习算法工程师</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button FollowButton ColumnPageHeader-FollowButton Button--primary Button--grey">已关注</button><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button><div class="Popover"><button title="更多" type="button" id="Popover-67772-46875-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67772-46875-content" class="Button ColumnPageHeader-MenuToggler Button--plain"><svg class="Zi Zi--Dots" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><img class="TitleImage" src="./分布式TensorFlow入门教程_files/v2-b51e38ce3bb1b4517c25bd2bc397d398_1200x500.jpg" alt="分布式TensorFlow入门教程"><article class="Post-Main Post-NormalMain"><header class="Post-Header"><h1 class="Post-Title">分布式TensorFlow入门教程</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小白将"><meta itemprop="image" content="https://pic3.zhimg.com/v2-4a32ecec0158b446318791a45fc4c538_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/xiaohuzc"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover-67776-20085-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67776-20085-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiaohuzc"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./分布式TensorFlow入门教程_files/v2-4a32ecec0158b446318791a45fc4c538_xs.jpg" srcset="https://pic3.zhimg.com/v2-4a32ecec0158b446318791a45fc4c538_l.jpg 2x" alt="小白将"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover-67776-71329-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67776-71329-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiaohuzc">小白将</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText ztext AuthorInfo-badgeText">为人民日益增长的美好生活需要而读书</div></div></div></div></div><button type="button" class="Button FollowButton Button--primary Button--blue"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Plus FollowButton-icon" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.491 10.488s-.012-5.387 0-5.998c-.037-1.987-3.035-1.987-2.997 0-.038 1.912 0 5.998 0 5.998H4.499c-1.999.01-1.999 3.009 0 3.009s5.995-.01 5.995-.01v5.999c0 2.019 3.006 2.019 2.997 0-.01-2.019 0-5.998 0-5.998s3.996.009 6.004.009c2.008 0 2.008-3-.01-3.009h-5.994z" fill-rule="evenodd"></path></svg></span>关注他</button></div><div><span class="Voters"><button type="button" class="Button Button--plain">129 人赞了该文章</button></span></div></header><div><div class="RichText ztext Post-RichText"><p><b>码字不易，欢迎给个赞！</b></p><p><b>欢迎交流与转载，文章会同步发布在公众号：机器学习算法全栈工程师(Jeemy110)</b></p><hr><h2>前言</h2><p>深度学习在各个领域实现突破的一部分原因是我们使用了更多的数据（大数据）来训练更复杂的模型（深度神经网络），并且可以利用一些高性能并行计算设备如GPU和FPGA来加速模型训练。但是有时候，模型之大或者训练数据量之多可能超出我们的想象，这个时候就需要分布式训练系统，利用分布式系统我们可以训练更加复杂的模型（单机无法装载），还可以加速我们的训练过程，这对于研究者实现模型的超参数优化是非常有意义的。2017年6月，Facebook发布了他们的论文<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.02677v1.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour</a>，文中指出他们采用分布在32个服务器上的256块GPUs将Resnet-50模型在ImageNet数据集上的训练时间从两周缩短为1个小时。在软件层面，他们使用了很大的minibatch（8192）来训练模型，并且使学习速率正比于minibatch的大小。这意味着，采用分布式系统可以实现模型在成百个GPUs上的训练，从而大大减少训练时间，你也将有更多的机会去尝试各种各样的超参数组合。作为使用人数最多的深度学习框架，TensorFlow从version 0.8开始支持模型的分布式训练，现在的TensorFlow支持模型的多机多卡（GPUs和 CPUs）训练。在这篇文章里面，我将简单介绍分布式TensorFlow的基础知识，并通过实例来讲解如何使用分布式TensorFlow来训练模型。</p><blockquote>Methods that scale with computation are the future of AI.<br>                                                                         —Rich Sutton, 强化学习之父</blockquote><p>在开始之前，有必要先简单介绍一下深度学习的分布式训练策略以及分布式架构。这有助于理解分布式TensorFlow系统。</p><p><br></p><h2>分布式训练策略</h2><p><b>模型并行</b></p><p>所谓模型并行指的是将模型部署到很多设备上（设备可能分布在不同机器上，下同）运行，比如多个机器的GPUs。当神经网络模型很大时，由于显存限制，它是难以在跑在单个GPU上，这个时候就需要模型并行。比如Google的神经机器翻译系统，其可能采用深度LSTM模型，如下图所示，此时模型的不同部分需要分散到许多设备上进行并行训练。深度学习模型一般包含很多层，如果要采用模型并行策略，一般需要将不同的层运行在不同的设备上，但是实际上层与层之间的运行是存在约束的：前向运算时，后面的层需要等待前面层的输出作为输入，而在反向传播时，前面的层又要受限于后面层的计算结果。所以除非模型本身很大，一般不会采用模型并行，因为模型层与层之间存在串行逻辑。但是如果模型本身存在一些可以并行的单元，那么也是可以利用模型并行来提升训练速度，比如<a href="https://link.zhihu.com/?target=https%3A//www.cs.unc.edu/%7Ewliu/papers/GoogLeNet.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">GoogLeNet</a>的Inception模块。</p><figure><noscript><img src="https://pic4.zhimg.com/v2-8bef056b12502410bf0c589e86632a7c_b.jpg" data-size="normal" data-rawwidth="502" data-rawheight="420" class="origin_image zh-lightbox-thumb" width="502" data-original="https://pic4.zhimg.com/v2-8bef056b12502410bf0c589e86632a7c_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-8bef056b12502410bf0c589e86632a7c_hd.jpg" data-size="normal" data-rawwidth="502" data-rawheight="420" class="origin_image zh-lightbox-thumb lazy" width="502" data-original="https://pic4.zhimg.com/v2-8bef056b12502410bf0c589e86632a7c_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-8bef056b12502410bf0c589e86632a7c_b.jpg"><figcaption>模型并行训练</figcaption></figure><p><b>数据并行</b></p><p>深度学习模型最常采用的分布式训练策略是数据并行，因为训练费时的一个重要原因是训练数据量很大。数据并行就是在很多设备上放置相同的模型，并且各个设备采用不同的训练样本对模型训练。训练深度学习模型常采用的是batch SGD方法，采用数据并行，可以每个设备都训练不同的batch，然后收集这些梯度用于模型参数更新。前面所说的Facebook训练Resnet50就是采用数据并行策略，使用256个GPUs，每个GPU读取32个图片进行训练，如下图所示，这样相当于采用非常大的batch（ <img src="./分布式TensorFlow入门教程_files/equation" alt="32\times 256=8192" eeimg="1"> ）来训练模型。</p><figure><noscript><img src="https://pic1.zhimg.com/v2-89ac3ab48edaa1bae2bde0cffb288ad5_b.jpg" data-size="normal" data-rawwidth="659" data-rawheight="231" class="origin_image zh-lightbox-thumb" width="659" data-original="https://pic1.zhimg.com/v2-89ac3ab48edaa1bae2bde0cffb288ad5_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-89ac3ab48edaa1bae2bde0cffb288ad5_hd.jpg" data-size="normal" data-rawwidth="659" data-rawheight="231" class="origin_image zh-lightbox-thumb lazy" width="659" data-original="https://pic1.zhimg.com/v2-89ac3ab48edaa1bae2bde0cffb288ad5_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-89ac3ab48edaa1bae2bde0cffb288ad5_b.jpg"><figcaption>数据并行训练</figcaption></figure><p>数据并行可以是同步的（synchronous），也可以是异步的（asynchronous）。所谓同步指的是所有的设备都是采用相同的模型参数来训练，等待所有设备的mini-batch训练完成后，收集它们的梯度然后取均值，然后执行模型的一次参数更新。这相当于通过聚合很多设备上的mini-batch形成一个很大的batch来训练模型，Facebook就是这样做的，但是他们发现当batch大小增加时，同时线性增加学习速率会取得不错的效果。同步训练看起来很不错，但是实际上需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡，类似于木桶效应，一个拖油瓶会严重拖慢训练进度，所以同步训练方式相对来说训练速度会慢一些。异步训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数，这样总体会训练速度会快很多。但是异步训练的一个很严重的问题是梯度失效问题（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数其实已经被其它设备更新过了，此时这个梯度就过期了，因为现在的模型参数和训练前采用的参数是不一样的。由于梯度失效问题，异步训练虽然速度快，但是可能陷入次优解（sub-optimal training performance）。异步训练和同步训练在TensorFlow中不同点如下图所示：</p><figure><noscript><img src="https://pic3.zhimg.com/v2-1416837956874bb92c719aca09634a17_b.jpg" data-size="normal" data-rawwidth="651" data-rawheight="545" class="origin_image zh-lightbox-thumb" width="651" data-original="https://pic3.zhimg.com/v2-1416837956874bb92c719aca09634a17_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-1416837956874bb92c719aca09634a17_hd.jpg" data-size="normal" data-rawwidth="651" data-rawheight="545" class="origin_image zh-lightbox-thumb lazy" width="651" data-original="https://pic3.zhimg.com/v2-1416837956874bb92c719aca09634a17_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-1416837956874bb92c719aca09634a17_b.jpg"><figcaption>数据并行中的同步方式和异步方式</figcaption></figure><p>为了解决异步训练出现的梯度失效问题，微软提出了一种<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1609.08326" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Asynchronous Stochastic Gradient Descent</a>方法，主要是通过梯度补偿来提升训练效果。应该还有其他类似的研究，感兴趣的可以深入了解一下。</p><p><br></p><h2>分布式训练架构</h2><p>前面说的是分布式训练策略，这里要谈的是系统架构层，包括两种架构：Parameter server architecture（就是常见的PS架构，参数服务器）和Ring-allreduce architecture。这里主要参考<a href="https://link.zhihu.com/?target=https%3A//www.oreilly.com/ideas/distributed-tensorflow" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Distributed TensorFlow</a>，完全是拿来主义了。</p><p><br></p><p><b>Parameter server架构</b></p><p>在Parameter server架构（PS架构）中，集群中的节点被分为两类：parameter server和worker。其中parameter server存放模型的参数，而worker负责计算参数的梯度。在每个迭代过程，worker从parameter sever中获得参数，然后将计算的梯度返回给parameter server，parameter server聚合从worker传回的梯度，然后更新参数，并将新的参数广播给worker。采用同步SGD方式的PS架构如下图所示：</p><figure><noscript><img src="https://pic4.zhimg.com/v2-eaa0cc1152eea0c99471595499b1d3b9_b.jpg" data-size="normal" data-rawwidth="885" data-rawheight="534" class="origin_image zh-lightbox-thumb" width="885" data-original="https://pic4.zhimg.com/v2-eaa0cc1152eea0c99471595499b1d3b9_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-eaa0cc1152eea0c99471595499b1d3b9_hd.jpg" data-size="normal" data-rawwidth="885" data-rawheight="534" class="origin_image zh-lightbox-thumb lazy" width="885" data-original="https://pic4.zhimg.com/v2-eaa0cc1152eea0c99471595499b1d3b9_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-eaa0cc1152eea0c99471595499b1d3b9_b.jpg"><figcaption>PS架构中的同步SGD训练方式</figcaption></figure><p>在TensorFlow之前，Google采用的是<a href="https://link.zhihu.com/?target=https%3A//research.google.com/archive/large_deep_networks_nips2012.html" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">DistBelief</a>框架，其支持PS架构。TensorFlow从DistBelief借鉴了它的很多分布式训练模式，所以TensorFlow也支持PS架构。Mxnet的主要创建者李沐在前人基础上开发了更加通用的轻量级<a href="https://link.zhihu.com/?target=https%3A//github.com/dmlc/ps-lite" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">ps-lite</a>，如果想深入理解PS架构，可以看一下沐神的<a href="https://link.zhihu.com/?target=http%3A//www.mlss2014.com/files/Li/introduction.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">讲解</a>。PS架构是深度学习最常采用的分布式训练架构。</p><p><br></p><p><b>Ring-allreduce架构</b></p><p>在Ring-allreduce架构中，各个设备都是worker，并且形成一个环，如下图所示，没有中心节点来聚合所有worker计算的梯度。在一个迭代过程，每个worker完成自己的mini-batch训练，计算出梯度，并将梯度传递给环中的下一个worker，同时它也接收从上一个worker的梯度。对于一个包含 <img src="./分布式TensorFlow入门教程_files/equation(1)" alt="N" eeimg="1"> 个worker的环，各个worker需要收到其它个 <img src="./分布式TensorFlow入门教程_files/equation(2)" alt="N-1" eeimg="1"> worker的梯度后就可以更新模型参数。其实这个过程需要两个部分：scatter-reduce和allgather，百度的<a href="https://link.zhihu.com/?target=http%3A//research.baidu.com/bringing-hpc-techniques-deep-learning/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">教程</a>对这个过程给出了详细的图文解释。百度开发了自己的<a href="https://link.zhihu.com/?target=https%3A//github.com/baidu-research/baidu-allreduce" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">allreduce框架</a>，并将其用在了深度学习的分布式训练中。</p><figure><noscript><img src="https://pic3.zhimg.com/v2-7b05382a7a62e3664ebc83f1272ea9e3_b.jpg" data-size="normal" data-rawwidth="804" data-rawheight="497" class="origin_image zh-lightbox-thumb" width="804" data-original="https://pic3.zhimg.com/v2-7b05382a7a62e3664ebc83f1272ea9e3_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-7b05382a7a62e3664ebc83f1272ea9e3_hd.jpg" data-size="normal" data-rawwidth="804" data-rawheight="497" class="origin_image zh-lightbox-thumb lazy" width="804" data-original="https://pic3.zhimg.com/v2-7b05382a7a62e3664ebc83f1272ea9e3_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-7b05382a7a62e3664ebc83f1272ea9e3_b.jpg"><figcaption>Ring-allreduce架构示意图</figcaption></figure><p>相比PS架构，Ring-allreduce架构是带宽优化的，因为集群中每个节点的带宽都被充分利用。此外，在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于前面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。在百度的实验中，他们发现训练速度基本上线性正比于GPUs数目（worker数）。</p><p><br></p><h2>分布式TensorFlow简介</h2><p>好了，言归正传，现在开始介绍分布式TensorFlow的基础知识。在分布式TensorFlow中，参与分布式系统的所有节点或者设备被总称为一个集群（cluster），一个cluster中包含很多服务器（server），每个server去执行一项任务（task），server和task是一一对应的。所以，cluster可以看成是server的集合，也可以看成是task的集合。TensorFlow为各个task又增加了一个抽象层，将一系列相似的task集合称为一个job，比如在PS架构中，习惯称parameter server的task集合为ps，而称执行梯度计算的task集合为worker。所以cluster又可以看成是job的集合，不过这只是逻辑上的意义，具体还要看这个server真正干什么。在TensorFlow中，job用name（字符串）标识，而task用index（整数索引）标识，那么cluster中的每个task可以用job的name加上task的index来唯一标识。在分布式系统中，一般情况下各个task在不同的节点或者设备上执行。TensorFlow中用<code>tf.train.ClusterSpec</code>创建一个cluster：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">cluster</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ClusterSpec</span><span class="p">({</span>
<span class="s2">"worker"</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">"worker0.example.com:2222"</span><span class="p">,</span>
    <span class="s2">"worker1.example.com:2222"</span><span class="p">,</span>
    <span class="s2">"worker2.example.com:2222"</span>
<span class="p">],</span>
<span class="s2">"ps"</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">"ps0.example.com:2222"</span><span class="p">,</span>
    <span class="s2">"ps1.example.com:2222"</span>
<span class="p">]})</span>
</code></pre></div><p>可以看出，cluster接收的其实就是一个字典，字典里面包含了各个task所在host的主机地址，这个cluster共包含两类job：ps和worker，共5个task:</p><div class="highlight"><pre><code class="language-text"><span></span>/job:worker/task:0
/job:worker/task:1
/job:worker/task:2
/job:ps/task:0
/job:ps/task:1
</code></pre></div><p>创建好cluster，需要创建各个task的server，使用<code>tf.train.Server</code>函数，比如创建第一个worker的server：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Server</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">job_name</span><span class="o">=</span><span class="s2">"worker"</span><span class="p">,</span> <span class="n">task_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div><p>在创建sever时必须要传入cluster，这样每个server才可以知道自己所在的cluster包含哪些hosts，然后server与server之间才可以通信。sever的创建需要在自己所在host上，一旦所有的server在各自的host上创建好了，整个集群就搭建好了，cluster之间的各个server可以互相通信。具体来说，每个server包含两个组件：<code>master</code>和<code>worker</code>。其中<code>master</code>提供master service，其主要可以提供对cluster中各个设备的远程访问（RPC协议），同时它的另外一个重要功能是作为创建<code>tf.Session</code>的target。而<code>worker</code>提供worker service，可以用本地设备执行TF中的计算子图。这两个东西并不好理解，这里我们先讲TensorFlow中的另外一个重要概念：<code>client</code>，先抛出官方英文解释：</p><blockquote>A client is typically a program that builds a TensorFlow graph and constructs a tensorflow::Session to interact with a cluster. Clients are typically written in Python or C++. A single client process can directly interact with multiple TensorFlow servers (see "Replicated training" above), and a single server can serve multiple clients.</blockquote><p>这个<code>client</code>是个很重要的概念，简单来说就是一个程序，它创建了TF的计算图，并通过建立<code>Session</code>与cluster中的设备进行交互。说白了前面创建的cluster与server只是搭建分布式环境，真正要执行计算需要创建<code>client</code>。对于<code>tf.Session</code><a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/Session" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">这个类</a>，其第一个参数是<code>target</code>，一般情况下大家确实用不到，因为不指定这个参数的话，Session就默认调用本地设备，但是在分布式环境就需要指定了，这就是server里面的<code>master</code>（server.target提供这个参数）。实际上，TensorFlow的完整执行逻辑如下图所示：</p><figure><noscript><img src="https://pic2.zhimg.com/v2-50e3d5b2e1188fb69c830de93ae08aff_b.jpg" data-size="normal" data-rawwidth="827" data-rawheight="291" class="origin_image zh-lightbox-thumb" width="827" data-original="https://pic2.zhimg.com/v2-50e3d5b2e1188fb69c830de93ae08aff_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-50e3d5b2e1188fb69c830de93ae08aff_hd.jpg" data-size="normal" data-rawwidth="827" data-rawheight="291" class="origin_image zh-lightbox-thumb lazy" width="827" data-original="https://pic2.zhimg.com/v2-50e3d5b2e1188fb69c830de93ae08aff_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-50e3d5b2e1188fb69c830de93ae08aff_b.jpg"><figcaption>TensorFlow计算图在单机和分布式系统的执行流程图</figcaption></figure><p>就是说<code>client</code>要跑计算时，其实要先要把计算图以及要执行的节点（Graph中的Node）发给<code>master</code>，<code>master</code>负责资源调度（就是这个计算该怎么执行，在哪些设备执行），最终的执行需要各个<code>worker</code>进程（使用本地设备执行计算），所以每个server会包含<code>master</code>和<code>worker</code>两个部分。关于<code>master</code>的具体作用，可以参考一下TF教程中的<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/extend/architecture" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">TensorFlow Architecture</a>，不过这里贴一张图，大家意淫一下：</p><figure><noscript><img src="https://pic1.zhimg.com/v2-b0b27fa412b7ebe385267a4da3df6d52_b.jpg" data-size="normal" data-rawwidth="820" data-rawheight="365" class="origin_image zh-lightbox-thumb" width="820" data-original="https://pic1.zhimg.com/v2-b0b27fa412b7ebe385267a4da3df6d52_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-b0b27fa412b7ebe385267a4da3df6d52_hd.jpg" data-size="normal" data-rawwidth="820" data-rawheight="365" class="origin_image zh-lightbox-thumb lazy" width="820" data-original="https://pic1.zhimg.com/v2-b0b27fa412b7ebe385267a4da3df6d52_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-b0b27fa412b7ebe385267a4da3df6d52_b.jpg"><figcaption>分布式TensorFlow中的master划分子图</figcaption></figure><p>上面简单解释了一下<code>server</code>和<code>client</code>相关的一些重要概念，帮助大家理解分布式TensorFlow的执行逻辑。那么，在构建Graph时如何调用cluster中的各个server呢？很简单，使用<code>tf.device</code>，只需要指定task的详细信息即可：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/job:ps/task:0"</span><span class="p">):</span>
  <span class="n">weights_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">biases_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/job:ps/task:1"</span><span class="p">):</span>
  <span class="n">weights_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">biases_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/job:worker/task:7"</span><span class="p">):</span>
  <span class="nb">input</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases_1</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">weights_2</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases_2</span><span class="p">)</span>
  <span class="c1"># ...</span>
  <span class="n">train_op</span> <span class="o">=</span> <span class="o">...</span>
</code></pre></div><p>很简单，就如同把cluster里面的设备当成本机设备一样使用，至于怎么真正执行，那就是系统层面的事了。构建了Graph后，我们需要创建Session来执行计算图：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="s2">"grpc://worker7.example.com:2222"</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">)</span>
</code></pre></div><p>注意由于是分布式系统，需要指定Session的target参数，或者采用grpc+主机地址，或者直接利用sever.target，两个是完全一样的。下面我们通过一个简单的实例来理解上面过程，这个例子的cluster共包含3个task：1个ps和2个worker。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s2">"ps_hosts"</span><span class="p">,</span> <span class="s2">"localhost:2222"</span><span class="p">,</span> <span class="s2">"ps hosts"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s2">"worker_hosts"</span><span class="p">,</span> <span class="s2">"localhost:2223,localhost:2224"</span><span class="p">,</span> <span class="s2">"worker hosts"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s2">"job_name"</span><span class="p">,</span> <span class="s2">"worker"</span><span class="p">,</span> <span class="s2">"'ps' or'worker'"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s2">"task_index"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"Index of task within the job"</span><span class="p">)</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
    <span class="n">ps_hosts</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">ps_hosts</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">)</span>
    <span class="n">worker_hosts</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">worker_hosts</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">)</span>
    <span class="c1"># create cluster</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ClusterSpec</span><span class="p">({</span><span class="s2">"ps"</span><span class="p">:</span> <span class="n">ps_hosts</span><span class="p">,</span> <span class="s2">"worker"</span><span class="p">:</span> <span class="n">worker_hosts</span><span class="p">})</span>
    <span class="c1"># create the server</span>
    <span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Server</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">job_name</span><span class="o">=</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">job_name</span><span class="p">,</span> <span class="n">task_index</span><span class="o">=</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">task_index</span><span class="p">)</span>

    <span class="n">server</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</code></pre></div><p>注意这里用单机环境模拟多机环境，然后分别执行下面三个命令行来创建三个server：</p><div class="highlight"><pre><code class="language-text"><span></span>python example.py --job_name=ps --task_index=0
python example.py --job_name=worker --task_index=0
python example.py --job_name=worker --task_index=1
</code></pre></div><p>执行完毕后，三个server都处在等待状态，现在我们在创建一个client来执行一个计算图，并且采用/job:worker/task:0这个server所对应的master，即grpc://localhost:2223来创建Session，如下所示：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/job:ps/task:0"</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/job:worker/task:0"</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/job:worker/task:1"</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="s2">"grpc://localhost:2223"</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
</code></pre></div><p>其实这个client就是一个进程，但是其在计算时需要依靠cluster中的device来执行部分计算子图。值得注意的是上面的程序我们遵循了PS架构，参数放置在ps，而worker执行计算。但是在TensorFlow中，其实每个task所属的job只是一个概念，并没有什么差别，就是说对于上面的程序，你完全可以把参数放置在worker上。所以说，TensorFlow的分布式架构支持PS模式，并且也往往采用这种方式，但是TensorFlow并不完全与PS架构对等。</p><h2>复制训练(Replicated training)</h2><p>前面已经说过了，深度学习模型分布式训练最常用的是数据并行策略，在TensorFlow中称这为复制训练（Replicated training），就是说多个worker使用不同的mini-batch训练相同的模型，计算出的梯度用于更新放置在ps的模型参数。由于复制训练是一种最常用的模式，TensorFlow也增加了一些库函数来简化复制训练的实现。在TensorFlow中共有四种不同的方式来实现复制训练：</p><ol><li><b>In-graph replication</b>：只构建一个client，这个client构建一个Graph，Graph中包含一套模型参数，放置在ps上，同时Graph中包含模型计算部分的多个副本，每个副本都放置在一个worker上，这样多个worker可以同时训练复制的模型。TensorFlow教程中的使用多个GPUs训练<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/tutorials/deep_cnn%23training_a_model_using_multiple_gpu_cards" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">cifar10分类模型</a>就属于这个类型，每个GPUs上的计算子图是相同的，但是属于同一个Graph。这种方法很少使用，因为一旦client挂了，整个系统就全崩溃了，容错能力差。</li><li><b>Between-graph replication</b>：每个worker都创建一个client，这个client一般还与task的主程序在同一进程中。各个client构建相同的Graph，但是参数还是放置在ps上。这种方式就比较好，一个worker的client挂掉了，系统还可以继续跑。</li><li><b>Asynchronous training</b>：异步方式训练，各个worker自己干自己的，不需要与其它worker来协调，前面也已经详细介绍了异步训练，上面两种方式都可以采用异步训练。</li><li><b>Synchronous training</b>：同步训练，各个worker要统一步伐，计算出的梯度要先聚合才可以执行一次模型更新，对于In-graph replication方法，由于各个worker的计算子图属于同一个Graph，很容易实现同步训练。但是对于Between-graph replication方式，各个worker都有自己的client，这就需要系统上的设计了，TensorFlow提供了<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">tf.train.SyncReplicasOptimizer</a>来实现Between-graph replication的同步训练。</li></ol><p>由于在TensorFlow中最常用的是Between-graph replication方式，这里着重讲一下如何实现这种方式。在Between-graph replication中，各个worker都包含一个client，它们构建相同的计算图，然后把参数放在ps上，TensorFlow提供了一个专门的函数<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/train/replica_device_setter" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">tf.train.replica_device_setter</a>来方便Graph构建，先看代码：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># cluster包含两个ps 和三个 worker</span>
<span class="n">cluster_spec</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"ps"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"ps0:2222"</span><span class="p">,</span> <span class="s2">"ps1:2222"</span><span class="p">],</span>
    <span class="s2">"worker"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"worker0:2222"</span><span class="p">,</span> <span class="s2">"worker1:2222"</span><span class="p">,</span> <span class="s2">"worker2:2222"</span><span class="p">]}</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ClusterSpec</span><span class="p">(</span><span class="n">cluster_spec</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">replica_device_setter</span><span class="p">(</span>
                <span class="n">worker_device</span><span class="o">=</span><span class="s2">"/job:worker/task:</span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">task_index</span><span class="p">,</span>
               <span class="n">cluster</span><span class="o">=</span><span class="n">cluster</span><span class="p">)):</span>
  <span class="c1"># Build your graph</span>
  <span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># assigned to /job:ps/task:0</span>
  <span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># assigned to /job:ps/task:1</span>
  <span class="n">v3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># assigned to /job:ps/task:0</span>
  <span class="c1"># Run compute</span>
</code></pre></div><p>使用tf.train.replica_device_setter可以自动把Graph中的Variables放到ps上，而同时将Graph的计算部分放置在当前worker上，省去了很多麻烦。由于ps往往不止一个，这个函数在为各个Variable分配ps时默认采用简单的round-robin方式，就是按次序将参数挨个放到各个ps上，但这个方式可能不能使ps负载均衡，如果需要更加合理，可以采用<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/contrib/training/GreedyLoadBalancingStrategy" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">tf.contrib.training.GreedyLoadBalancingStrategy</a>策略。</p><p>采用Between-graph replication方式的另外一个问题，由于各个worker都独立拥有自己的client，但是对于一些公共操作比如模型参数初始化与checkpoint文件保存等，如果每个client都独立进行这些操作，显然是对资源的浪费。为了解决这个问题，一般会指定一个worker为chief worker，它将作为各个worker的管家，协调它们之间的训练，并且完成模型初始化和模型保存和恢复等公共操作。在TensorFlow中，可以使用<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">tf.train.MonitoredTrainingSession</a>创建client的Session，并且其可以指定哪个worker是chief worker。关于这些方面，想深入理解可以看一下2017 TensorFlow 开发峰会的<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dla_M6bCV91M" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">官方讲解</a>，其中也对分布式TensorFlow的容错机制做了简单介绍。</p><figure><noscript><img src="https://pic3.zhimg.com/v2-fc47066e84dea410b2ca3e3542124d31_b.jpg" data-size="normal" data-rawwidth="955" data-rawheight="555" class="origin_image zh-lightbox-thumb" width="955" data-original="https://pic3.zhimg.com/v2-fc47066e84dea410b2ca3e3542124d31_r.jpg"></noscript><img src="./分布式TensorFlow入门教程_files/v2-fc47066e84dea410b2ca3e3542124d31_hd.jpg" data-size="normal" data-rawwidth="955" data-rawheight="555" class="origin_image zh-lightbox-thumb lazy" width="955" data-original="https://pic3.zhimg.com/v2-fc47066e84dea410b2ca3e3542124d31_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-fc47066e84dea410b2ca3e3542124d31_b.jpg"><figcaption>Between-graph replication方式的编程结构图</figcaption></figure><h2>MNIST分布式训练实例</h2><p>最后，我们给出MNIST的分布式训练实例，采用Between-graph replication方式，并且同步训练和异步训练都支持。在这个例子中，cluster共包含2个ps和2个worker，其中worker1为chief worker。代码如下：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib.learn.python.learn.datasets.mnist</span> <span class="kn">import</span> <span class="n">read_data_sets</span>

<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s2">"ps_hosts"</span><span class="p">,</span> <span class="s2">"localhost:2222"</span><span class="p">,</span> <span class="s2">"ps hosts"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s2">"worker_hosts"</span><span class="p">,</span> <span class="s2">"localhost:2223,localhost:2224"</span><span class="p">,</span> <span class="s2">"worker hosts"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s2">"job_name"</span><span class="p">,</span> <span class="s2">"worker"</span><span class="p">,</span> <span class="s2">"'ps' or'worker'"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s2">"task_index"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"Index of task within the job"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s2">"num_workers"</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">"Number of workers"</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_boolean</span><span class="p">(</span><span class="s2">"is_sync"</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="s2">"using synchronous training or not"</span><span class="p">)</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
    <span class="sd">"""Define a simple mnist classifier"""</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">net</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
    <span class="n">ps_hosts</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">ps_hosts</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">)</span>
    <span class="n">worker_hosts</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">worker_hosts</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">)</span>

    <span class="c1"># create the cluster configured by `ps_hosts' and 'worker_hosts'</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ClusterSpec</span><span class="p">({</span><span class="s2">"ps"</span><span class="p">:</span> <span class="n">ps_hosts</span><span class="p">,</span> <span class="s2">"worker"</span><span class="p">:</span> <span class="n">worker_hosts</span><span class="p">})</span>

    <span class="c1"># create a server for local task</span>
    <span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Server</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">job_name</span><span class="o">=</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">job_name</span><span class="p">,</span>
                             <span class="n">task_index</span><span class="o">=</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">task_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">job_name</span> <span class="o">==</span> <span class="s2">"ps"</span><span class="p">:</span>
        <span class="n">server</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>  <span class="c1"># ps hosts only join</span>
    <span class="k">elif</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">job_name</span> <span class="o">==</span> <span class="s2">"worker"</span><span class="p">:</span>
        <span class="c1"># workers perform the operation</span>
        <span class="c1"># ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(FLAGS.num_ps)</span>

        <span class="c1"># Note: tf.train.replica_device_setter automatically place the paramters (Variables)</span>
        <span class="c1"># on the ps hosts (default placement strategy:  round-robin over all ps hosts, and also</span>
        <span class="c1"># place multi copies of operations to each worker host</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">replica_device_setter</span><span class="p">(</span><span class="n">worker_device</span><span class="o">=</span><span class="s2">"/job:worker/task:</span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">task_index</span><span class="p">),</span>
                                                      <span class="n">cluster</span><span class="o">=</span><span class="n">cluster</span><span class="p">)):</span>
            <span class="c1"># load mnist dataset</span>
            <span class="n">mnist</span> <span class="o">=</span> <span class="n">read_data_sets</span><span class="p">(</span><span class="s2">"./dataset"</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="c1"># the model</span>
            <span class="n">images</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">))</span>

            <span class="c1"># The StopAtStepHook handles stopping after running given steps.</span>
            <span class="n">hooks</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">StopAtStepHook</span><span class="p">(</span><span class="n">last_step</span><span class="o">=</span><span class="mi">2000</span><span class="p">)]</span>

            <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_or_create_global_step</span><span class="p">()</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">is_sync</span><span class="p">:</span>
                <span class="c1"># asynchronous training</span>
                <span class="c1"># use tf.train.SyncReplicasOptimizer wrap optimizer</span>
                <span class="c1"># ref: https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer</span>
                <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">SyncReplicasOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">replicas_to_aggregate</span><span class="o">=</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
                                                       <span class="n">total_num_replicas</span><span class="o">=</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>
                <span class="c1"># create the hook which handles initialization and queues</span>
                <span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">make_session_run_hook</span><span class="p">((</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">task_index</span><span class="o">==</span><span class="mi">0</span><span class="p">)))</span>

            <span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">,</span>
                                          <span class="n">aggregation_method</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AggregationMethod</span><span class="o">.</span><span class="n">ADD_N</span><span class="p">)</span>

            <span class="c1"># The MonitoredTrainingSession takes care of session initialization,</span>
            <span class="c1"># restoring from a checkpoint, saving to a checkpoint, and closing when done</span>
            <span class="c1"># or an error occurs.</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MonitoredTrainingSession</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="n">server</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                   <span class="n">is_chief</span><span class="o">=</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">task_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
                                                   <span class="n">checkpoint_dir</span><span class="o">=</span><span class="s2">"./checkpoint_dir"</span><span class="p">,</span>
                                                   <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span> <span class="k">as</span> <span class="n">mon_sess</span><span class="p">:</span>

                <span class="k">while</span> <span class="ow">not</span> <span class="n">mon_sess</span><span class="o">.</span><span class="n">should_stop</span><span class="p">():</span>
                    <span class="c1"># mon_sess.run handles AbortedError in case of preempted PS.</span>
                    <span class="n">img_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">ls</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="n">mon_sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="p">],</span>
                                               <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">images</span><span class="p">:</span> <span class="n">img_batch</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">label_batch</span><span class="p">})</span>
                    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">print</span><span class="p">(</span><span class="s2">"Train step </span><span class="si">%d</span><span class="s2">, loss: </span><span class="si">%f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">ls</span><span class="p">))</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</code></pre></div><p>异步执行时，分别执行下面四条语句：</p><div class="highlight"><pre><code class="language-bash"><span></span>python distributed_mnist.py --ps_hosts<span class="o">=</span>localhost:2222,localhost:2223 --worker_hosts<span class="o">=</span>localhost:2224,localhost:2225 --job_name<span class="o">=</span>ps --task_index<span class="o">=</span>0

python distributed_mnist.py --ps_hosts<span class="o">=</span>localhost:2222,localhost:2223 --worker_hosts<span class="o">=</span>localhost:2224,localhost:2225 --job_name<span class="o">=</span>ps --task_index<span class="o">=</span>1

python distributed_mnist.py --ps_hosts<span class="o">=</span>localhost:2222,localhost:2223 --worker_hosts<span class="o">=</span>localhost:2224,localhost:2225 --job_name<span class="o">=</span>worker --task_index<span class="o">=</span>0

python distributed_mnist.py --ps_hosts<span class="o">=</span>localhost:2222,localhost:2223 --worker_hosts<span class="o">=</span>localhost:2224,localhost:2225 --job_name<span class="o">=</span>worker --task_index<span class="o">=</span>1
</code></pre></div><p>此时你会看到两个worker打印出的step是交叉的，说明此时是异步执行的，每个worker执行一次梯度计算后，立即将梯度发给ps完成参数更新。</p><p>对于同步执行，采用tf.train.SyncReplicasOptimizer，分别执行下面四条语句：</p><div class="highlight"><pre><code class="language-text"><span></span>python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=0 --is_sync=True

python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=1 --is_sync=True

python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=worker --task_index=0 --is_sync=True

python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=worker --task_index=1 --is_sync=True
</code></pre></div><p>此时你可以看到两个worker基本上同时打印相同的step（但是loss是不一样的），说明是同步执行。值得注意的是，TensorFlow中的同步训练可能与你想象中不同，它只是收集足够的梯度（N个step的梯度结果）就聚合这些梯度值然后执行一次参数更新。但是它不管这N个结果是从哪里来的，如果其中某个worker速度很慢，可能这N个结果都是从其他worker计算出的。言外之意就是chief worker聚合的梯度不一定是从全部worker中收集而来的（参考<a href="https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/issues/11753" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">这个issues</a>）。这个机制很怪异，我想是为了容错机制，不至于一个worker死掉了而终止整个训练过程。所以，在同步训练过程中，最好每个worker的能力都差不多，要不然很难得到想要的加速效果（某个worker慢的话，它计算的梯度可能过期，那么只能被丢弃，这种情况下这个worker做的就是无用功）。</p><p><br></p><h2>走的更远</h2><p>TensorFlow可以与Hadoop和Spark等工具结合，感兴趣的话可以自己深入去学习：</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/deploy/hadoop" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">TensorFlow官方教程：How to run TensorFlow on Hadoop</a>.</li><li><a href="https://link.zhihu.com/?target=http%3A//yahoohadoop.tumblr.com/post/157196317141/open-sourcing-tensorflowonspark-distributed-deep" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Yahho: Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters</a>.</li></ul><p><br></p><h2>小结</h2><p>最近打算学习一下分布式TensorFlow，所以系统地看了官方文档以及一些国外的博客，然后就把其中一些讲解得比较好的地方以及自己的学习心得总结了一下，所以就有了此文。但是网上的资料并不是很多，所以文中有错误之处在所难免，也恳请各位大佬斧正。很偶然地看到一篇最新的综述文章<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.09941" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</a>，60页的paper系统总结了深度学习的并行化策略，想深入学习的可以读读这个paper。</p><p><br></p><hr><h2>参考</h2><ol><li><u><a href="https://link.zhihu.com/?target=https%3A//www.oreilly.com/ideas/distributed-tensorflow" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Distributed TensorFlow</a></u>.</li><li><a href="https://link.zhihu.com/?target=https%3A//medium.com/clusterone/how-to-write-distributed-tensorflow-code-with-an-example-on-tensorport-70bf3306adcb" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">How to write distributed TensorFlow code — with an example on Clusterone</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//henning.kropponline.de/2017/03/19/distributing-tensorflow/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">MNIST实例：Distributing TensorFlow</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/deploy/distributed" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Google TF 官网：Distributed TensorFlow</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dla_M6bCV91M" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Distributed TensorFlow（2017 TensorFlow 开发峰会）</a>.</li><li><a href="https://link.zhihu.com/?target=http%3A//research.baidu.com/bringing-hpc-techniques-deep-learning/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Baidu Research: Bringing HPC Techniques to Deep Learning</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//segmentfault.com/a/1190000008376957" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">TensorFlow学习笔记（9）：分布式TensorFlow</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//ischlag.github.io/2016/06/12/async-distributed-tensorflow/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Distributed TensorFlow Example</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">TensorFlow白皮书2016：TensorFlow: A System for Large-Scale Machine Learning</a>.</li><li><a href="https://link.zhihu.com/?target=https%3A//static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">TensorFlow白皮书2015：TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</a>.</li></ol><p><i>（注：文中图片原始来源均可以在文中或者参考文章的链接中找到）</i></p><p><br></p><p><b>码字不易，欢迎给个赞！</b></p><p><b>欢迎交流与转载，文章会同步发布在公众号：机器学习算法全栈工程师(Jeemy110)</b></p><p></p><p></p></div></div><div class="ContentItem-time"><a target="_blank" href="http://zhuanlan.zhihu.com/p/35083779"><span data-tooltip="发布于 2018-03-29 16:32">编辑于 2018-03-29</span></a></div><div class="Reward"><div><div class="Reward-tagline">「真诚赞赏，手留余香」</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19813032&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="Popover-67782-16067-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67782-16067-content">深度学习（Deep Learning）</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20032249&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20032249" target="_blank"><div class="Popover"><div id="Popover-67783-77167-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67783-77167-content">TensorFlow</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19552071&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19552071" target="_blank"><div class="Popover"><div id="Popover-67784-11034-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67784-11034-content">分布式计算</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 414.6px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;35083779&quot;}}}"><button type="button" class="Button LikeButton ContentItem-action"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like" style="height:16px;width:13px" width="13" height="16" aria-hidden="true"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>129</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>8 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="" id="Popover-67784-70111-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67784-70111-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover-67784-16622-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67784-16622-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div data-za-detail-view-path-module="LeftTabBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;35083779&quot;}}}"><div><div class="Post-SideActions" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--Like" height="48" fill="currentColor" viewBox="0 0 24 24" width="24"><path d="M2.718 10.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.649.718.649h1.45c.916 0 .847-.649.847-.649v-9.554c-.09-.879-.854-.791-.847-.791l-2.446.022zm11.727-.05s.756-1.577 1.156-4.083c.163-2.861-2.052-3.491-3.037-2.362-1.014 1.233 0 2.057-4.45 5.83C7.336 9.966 7 11.011 7 11.998v6.998c-.016 1.104.491 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.911c1.047-3.515-1.95-3.703-1.95-3.703l-5.387.007z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="130">129</div></div></button><div class="Popover ShareMenu"><div class="" id="Popover-76853-62108-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-76853-62108-content"><button><div class="Post-SideActions-icon"><svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></div>分享</button></div></div></div></div></div></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div><div class="PostIndex-Contributes" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/JeemyJohn"><div class="Popover"><div id="Popover-67784-5328-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67784-5328-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./分布式TensorFlow入门教程_files/v2-4f43d00a3ae064f695fb1314ca3b7631_xs.jpg" srcset="https://pic4.zhimg.com/v2-4f43d00a3ae064f695fb1314ca3b7631_l.jpg 2x" alt="机器学习算法工程师"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/JeemyJohn"><div class="Popover"><div id="Popover-67784-7045-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-67784-7045-content">机器学习算法工程师</div></div></a></h2><div class="ContentItem-meta">学习AI算法，请关注微信公众号：机器学习算法工程师……</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton Button--primary Button--grey">已关注</button></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1519px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled=""><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="http://zhuanlan.zhihu.com/p/30497011" class="PostItem"><div><img src="./分布式TensorFlow入门教程_files/v2-dc30927a031a1c395207a2a284ec3fe9_250x0.jpg" srcset="https://pic3.zhimg.com/v2-dc30927a031a1c395207a2a284ec3fe9_qhd.jpg 2x" class="PostItem-TitleImage" alt="分布式 tensorflow 指南"><h1 class="PostItem-Title">分布式 tensorflow 指南</h1><div class="PostItem-Footer"><span>灰灰</span><span class="PostItem-FooterTitle">发表于Tenso...</span></div></div></a><a href="http://zhuanlan.zhihu.com/p/34532319" class="PostItem"><div><img src="./分布式TensorFlow入门教程_files/v2-3c65edb2d8c23b0cb5a14229839566b6_250x0.jpg" srcset="https://pic3.zhimg.com/v2-3c65edb2d8c23b0cb5a14229839566b6_qhd.jpg 2x" class="PostItem-TitleImage" alt="Tensorflow入门教程（2）"><h1 class="PostItem-Title">Tensorflow入门教程（2）</h1><div class="PostItem-Footer"><span>闫17</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="http://zhuanlan.zhihu.com/p/34172340" class="PostItem"><div><img src="./分布式TensorFlow入门教程_files/v2-ff2cf495bef7d66bf8429b4a7f6daf3c_250x0.jpg" srcset="https://pic4.zhimg.com/v2-ff2cf495bef7d66bf8429b4a7f6daf3c_qhd.jpg 2x" class="PostItem-TitleImage" alt="【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践"><h1 class="PostItem-Title">【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践</h1><div class="PostItem-Footer"><span>AI Talk</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="http://zhuanlan.zhihu.com/p/31417693" class="PostItem"><div><h1 class="PostItem-Title">tensorflow保存和恢复模型的两种方法介绍</h1><p class="PostItem-Summary">一、前言 本文将会介绍tensorflow保存和恢复模型的两种方法，一种是传统的Saver类save保存和restore恢复方法，还有一种是比较新颖的SavedModelBuilder类的builder保存和loader文件里的load…</p><div class="PostItem-Footer"><span>芯尚刃</span><span class="PostItem-FooterTitle"></span></div></div></a><button class="PagingButton PagingButton-Next"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Comments Comments--withEditor Comments-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">8 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div class="Comments-footer CommentEditor--normal"><div class="CommentEditor-input Input-wrapper Input-wrapper--spread Input-wrapper--large Input-wrapper--noPadding"><div class="Input Editable"><div class="Dropzone RichText ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-a7t06">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-a7t06" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; white-space: pre-wrap; word-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="a7t06" data-offset-key="eclvs-0-0"><div data-offset-key="eclvs-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="eclvs-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div></div><button disabled="" type="button" class="Button CommentEditor-singleButton Button--primary Button--blue">评论</button></div><div><div class="CommentList"><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68560-75836-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68560-75836-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/qinlibo_nlp"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/v2-8c07862e1f6c22c6aa53504b25a857a3_s.jpg" srcset="https://pic1.zhimg.com/v2-8c07862e1f6c22c6aa53504b25a857a3_xs.jpg 2x" alt="忆臻"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/qinlibo_nlp">忆臻</a></span><span class="CommentItem-time">1 个月前</span></div><div class="RichText ztext CommentItem-content"><p>赞</p></div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>2</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68560-13781-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68560-13781-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tank-abraham"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/c867e2575d1f805ccf948c5e99d7bb5a_s.jpg" srcset="https://pic1.zhimg.com/c867e2575d1f805ccf948c5e99d7bb5a_xs.jpg 2x" alt="Tank Abraham"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tank-abraham">Tank Abraham</a></span><span class="CommentItem-time">1 个月前</span></div><div class="RichText ztext CommentItem-content"><p>我正打算把我实验室的多块GPU整合一下呢</p></div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>1</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68561-96390-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68561-96390-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zc.liu"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/da8e974dc_s.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg 2x" alt="时间的朋友"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zc.liu">时间的朋友</a></span><span class="CommentItem-time">1 个月前</span></div><div class="RichText ztext CommentItem-content"><p>不错的干货</p></div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>1</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68561-17371-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68561-17371-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/youngsean80"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/da8e974dc_s.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg 2x" alt="youngsean"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/youngsean80">youngsean</a></span><span class="CommentItem-time">1 个月前</span></div><div class="RichText ztext CommentItem-content"><p>不错是干货 。赞</p></div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>1</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentList-divider"><div class="CommentList-dividerLine"></div><div class="CommentList-dividerText">以上为精选评论<svg viewBox="0 0 20 20" class="Icon CommentList-questionMark Icon--questionMark" width="14" height="16" aria-hidden="true" data-tooltip="精选评论包含回答作者推荐的评论和部分高赞评论" style="height: 16px; width: 14px;"><title></title><g><path d="M10 20c5.523 0 10-4.477 10-10S15.523 0 10 0 0 4.477 0 10s4.477 10 10 10zm0-3.68c-.328 0-.61-.116-.846-.345-.236-.23-.354-.515-.354-.856 0-.33.118-.612.354-.848.236-.235.518-.353.846-.353.328 0 .61.114.846.343.236.23.354.515.354.856 0 .34-.118.625-.354.855-.236.23-.518.344-.846.344zm2.76-7.4c-.227.233-.636.622-1.226 1.17-.163.155-.295.29-.393.408-.097.117-.17.224-.218.32-.048.098-.086.195-.112.293-.026.098-.066.268-.12.513-.09.518-.376.777-.856.777-.25 0-.46-.085-.63-.254-.17-.17-.255-.42-.255-.755 0-.418.06-.78.186-1.086.125-.307.29-.576.497-.808.206-.23.485-.506.835-.825.307-.28.53-.49.666-.63.137-.143.252-.3.346-.475.093-.175.14-.364.14-.568 0-.4-.143-.735-.428-1.01-.286-.273-.655-.41-1.106-.41-.528 0-.917.138-1.166.414-.25.278-.46.685-.634 1.223-.163.563-.472.845-.928.845-.27 0-.496-.1-.68-.295-.185-.197-.278-.41-.278-.64 0-.473.147-.952.44-1.438.292-.486.72-.888 1.28-1.207C8.683 4.158 9.34 4 10.087 4c.696 0 1.31.133 1.844.4.532.266.944.63 1.234 1.087.29.46.436.957.436 1.495 0 .423-.083.794-.248 1.113-.166.32-.363.594-.59.826z"></path></g></svg></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68561-88080-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68561-88080-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/sun-pei-liang-69"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/v2-94d0ea0dcc0128ec48a194facda9a115_s.jpg" srcset="https://pic4.zhimg.com/v2-94d0ea0dcc0128ec48a194facda9a115_xs.jpg 2x" alt="我是沛亮"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/sun-pei-liang-69">我是沛亮</a></span><span class="CommentItem-time">1 个月前</span></div><div class="RichText ztext CommentItem-content"><p>可以问一下 文中的图是用的什么办法画的吗</p></div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>赞</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68561-44834-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68561-44834-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiaohuzc"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/v2-4a32ecec0158b446318791a45fc4c538_s.jpg" srcset="https://pic4.zhimg.com/v2-4a32ecec0158b446318791a45fc4c538_xs.jpg 2x" alt="小白将"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiaohuzc">小白将</a></span><span class="CommentItem-roleInfo"> (作者) </span><span><span class="CommentItem-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/sun-pei-liang-69">我是沛亮</a></span></span><span class="CommentItem-time">1 个月前</span></div><div class="RichText ztext CommentItem-content">看一下文末</div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>赞</button><button type="button" class="Button CommentItem-talkBtn Button--plain"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--comments Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><g>     <path d="M9 0C3.394 0 0 4.13 0 8c0 1.654.522 3.763 2.014 5.566.314.292.518.82.454 1.17-.165 1.488-.842 1.905-.842 1.905-.328.332.105.67.588.582 1.112-.2 2.07-.58 3.526-1.122.4-.202.464-.147.78-.078C11.524 17.764 18 14 18 8c0-3.665-3.43-8-9-8z"></path>     <path d="M19.14 9.628c.758.988.86 2.01.86 3.15 0 1.195-.62 3.11-1.368 3.938-.21.23-.354.467-.308.722.12 1.073.614 1.5.614 1.5.237.24-.188.563-.537.5-.802-.145-1.494-.42-2.545-.81-.29-.146-.336-.106-.563-.057-2.043.712-4.398.476-6.083-.926 5.964-.524 8.726-3.03 9.93-8.016z"></path>   </g></g></svg>查看对话</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68561-41530-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68561-41530-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/he-lei-75-89"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/da8e974dc_s.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg 2x" alt="键盘上的追梦人"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/he-lei-75-89">键盘上的追梦人</a></span><span class="CommentItem-time">13 天前</span></div><div class="RichText ztext CommentItem-content">体诸你好，想问一下tf.train.monitoredtrainingsession就能完成模型准备和保存模型得操作吗，我看一些帖子用了sv=tf.train.supervisor  sv.prepare_or_wait_for_session 不知道效果是否一样呢？</div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>赞</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover-68561-49536-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover-68561-49536-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiaohuzc"><img class="Avatar UserLink-avatar" width="24" height="24" src="./分布式TensorFlow入门教程_files/v2-4a32ecec0158b446318791a45fc4c538_s.jpg" srcset="https://pic4.zhimg.com/v2-4a32ecec0158b446318791a45fc4c538_xs.jpg 2x" alt="小白将"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiaohuzc">小白将</a></span><span class="CommentItem-roleInfo"> (作者) </span><span><span class="CommentItem-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/he-lei-75-89">键盘上的追梦人</a></span></span><span class="CommentItem-time">13 天前</span></div><div class="RichText ztext CommentItem-content">那个API是低版本的，现在最新的是用这个</div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>赞</button><button type="button" class="Button CommentItem-talkBtn Button--plain"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--comments Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><g>     <path d="M9 0C3.394 0 0 4.13 0 8c0 1.654.522 3.763 2.014 5.566.314.292.518.82.454 1.17-.165 1.488-.842 1.905-.842 1.905-.328.332.105.67.588.582 1.112-.2 2.07-.58 3.526-1.122.4-.202.464-.147.78-.078C11.524 17.764 18 14 18 8c0-3.665-3.43-8-9-8z"></path>     <path d="M19.14 9.628c.758.988.86 2.01.86 3.15 0 1.195-.62 3.11-1.368 3.938-.21.23-.354.467-.308.722.12 1.073.614 1.5.614 1.5.237.24-.188.563-.537.5-.802-.145-1.494-.42-2.545-.81-.29-.146-.336-.106-.563-.057-2.043.712-4.398.476-6.083-.926 5.964-.524 8.726-3.03 9.93-8.016z"></path>   </g></g></svg>查看对话</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div></div><span></span></div></div></div></article></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script src="./分布式TensorFlow入门教程_files/vendor.c1ed8d16a6988c3797dd.js.下载"></script><script src="./分布式TensorFlow入门教程_files/column.raven.43187d3e7b4f1a19a5fc.js.下载" async=""></script><script src="./分布式TensorFlow入门教程_files/column.app.e5934a8571a959ca53ec.js.下载"></script><script></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><script src="./分布式TensorFlow入门教程_files/js"></script><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><div class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplet-68051-85830-0" id="Popover-68051-41410-toggle" aria-haspopup="true" aria-owns="Popover-68051-41410-content" class="Input" placeholder="选择语言" value=""><div class="Input-after"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></div></div></div></div></div></div></div></body></html>